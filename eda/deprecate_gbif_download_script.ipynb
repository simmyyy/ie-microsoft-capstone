{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15f45dc6",
      "metadata": {},
      "source": [
        "### 18) Download an existing GBIF download job by key (job id)\n",
        "\n",
        "If you already ran a GBIF download job, you can download the resulting archive later using its **download key**, e.g.:\n",
        "- `0003205-260208012135463`\n",
        "\n",
        "Direct link format:\n",
        "- `https://api.gbif.org/v1/occurrence/download/request/<KEY>.zip`\n",
        "\n",
        "The code below downloads the ZIP into a format-dependent folder:\n",
        "- `SIMPLE_CSV` -> `data/gbif_downloads/`\n",
        "- `SIMPLE_PARQUET` -> `data/gbif_parquet_downloads/`\n",
        "- `DWCA` -> `data/dwca_downloads/`\n",
        "\n",
        "It can also optionally extract the ZIP into `DOWNLOAD_DIR/_extracted/<key>/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7c9b4d00",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8569832",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COUNTRIES: ['PT', 'ES', 'FR']\n",
            "YEAR(S): [2024]\n",
            "DOWNLOAD_FORMAT: SIMPLE_PARQUET\n",
            "DOWNLOAD_DIR: data/gbif_parquet_downloads\n",
            "EXTRACT_AFTER_DOWNLOAD: True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# === Global config (edit here) ===\n",
        "# Countries (ISO-2): Portugal=PT, Spain=ES, France=FR\n",
        "COUNTRIES = [\"PT\", \"ES\", \"FR\"]\n",
        "# Single year (used when YEAR_END is None)\n",
        "YEAR = 2024\n",
        "# Optional: for multiple years set YEAR_START + YEAR_END (e.g. 2000â€“2026)\n",
        "YEAR_START = None  # e.g. 2000\n",
        "YEAR_END = None    # e.g. 2026\n",
        "\n",
        "# Download format (GBIF download jobs)\n",
        "# Must be one of: SIMPLE_CSV, SIMPLE_PARQUET, DWCA, SPECIES_LIST, SIMPLE_AVRO\n",
        "DOWNLOAD_FORMAT = \"SIMPLE_PARQUET\"  # recommended if you want Parquet directly\n",
        "\n",
        "# Where to store downloaded archives\n",
        "if DOWNLOAD_FORMAT == \"DWCA\":\n",
        "    DOWNLOAD_DIR = Path(\"data/dwca_downloads\")\n",
        "elif DOWNLOAD_FORMAT == \"SIMPLE_PARQUET\":\n",
        "    DOWNLOAD_DIR = Path(\"data/gbif_parquet_downloads\")\n",
        "else:\n",
        "    DOWNLOAD_DIR = Path(\"data/gbif_downloads\")\n",
        "\n",
        "# If True, after downloading a ZIP, extract it into DOWNLOAD_DIR/_extracted/<key>/\n",
        "EXTRACT_AFTER_DOWNLOAD = True\n",
        "\n",
        "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# Build YEARS list: range if YEAR_START/YEAR_END set, else single YEAR\n",
        "YEARS = list(range(YEAR_START, YEAR_END + 1)) if (YEAR_START is not None and YEAR_END is not None) else [YEAR]\n",
        "print(\"COUNTRIES:\", COUNTRIES)\n",
        "print(\"YEAR(S):\", YEARS)\n",
        "print(\"DOWNLOAD_FORMAT:\", DOWNLOAD_FORMAT)\n",
        "print(\"DOWNLOAD_DIR:\", DOWNLOAD_DIR)\n",
        "print(\"EXTRACT_AFTER_DOWNLOAD:\", EXTRACT_AFTER_DOWNLOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5a57e2b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DRY_RUN disabled -> submit jobs for COUNTRIES + YEARS from config\n",
            "COUNTRIES: ['PT', 'ES', 'FR']\n",
            "YEARS: [2024]\n",
            "Planned jobs: 3 -> [('PT', 2024), ('ES', 2024), ('FR', 2024)]\n",
            "Example queries (first job):\n",
            "- country = PT\n",
            "- year = 2024\n",
            "- hasCoordinate = TRUE\n",
            "DOWNLOAD_FORMAT is SIMPLE_PARQUET\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:Your download key is 0012599-260208012135463\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitted: PT_YEAR_2024 -> 0012599-260208012135463\n",
            "DOWNLOAD_FORMAT is SIMPLE_PARQUET\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:Your download key is 0012600-260208012135463\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitted: ES_YEAR_2024 -> 0012600-260208012135463\n",
            "DOWNLOAD_FORMAT is SIMPLE_PARQUET\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:Your download key is 0012601-260208012135463\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitted: FR_YEAR_2024 -> 0012601-260208012135463\n",
            "Submitted keys:\n",
            "{'PT_YEAR_2024': '0012599-260208012135463', 'ES_YEAR_2024': '0012600-260208012135463', 'FR_YEAR_2024': '0012601-260208012135463'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from pygbif import occurrences\n",
        "\n",
        "\n",
        "# --- Credentials (set these in your shell, not in the notebook output)\n",
        "# export GBIF_USER=\"...\"\n",
        "# export GBIF_PWD=\"...\"\n",
        "# export GBIF_EMAIL=\"...\"\n",
        "GBIF_USER = os.getenv(\"GBIF_USER\")\n",
        "GBIF_PWD = os.getenv(\"GBIF_PWD\")\n",
        "GBIF_EMAIL = os.getenv(\"GBIF_EMAIL\")\n",
        "\n",
        "\n",
        "# --- Flags\n",
        "# DRY_RUN=True -> run a single small Madrid bbox job (or just print the predicate)\n",
        "# DRY_RUN=False -> use COUNTRIES + YEAR from config cell above\n",
        "DRY_RUN = False\n",
        "\n",
        "# Safety switch: never submit by default\n",
        "SUBMIT_JOBS = True\n",
        "\n",
        "# DOWNLOAD_FORMAT, EXTRACT_AFTER_DOWNLOAD come from config cell (cell 2)\n",
        "# DOWNLOAD_FORMAT must be one of: SIMPLE_CSV, SIMPLE_PARQUET, DWCA, SPECIES_LIST, SIMPLE_AVRO\n",
        "\n",
        "# If True, download the resulting archive once the job succeeds.\n",
        "# NOTE: download_get always downloads a ZIP; DWCA is a ZIP too.\n",
        "DOWNLOAD_ON_SUCCESS = False\n",
        "\n",
        "# For multi-country/year submission: optionally wait/poll each job and download when ready (can take hours)\n",
        "WAIT_AND_DOWNLOAD = False\n",
        "\n",
        "\n",
        "def gbif_queries_country_year(country: str, year: int) -> list[str]:\n",
        "    \"\"\"Download query strings for a country+year slice (parsed by pygbif).\"\"\"\n",
        "    return [f\"country = {country}\", f\"year = {year}\", \"hasCoordinate = TRUE\"]\n",
        "\n",
        "\n",
        "def gbif_queries_madrid_bbox_year(bbox: dict, year: int, country: str = \"ES\") -> list[str]:\n",
        "    \"\"\"Small test query: Madrid bbox + year.\n",
        "\n",
        "    Uses a GEOMETRY WKT polygon (pygbif parses this string; avoids WithinPredicate JSON issues).\n",
        "    \"\"\"\n",
        "    south = float(bbox[\"min_lat\"])\n",
        "    west = float(bbox[\"min_lon\"])\n",
        "    north = float(bbox[\"max_lat\"])\n",
        "    east = float(bbox[\"max_lon\"])\n",
        "\n",
        "    wkt = f\"POLYGON(({west} {south}, {east} {south}, {east} {north}, {west} {north}, {west} {south}))\"\n",
        "    return [\n",
        "        f\"country = {country}\",\n",
        "        f\"year = {year}\",\n",
        "        \"hasCoordinate = TRUE\",\n",
        "        f\"geometry = {wkt}\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _normalize_download_key(x) -> str:\n",
        "    \"\"\"pygbif may return a key string or a structured object; normalize to a string key.\"\"\"\n",
        "    if x is None:\n",
        "        raise ValueError(\"Download submission returned None\")\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    # Sometimes a tuple/list is returned\n",
        "    if isinstance(x, (tuple, list)):\n",
        "        if not x:\n",
        "            raise ValueError(\"Download submission returned an empty tuple/list\")\n",
        "        # first element is typically the download key\n",
        "        if isinstance(x[0], str):\n",
        "            return x[0]\n",
        "        raise ValueError(f\"Unexpected download return tuple/list: {x!r}\")\n",
        "    # Sometimes a dict-like structure is returned\n",
        "    if isinstance(x, dict):\n",
        "        for k in (\"key\", \"downloadKey\", \"download_key\"):\n",
        "            if k in x and isinstance(x[k], str):\n",
        "                return x[k]\n",
        "        raise ValueError(f\"Unexpected download return dict (no key field): {x!r}\")\n",
        "    raise ValueError(f\"Unexpected download return type: {type(x)} -> {x!r}\")\n",
        "\n",
        "\n",
        "def submit_gbif_download(queries: list[str], label: str) -> str:\n",
        "    \"\"\"Submit a GBIF occurrence download job and return the download key.\"\"\"\n",
        "    if not (GBIF_USER and GBIF_PWD and GBIF_EMAIL):\n",
        "        raise RuntimeError(\"Missing GBIF credentials (GBIF_USER/GBIF_PWD/GBIF_EMAIL)\")\n",
        "\n",
        "    # IMPORTANT: occurrences.download signature is (queries, format=..., user=..., pwd=..., email=...)\n",
        "    print(f\"DOWNLOAD_FORMAT is {DOWNLOAD_FORMAT}\")\n",
        "    resp = occurrences.download(\n",
        "        queries,\n",
        "        format=DOWNLOAD_FORMAT,\n",
        "        user=GBIF_USER,\n",
        "        pwd=GBIF_PWD,\n",
        "        email=GBIF_EMAIL,\n",
        "        pred_type=\"and\",\n",
        "    )\n",
        "    key = _normalize_download_key(resp)\n",
        "    print(\"Submitted:\", label, \"->\", key)\n",
        "    return key\n",
        "\n",
        "\n",
        "def wait_for_download(key: str, poll_s: int = 30, timeout_s: int = 6 * 3600) -> dict:\n",
        "    \"\"\"Poll GBIF until the download is ready or fails.\"\"\"\n",
        "    t0 = time.time()\n",
        "    while True:\n",
        "        meta = occurrences.download_meta(key)\n",
        "        status = (meta or {}).get(\"status\")\n",
        "        print(\"Status:\", key, status)\n",
        "\n",
        "        if status in {\"SUCCEEDED\", \"KILLED\", \"CANCELLED\", \"FAILED\"}:\n",
        "            return meta\n",
        "\n",
        "        if time.time() - t0 > timeout_s:\n",
        "            raise TimeoutError(f\"Timeout waiting for download {key}\")\n",
        "\n",
        "        time.sleep(poll_s)\n",
        "\n",
        "\n",
        "if not (GBIF_USER and GBIF_PWD and GBIF_EMAIL):\n",
        "    print(\"GBIF credentials not found in env vars (GBIF_USER/GBIF_PWD/GBIF_EMAIL).\")\n",
        "    print(\"Predicate building will still work; set env vars + SUBMIT_JOBS=True to submit.\")\n",
        "\n",
        "\n",
        "if DRY_RUN:\n",
        "    # --- Single small test job: Madrid bbox\n",
        "    # Define a small bbox for testing (edit as needed)\n",
        "    BBOX = {\n",
        "        \"min_lat\": 40.20,\n",
        "        \"min_lon\": -3.90,\n",
        "        \"max_lat\": 40.60,\n",
        "        \"max_lon\": -3.50,\n",
        "    }\n",
        "\n",
        "    TEST_YEAR = 2024\n",
        "    queries = gbif_queries_madrid_bbox_year(BBOX, TEST_YEAR)\n",
        "    label = f\"DRYRUN_MADRID_{TEST_YEAR}\"\n",
        "\n",
        "    print(\"DRY_RUN enabled -> single Madrid job\")\n",
        "    print(\"Queries:\")\n",
        "    for q in queries:\n",
        "        print(\"-\", q)\n",
        "\n",
        "    if SUBMIT_JOBS:\n",
        "        key = submit_gbif_download(queries, label)\n",
        "        meta = wait_for_download(key)\n",
        "        print(\"Final status:\", meta.get(\"status\"))\n",
        "\n",
        "        if DOWNLOAD_ON_SUCCESS and (meta or {}).get(\"status\") == \"SUCCEEDED\":\n",
        "            import os\n",
        "            import zipfile\n",
        "            from pathlib import Path\n",
        "\n",
        "            DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "            # downloads {key}.zip into DOWNLOAD_DIR\n",
        "            zip_path = occurrences.download_get(key, path=str(DOWNLOAD_DIR))\n",
        "            print(\"Downloaded archive:\", zip_path)\n",
        "\n",
        "            if EXTRACT_AFTER_DOWNLOAD:\n",
        "                extract_root = DOWNLOAD_DIR / \"_extracted\" / key\n",
        "                extract_root.mkdir(parents=True, exist_ok=True)\n",
        "                with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "                    z.extractall(extract_root)\n",
        "                print(\"Extracted to:\", extract_root)\n",
        "else:\n",
        "    # --- Full plan: one job per (country, year) using COUNTRIES + YEARS from config cell\n",
        "    # YEARS is built in config cell: [YEAR] or range(YEAR_START, YEAR_END+1)\n",
        "    job_plan = [(c, y) for c in COUNTRIES for y in YEARS]\n",
        "    print(\"DRY_RUN disabled -> submit jobs for COUNTRIES + YEARS from config\")\n",
        "    print(\"COUNTRIES:\", COUNTRIES)\n",
        "    print(\"YEARS:\", YEARS)\n",
        "    print(\"Planned jobs:\", len(job_plan), \"->\", job_plan)\n",
        "    print(\"Example queries (first job):\")\n",
        "    for q in gbif_queries_country_year(COUNTRIES[0], YEARS[0]):\n",
        "        print(\"-\", q)\n",
        "\n",
        "    if SUBMIT_JOBS:\n",
        "        keys = {}\n",
        "        for country, y in job_plan:\n",
        "            queries = gbif_queries_country_year(country, y)\n",
        "            label = f\"{country}_YEAR_{y}\"\n",
        "            try:\n",
        "                keys[label] = submit_gbif_download(queries, label)\n",
        "            except Exception as e:\n",
        "                print(\"Submit failed:\", label, e)\n",
        "                keys[label] = None\n",
        "\n",
        "        print(\"Submitted keys:\")\n",
        "        print(keys)\n",
        "        print(\"-> Copy the dict above to DOWNLOAD_KEYS in the status cell and download cell.\")\n",
        "\n",
        "        if WAIT_AND_DOWNLOAD:\n",
        "            import zipfile\n",
        "\n",
        "            for label, key in keys.items():\n",
        "                if not key:\n",
        "                    continue\n",
        "                print(\"\\nWaiting for:\", label, key)\n",
        "                meta = wait_for_download(key)\n",
        "                print(\"Final status:\", meta.get(\"status\"))\n",
        "\n",
        "                if DOWNLOAD_ON_SUCCESS and (meta or {}).get(\"status\") == \"SUCCEEDED\":\n",
        "                    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "                    zip_path = occurrences.download_get(key, path=str(DOWNLOAD_DIR))\n",
        "                    print(\"Downloaded archive:\", zip_path)\n",
        "\n",
        "                    if EXTRACT_AFTER_DOWNLOAD:\n",
        "                        extract_root = DOWNLOAD_DIR / \"_extracted\" / key\n",
        "                        extract_root.mkdir(parents=True, exist_ok=True)\n",
        "                        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "                            z.extractall(extract_root)\n",
        "                        print(\"Extracted to:\", extract_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "84093743",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PT_YEAR_2024 (0012599-260208012135463) -> status=RUNNING created=2026-02-12T21:41:26.531+00:00 modified=2026-02-12T21:42:59.703+00:00 totalRecords=1941250 size=0\n",
            "ES_YEAR_2024 (0012600-260208012135463) -> status=RUNNING created=2026-02-12T21:41:28.031+00:00 modified=2026-02-12T21:43:10.136+00:00 totalRecords=7414511 size=0\n",
            "FR_YEAR_2024 (0012601-260208012135463) -> status=RUNNING created=2026-02-12T21:41:29.549+00:00 modified=2026-02-12T21:45:02.161+00:00 totalRecords=2688919 size=0\n"
          ]
        }
      ],
      "source": [
        "from pygbif import occurrences\n",
        "\n",
        "# Paste output from submit cell (print(keys)) - dict {label: key} or list of keys\n",
        "DOWNLOAD_KEYS = {\n",
        "    \"PT_YEAR_2024\": \"0012599-260208012135463\",\n",
        "    \"ES_YEAR_2024\": \"0012600-260208012135463\",\n",
        "    \"FR_YEAR_2024\": \"0012601-260208012135463\",\n",
        "}\n",
        "\n",
        "\n",
        "def _iter_keys(keys):\n",
        "    \"\"\"Yield (label, key) from dict or list.\"\"\"\n",
        "    if isinstance(keys, dict):\n",
        "        for label, k in keys.items():\n",
        "            yield (label, k)\n",
        "    else:\n",
        "        for k in keys:\n",
        "            yield (k, k)\n",
        "\n",
        "\n",
        "def print_download_status(keys) -> None:\n",
        "    for label, k in _iter_keys(keys):\n",
        "        try:\n",
        "            meta = occurrences.download_meta(k) or {}\n",
        "            status = meta.get(\"status\")\n",
        "            created = meta.get(\"created\")\n",
        "            modified = meta.get(\"modified\")\n",
        "            size = meta.get(\"size\")\n",
        "            total = meta.get(\"totalRecords\") or meta.get(\"total\")\n",
        "            print(f\"{label} ({k}) -> status={status} created={created} modified={modified} totalRecords={total} size={size}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{label} ({k}) -> ERROR: {e}\")\n",
        "\n",
        "\n",
        "print_download_status(DOWNLOAD_KEYS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "313f196b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download key: 0006909-260208012135463\n",
            "Direct URL: https://api.gbif.org/v1/occurrence/download/request/0006909-260208012135463.zip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:Download file size: 497252637 bytes\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: SUCCEEDED\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStatus:\u001b[39m\u001b[33m\"\u001b[39m, (meta \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2) Download the ZIP archive (no credentials required)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m zip_path = \u001b[43moccurrences\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOWNLOAD_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved ZIP:\u001b[39m\u001b[33m\"\u001b[39m, zip_path)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 3) Optional: list ZIP contents\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/pygbif/occurrences/download.py:624\u001b[39m, in \u001b[36mdownload_get\u001b[39m\u001b[34m(key, path, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttp://api.gbif.org/v1/occurrence/download/request/\u001b[39m\u001b[33m\"\u001b[39m + key\n\u001b[32m    623\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.zip\u001b[39m\u001b[33m\"\u001b[39m % (path, key)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[43mgbif_GET_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mOn disk at \u001b[39m\u001b[33m\"\u001b[39m + path)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: path, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m: meta[\u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: key}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/pygbif/gbifutils.py:54\u001b[39m, in \u001b[36mgbif_GET_write\u001b[39m\u001b[34m(url, path, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/urllib3/response.py:1250\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1235\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1236\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1247\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[32m   1253\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp)\n\u001b[32m   1254\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1256\u001b[39m     ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/urllib3/response.py:1421\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1420\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m     chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m._decode(\n\u001b[32m   1423\u001b[39m     chunk,\n\u001b[32m   1424\u001b[39m     decode_content=decode_content,\n\u001b[32m   1425\u001b[39m     flush_decoder=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1426\u001b[39m     max_length=amt,\n\u001b[32m   1427\u001b[39m )\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/urllib3/response.py:1354\u001b[39m, in \u001b[36mHTTPResponse._handle_chunk\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m   1352\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt < \u001b[38;5;28mself\u001b[39m.chunk_left:\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1355\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28mself\u001b[39m.chunk_left - amt\n\u001b[32m   1356\u001b[39m     returned_chunk = value\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/lib/python3.12/http/client.py:640\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[32m    634\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[32m    635\u001b[39m \n\u001b[32m    636\u001b[39m \u001b[33;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[33;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m    642\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "from pygbif import occurrences\n",
        "\n",
        "\n",
        "# Paste same dict as in status cell: {label: key} from submit output\n",
        "DOWNLOAD_KEYS = {\n",
        "    \"PT_YEAR_2024\": \"0012599-260208012135463\",\n",
        "    \"ES_YEAR_2024\": \"0012600-260208012135463\",\n",
        "    \"FR_YEAR_2024\": \"0012601-260208012135463\",\n",
        "}\n",
        "\n",
        "\n",
        "def _iter_keys(keys):\n",
        "    \"\"\"Yield (label, key) from dict or list.\"\"\"\n",
        "    if isinstance(keys, dict):\n",
        "        for label, k in keys.items():\n",
        "            yield (label, k)\n",
        "    else:\n",
        "        for k in keys:\n",
        "            yield (k, k)\n",
        "\n",
        "\n",
        "OUT_DIR = DOWNLOAD_DIR\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for label, key in _iter_keys(DOWNLOAD_KEYS):\n",
        "    print(f\"\\n--- {label} ({key}) ---\")\n",
        "    try:\n",
        "        meta = occurrences.download_meta(key) or {}\n",
        "        status = meta.get(\"status\")\n",
        "        print(\"Status:\", status)\n",
        "\n",
        "        if status != \"SUCCEEDED\":\n",
        "            print(f\"Skipping (status={status})\")\n",
        "            continue\n",
        "\n",
        "        zip_path = occurrences.download_get(key, path=str(OUT_DIR))\n",
        "        print(\"Saved ZIP:\", zip_path)\n",
        "\n",
        "        if EXTRACT_AFTER_DOWNLOAD:\n",
        "            extract_dir = OUT_DIR / \"_extracted\" / key\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "                z.extractall(extract_dir)\n",
        "            print(\"Extracted to:\", extract_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2053a48a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---\n",
            "Key: 0006909-260208012135463\n",
            "Status before: SUCCEEDED\n",
            "Cancel response: True\n",
            "Status after: SUCCEEDED\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pygbif import occurrences\n",
        "\n",
        "# Cancel downloads (requires GBIF login)\n",
        "GBIF_USER = os.getenv(\"GBIF_USER\")\n",
        "GBIF_PWD = os.getenv(\"GBIF_PWD\")\n",
        "\n",
        "if not (GBIF_USER and GBIF_PWD):\n",
        "    print(\"Missing GBIF_USER/GBIF_PWD env vars -> cannot cancel downloads via API.\")\n",
        "\n",
        "# Put download keys you want to cancel here\n",
        "CANCEL_KEYS = [\n",
        "    \"0006909-260208012135463\",\n",
        "]\n",
        "\n",
        "# Safety switch\n",
        "DO_CANCEL = True\n",
        "\n",
        "for k in CANCEL_KEYS:\n",
        "    print(\"\\n---\")\n",
        "    print(\"Key:\", k)\n",
        "\n",
        "    try:\n",
        "        meta_before = occurrences.download_meta(k) or {}\n",
        "        print(\"Status before:\", meta_before.get(\"status\"))\n",
        "    except Exception as e:\n",
        "        print(\"Could not fetch meta before cancel:\", e)\n",
        "        meta_before = {}\n",
        "\n",
        "    if not DO_CANCEL:\n",
        "        print(\"DO_CANCEL is False -> skipping cancel for\", k)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        resp = occurrences.download_cancel(k, user=GBIF_USER, pwd=GBIF_PWD)\n",
        "        print(\"Cancel response:\", resp)\n",
        "    except Exception as e:\n",
        "        print(\"Cancel failed:\", e)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        meta_after = occurrences.download_meta(k) or {}\n",
        "        print(\"Status after:\", meta_after.get(\"status\"))\n",
        "    except Exception as e:\n",
        "        print(\"Could not fetch meta after cancel:\", e)\n",
        "\n",
        "print(\"\\nDone.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
