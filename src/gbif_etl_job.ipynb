{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GBIF ETL Job\n",
        "\n",
        "Downloads GBIF occurrence data per country × year, enriches it with:\n",
        "- **Threatened species** (`iucn_cat`: VU / EN / CR) via GBIF search API facet\n",
        "- **Invasive / introduced status** from both the export columns (`establishmentMeans`, `degreeOfEstablishment`) and GBIF search API\n",
        "\n",
        "Writes partitioned Parquet files to S3:\n",
        "```\n",
        "s3://ie-datalake/bronze/gbif/country=ES/year=2024/part-XXXXX.parquet\n",
        "```\n",
        "\n",
        "Country and year are **also stored as regular columns** inside each file.\n",
        "\n",
        "### Flow\n",
        "1. Build job plan: `(country, year)` pairs for all COUNTRIES × YEARS\n",
        "2. Submit GBIF download jobs in batches of up to `MAX_CONCURRENT_JOBS`\n",
        "3. Poll each job until `SUCCEEDED`\n",
        "4. Download ZIP, extract, read Parquet files\n",
        "5. Enrich with threatened + invasive metadata\n",
        "6. Write to S3 (partitioned, snappy-compressed)\n",
        "7. Cleanup local temp files\n",
        "\n",
        "### Requirements\n",
        "```\n",
        "pip install pygbif boto3 s3fs pyarrow pandas python-dotenv\n",
        "```\n",
        "\n",
        "### Credentials\n",
        "Set in `.env` (or shell env vars):\n",
        "```\n",
        "GBIF_USER=...\n",
        "GBIF_PWD=...\n",
        "GBIF_EMAIL=...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "14:27:35 [INFO] Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CONFIGURATION – edit these parameters before running\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Countries to process (ISO-2 codes)\n",
        "# COUNTRIES: list[str] = [\"ES\", \"PT\", \"FR\"]\n",
        "COUNTRIES: list[str] = [\"PT\"]\n",
        "\n",
        "# Year range (inclusive)\n",
        "YEAR_START: int = 2010\n",
        "YEAR_END: int = 2026\n",
        "\n",
        "# S3 destination\n",
        "S3_BUCKET: str = \"ie-datalake\"\n",
        "S3_PREFIX: str = \"bronze/gbif\"  # files land at s3://S3_BUCKET/S3_PREFIX/country=XX/year=YYYY/\n",
        "AWS_PROFILE: str = \"486717354268_PowerUserAccess\"\n",
        "\n",
        "import s3fs\n",
        "s3fs.S3FileSystem.clear_instance_cache()\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "\n",
        "# GBIF download settings\n",
        "DOWNLOAD_FORMAT: str = \"SIMPLE_PARQUET\"  # SIMPLE_PARQUET gives Parquet files directly in the ZIP\n",
        "MAX_CONCURRENT_JOBS: int = 3  # how many GBIF download jobs to submit per batch\n",
        "\n",
        "# Polling / timeouts\n",
        "GBIF_POLL_INTERVAL_S: int = 30\n",
        "GBIF_TIMEOUT_S: int = 6 * 3600  # 6 hours max per job\n",
        "\n",
        "# Local temp dir (cleaned up after each job)\n",
        "from pathlib import Path\n",
        "LOCAL_TEMP_DIR = Path(\"data/etl_temp\")\n",
        "\n",
        "# IUCN threatened categories to enrich\n",
        "THREATENED_CATS: list[str] = [\"VU\", \"EN\", \"CR\"]\n",
        "\n",
        "# GBIF facet page size for threatened / invasive species lookups\n",
        "FACET_LIMIT: int = 1_000\n",
        "\n",
        "# Parquet write settings\n",
        "PARQUET_ROW_GROUP_SIZE: int = 250_000  # rows per row group\n",
        "PARQUET_COMPRESSION: str = \"snappy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "14:27:53 [INFO] S3FileSystem initialized (profile=486717354268_PowerUserAccess)\n",
            "14:27:53 [INFO] Job plan: 17 total jobs, 6 batch(es) of max 3\n",
            "14:27:53 [INFO] \n",
            "════════════════════════════════════════════════════════════\n",
            "14:27:53 [INFO] Batch 1/6: [('PT', 2026), ('PT', 2025), ('PT', 2024)]\n",
            "14:27:53 [INFO] ════════════════════════════════════════════════════════════\n",
            "\n",
            "14:27:54 [INFO] Your download key is 0033827-260208012135463\n",
            "14:27:54 [INFO] Submitted: country=PT year=2026 -> 0033827-260208012135463\n",
            "14:27:54 [INFO] Your download key is 0033828-260208012135463\n",
            "14:27:54 [INFO] Submitted: country=PT year=2025 -> 0033828-260208012135463\n",
            "14:27:54 [INFO] Your download key is 0033195-260208012135463\n",
            "14:27:54 [INFO] Submitted: country=PT year=2024 -> 0033195-260208012135463\n",
            "14:27:54 [INFO] Polling PT/2026 (0033827-260208012135463) -> SUCCEEDED (0 min)\n",
            "14:27:54 [INFO] SUCCEEDED: PT/2026 | records=22710 size=7.2 MB – starting ETL\n",
            "14:27:54 [INFO] Downloading ZIP for 0033827-260208012135463 …\n",
            "14:27:54 [INFO] Download file size: 7236108 bytes\n",
            "14:27:55 [INFO] On disk at data/etl_temp/PT_2026_0033827-260208012135463/zips/0033827-260208012135463.zip\n",
            "14:27:55 [INFO] ZIP saved: 0033827-260208012135463.zip (7.2 MB)\n",
            "14:27:55 [INFO] Extracted to: data/etl_temp/PT_2026_0033827-260208012135463/extracted/0033827-260208012135463\n",
            "14:27:55 [INFO] ZIP contents (901 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/000125', 'occurrence.parquet/000317', 'occurrence.parquet/000773', 'occurrence.parquet/000541', 'occurrence.parquet/000328', 'occurrence.parquet/000546', 'occurrence.parquet/000774', 'occurrence.parquet/000310', 'occurrence.parquet/000122', 'occurrence.parquet/000780', 'occurrence.parquet/000579', 'occurrence.parquet/000326', 'occurrence.parquet/000114', 'occurrence.parquet/000570', 'occurrence.parquet/000742', 'occurrence.parquet/000584', 'occurrence.parquet/000319', 'occurrence.parquet/000789']\n",
            "14:27:55 [INFO] Reading occurrence.parquet/ directory: 899 non-empty part file(s) (skipped 1 empty)\n",
            "14:27:56 [INFO] Read 22710 rows, 50 columns from occurrence.parquet/\n",
            "14:27:56 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:27:56 [INFO] Fetching threatened species (VU/EN/CR) for PT via GBIF search API …\n",
            "14:27:57 [INFO] Threatened species for PT: 686 speciesKeys total\n",
            "14:27:57 [INFO] Fetching invasive/introduced speciesKeys for PT via GBIF search API …\n",
            "14:27:58 [INFO] Invasive API keys for PT: degreeOfEstablishment=invasive -> 64, establishmentMeans=introduced -> 3615\n",
            "14:27:58 [INFO] Enrichment for PT/2026: threatened=549, invasive_em=0, invasive_doe=0, invasive_api=683, invasive_any=683\n",
            "14:27:58 [INFO] Writing 22710 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2026 …\n",
            "14:28:02 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2026\n",
            "14:28:02 [INFO] ✓ Done: PT/2026 -> s3://ie-datalake/bronze/gbif/country=PT/year=2026 (22710 rows)\n",
            "14:28:02 [INFO] Cleaned up temp dir: data/etl_temp/PT_2026_0033827-260208012135463\n",
            "14:28:02 [INFO] Polling PT/2025 (0033828-260208012135463) -> SUCCEEDED (0 min)\n",
            "14:28:02 [INFO] SUCCEEDED: PT/2025 | records=336391 size=50.6 MB – starting ETL\n",
            "14:28:02 [INFO] Downloading ZIP for 0033828-260208012135463 …\n",
            "14:28:02 [INFO] Download file size: 50636719 bytes\n",
            "14:28:11 [INFO] On disk at data/etl_temp/PT_2025_0033828-260208012135463/zips/0033828-260208012135463.zip\n",
            "14:28:11 [INFO] ZIP saved: 0033828-260208012135463.zip (50.6 MB)\n",
            "14:28:12 [INFO] Extracted to: data/etl_temp/PT_2025_0033828-260208012135463/extracted/0033828-260208012135463\n",
            "14:28:12 [INFO] ZIP contents (1103 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001003', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/000317', 'occurrence.parquet/000773', 'occurrence.parquet/000541', 'occurrence.parquet/000328', 'occurrence.parquet/000926', 'occurrence.parquet/000546', 'occurrence.parquet/000774', 'occurrence.parquet/000310', 'occurrence.parquet/000122', 'occurrence.parquet/001004', 'occurrence.parquet/000780', 'occurrence.parquet/000921', 'occurrence.parquet/000579', 'occurrence.parquet/000326', 'occurrence.parquet/000114']\n",
            "14:28:12 [INFO] Reading occurrence.parquet/ directory: 1101 non-empty part file(s) (skipped 1 empty)\n",
            "14:28:13 [INFO] Read 336391 rows, 50 columns from occurrence.parquet/\n",
            "14:28:13 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:28:13 [INFO] Enrichment for PT/2025: threatened=8288, invasive_em=0, invasive_doe=0, invasive_api=12849, invasive_any=12849\n",
            "14:28:13 [INFO] Writing 336391 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2025 …\n",
            "14:28:23 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2025\n",
            "14:28:23 [INFO] ✓ Done: PT/2025 -> s3://ie-datalake/bronze/gbif/country=PT/year=2025 (336391 rows)\n",
            "14:28:23 [INFO] Cleaned up temp dir: data/etl_temp/PT_2025_0033828-260208012135463\n",
            "14:28:23 [INFO] Polling PT/2024 (0033195-260208012135463) -> SUCCEEDED (0 min)\n",
            "14:28:23 [INFO] SUCCEEDED: PT/2024 | records=1942667 size=216.8 MB – starting ETL\n",
            "14:28:23 [INFO] Downloading ZIP for 0033195-260208012135463 …\n",
            "14:28:23 [INFO] Download file size: 216848800 bytes\n",
            "14:28:30 [INFO] On disk at data/etl_temp/PT_2024_0033195-260208012135463/zips/0033195-260208012135463.zip\n",
            "14:28:30 [INFO] ZIP saved: 0033195-260208012135463.zip (216.8 MB)\n",
            "14:28:33 [INFO] Extracted to: data/etl_temp/PT_2024_0033195-260208012135463/extracted/0033195-260208012135463\n",
            "14:28:33 [INFO] ZIP contents (4850 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585', 'occurrence.parquet/000317']\n",
            "14:28:33 [INFO] Reading occurrence.parquet/ directory: 4848 non-empty part file(s) (skipped 1 empty)\n",
            "14:28:39 [INFO] Read 1942667 rows, 50 columns from occurrence.parquet/\n",
            "14:28:40 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:28:42 [INFO] Enrichment for PT/2024: threatened=28378, invasive_em=0, invasive_doe=0, invasive_api=11609, invasive_any=11609\n",
            "14:28:42 [INFO] Writing 1942667 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2024 …\n",
            "14:29:18 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2024\n",
            "14:29:18 [INFO] ✓ Done: PT/2024 -> s3://ie-datalake/bronze/gbif/country=PT/year=2024 (1942667 rows)\n",
            "14:29:18 [INFO] Cleaned up temp dir: data/etl_temp/PT_2024_0033195-260208012135463\n",
            "14:29:20 [INFO] Batch 1/6 complete. Running totals: 3 succeeded, 0 failed.\n",
            "14:29:20 [INFO] \n",
            "════════════════════════════════════════════════════════════\n",
            "14:29:20 [INFO] Batch 2/6: [('PT', 2023), ('PT', 2022), ('PT', 2021)]\n",
            "14:29:20 [INFO] ════════════════════════════════════════════════════════════\n",
            "\n",
            "14:29:20 [INFO] Your download key is 0033847-260208012135463\n",
            "14:29:20 [INFO] Submitted: country=PT year=2023 -> 0033847-260208012135463\n",
            "14:29:21 [INFO] Your download key is 0033848-260208012135463\n",
            "14:29:21 [INFO] Submitted: country=PT year=2022 -> 0033848-260208012135463\n",
            "14:29:21 [INFO] Your download key is 0033849-260208012135463\n",
            "14:29:21 [INFO] Submitted: country=PT year=2021 -> 0033849-260208012135463\n",
            "14:29:21 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (0 min)\n",
            "14:29:22 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (0 min)\n",
            "14:29:22 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (0 min)\n",
            "14:29:52 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (1 min)\n",
            "14:29:52 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (1 min)\n",
            "14:29:52 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (1 min)\n",
            "14:30:22 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (1 min)\n",
            "14:30:22 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (1 min)\n",
            "14:30:22 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (1 min)\n",
            "14:30:53 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (2 min)\n",
            "14:30:53 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (2 min)\n",
            "14:30:53 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (2 min)\n",
            "14:31:23 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (2 min)\n",
            "14:31:23 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (2 min)\n",
            "14:31:23 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (2 min)\n",
            "14:31:54 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (3 min)\n",
            "14:31:54 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (3 min)\n",
            "14:31:54 [INFO] Polling PT/2021 (0033849-260208012135463) -> RUNNING (3 min)\n",
            "14:32:24 [INFO] Polling PT/2023 (0033847-260208012135463) -> RUNNING (3 min)\n",
            "14:32:24 [INFO] Polling PT/2022 (0033848-260208012135463) -> RUNNING (3 min)\n",
            "14:32:24 [INFO] Polling PT/2021 (0033849-260208012135463) -> SUCCEEDED (3 min)\n",
            "14:32:24 [INFO] SUCCEEDED: PT/2021 | records=1629388 size=179.7 MB – starting ETL\n",
            "14:32:25 [INFO] Downloading ZIP for 0033849-260208012135463 …\n",
            "14:32:25 [INFO] Download file size: 179678948 bytes\n",
            "14:32:31 [INFO] On disk at data/etl_temp/PT_2021_0033849-260208012135463/zips/0033849-260208012135463.zip\n",
            "14:32:31 [INFO] ZIP saved: 0033849-260208012135463.zip (179.7 MB)\n",
            "14:32:33 [INFO] Extracted to: data/etl_temp/PT_2021_0033849-260208012135463/extracted/0033849-260208012135463\n",
            "14:32:33 [INFO] ZIP contents (4876 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585', 'occurrence.parquet/000317']\n",
            "14:32:33 [INFO] Reading occurrence.parquet/ directory: 4874 non-empty part file(s) (skipped 1 empty)\n",
            "14:32:39 [INFO] Read 1629388 rows, 50 columns from occurrence.parquet/\n",
            "14:32:40 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:32:42 [INFO] Enrichment for PT/2021: threatened=25465, invasive_em=0, invasive_doe=0, invasive_api=9226, invasive_any=9226\n",
            "14:32:42 [INFO] Writing 1629388 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2021 …\n",
            "14:33:15 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2021\n",
            "14:33:15 [INFO] ✓ Done: PT/2021 -> s3://ie-datalake/bronze/gbif/country=PT/year=2021 (1629388 rows)\n",
            "14:33:15 [INFO] Cleaned up temp dir: data/etl_temp/PT_2021_0033849-260208012135463\n",
            "14:33:46 [INFO] Polling PT/2023 (0033847-260208012135463) -> SUCCEEDED (4 min)\n",
            "14:33:46 [INFO] SUCCEEDED: PT/2023 | records=1911867 size=210.5 MB – starting ETL\n",
            "14:33:46 [INFO] Downloading ZIP for 0033847-260208012135463 …\n",
            "14:33:46 [INFO] Download file size: 210523074 bytes\n",
            "14:33:53 [INFO] On disk at data/etl_temp/PT_2023_0033847-260208012135463/zips/0033847-260208012135463.zip\n",
            "14:33:53 [INFO] ZIP saved: 0033847-260208012135463.zip (210.5 MB)\n",
            "14:33:56 [INFO] Extracted to: data/etl_temp/PT_2023_0033847-260208012135463/extracted/0033847-260208012135463\n",
            "14:33:56 [INFO] ZIP contents (4935 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "14:33:56 [INFO] Reading occurrence.parquet/ directory: 4933 non-empty part file(s) (skipped 1 empty)\n",
            "14:34:01 [INFO] Read 1911867 rows, 50 columns from occurrence.parquet/\n",
            "14:34:03 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:34:05 [INFO] Enrichment for PT/2023: threatened=31142, invasive_em=0, invasive_doe=0, invasive_api=9398, invasive_any=9398\n",
            "14:34:05 [INFO] Writing 1911867 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2023 …\n",
            "14:34:43 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2023\n",
            "14:34:43 [INFO] ✓ Done: PT/2023 -> s3://ie-datalake/bronze/gbif/country=PT/year=2023 (1911867 rows)\n",
            "14:34:43 [INFO] Cleaned up temp dir: data/etl_temp/PT_2023_0033847-260208012135463\n",
            "14:34:44 [INFO] Polling PT/2022 (0033848-260208012135463) -> SUCCEEDED (5 min)\n",
            "14:34:44 [INFO] SUCCEEDED: PT/2022 | records=1847577 size=200.5 MB – starting ETL\n",
            "14:34:44 [INFO] Downloading ZIP for 0033848-260208012135463 …\n",
            "14:34:44 [INFO] Download file size: 200452926 bytes\n",
            "14:34:51 [INFO] On disk at data/etl_temp/PT_2022_0033848-260208012135463/zips/0033848-260208012135463.zip\n",
            "14:34:51 [INFO] ZIP saved: 0033848-260208012135463.zip (200.5 MB)\n",
            "14:34:54 [INFO] Extracted to: data/etl_temp/PT_2022_0033848-260208012135463/extracted/0033848-260208012135463\n",
            "14:34:54 [INFO] ZIP contents (5019 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "14:34:54 [INFO] Reading occurrence.parquet/ directory: 5017 non-empty part file(s) (skipped 1 empty)\n",
            "14:35:00 [INFO] Read 1847577 rows, 50 columns from occurrence.parquet/\n",
            "14:35:02 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:35:04 [INFO] Enrichment for PT/2022: threatened=30389, invasive_em=0, invasive_doe=0, invasive_api=13947, invasive_any=13947\n",
            "14:35:04 [INFO] Writing 1847577 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2022 …\n",
            "14:35:40 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2022\n",
            "14:35:40 [INFO] ✓ Done: PT/2022 -> s3://ie-datalake/bronze/gbif/country=PT/year=2022 (1847577 rows)\n",
            "14:35:40 [INFO] Cleaned up temp dir: data/etl_temp/PT_2022_0033848-260208012135463\n",
            "14:35:42 [INFO] Batch 2/6 complete. Running totals: 6 succeeded, 0 failed.\n",
            "14:35:42 [INFO] \n",
            "════════════════════════════════════════════════════════════\n",
            "14:35:42 [INFO] Batch 3/6: [('PT', 2020), ('PT', 2019), ('PT', 2018)]\n",
            "14:35:42 [INFO] ════════════════════════════════════════════════════════════\n",
            "\n",
            "14:35:43 [INFO] Your download key is 0033866-260208012135463\n",
            "14:35:43 [INFO] Submitted: country=PT year=2020 -> 0033866-260208012135463\n",
            "14:35:45 [INFO] Your download key is 0033867-260208012135463\n",
            "14:35:45 [INFO] Submitted: country=PT year=2019 -> 0033867-260208012135463\n",
            "14:35:46 [INFO] Your download key is 0033868-260208012135463\n",
            "14:35:46 [INFO] Submitted: country=PT year=2018 -> 0033868-260208012135463\n",
            "14:35:46 [INFO] Polling PT/2020 (0033866-260208012135463) -> PREPARING (0 min)\n",
            "14:35:47 [INFO] Polling PT/2019 (0033867-260208012135463) -> PREPARING (0 min)\n",
            "14:35:47 [INFO] Polling PT/2018 (0033868-260208012135463) -> PREPARING (0 min)\n",
            "14:36:17 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (1 min)\n",
            "14:36:17 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (1 min)\n",
            "14:36:17 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (1 min)\n",
            "14:36:47 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (1 min)\n",
            "14:36:47 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (1 min)\n",
            "14:36:47 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (1 min)\n",
            "14:37:18 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (2 min)\n",
            "14:37:18 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (2 min)\n",
            "14:37:18 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (2 min)\n",
            "14:37:48 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (2 min)\n",
            "14:37:48 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (2 min)\n",
            "14:37:48 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (2 min)\n",
            "14:38:18 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (3 min)\n",
            "14:38:19 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (3 min)\n",
            "14:38:19 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (3 min)\n",
            "14:38:49 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (3 min)\n",
            "14:38:49 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (3 min)\n",
            "14:38:49 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (3 min)\n",
            "14:39:19 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (4 min)\n",
            "14:39:19 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (4 min)\n",
            "14:39:19 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (4 min)\n",
            "14:39:50 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (4 min)\n",
            "14:39:50 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (4 min)\n",
            "14:39:50 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (4 min)\n",
            "14:40:20 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (5 min)\n",
            "14:40:20 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (5 min)\n",
            "14:40:20 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (5 min)\n",
            "14:40:50 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (5 min)\n",
            "14:40:50 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (5 min)\n",
            "14:40:51 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (5 min)\n",
            "14:41:21 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (6 min)\n",
            "14:41:21 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (6 min)\n",
            "14:41:21 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (6 min)\n",
            "14:41:51 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (6 min)\n",
            "14:41:51 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (6 min)\n",
            "14:41:51 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (6 min)\n",
            "14:42:21 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (7 min)\n",
            "14:42:21 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (7 min)\n",
            "14:42:22 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (7 min)\n",
            "14:42:52 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (7 min)\n",
            "14:42:52 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (7 min)\n",
            "14:42:52 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (7 min)\n",
            "14:43:22 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (8 min)\n",
            "14:43:22 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (8 min)\n",
            "14:43:22 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (8 min)\n",
            "14:43:52 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (8 min)\n",
            "14:43:53 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (8 min)\n",
            "14:43:53 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (8 min)\n",
            "14:44:23 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (9 min)\n",
            "14:44:23 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (9 min)\n",
            "14:44:23 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (9 min)\n",
            "14:44:53 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (9 min)\n",
            "14:44:53 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (9 min)\n",
            "14:44:53 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (9 min)\n",
            "14:45:24 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (10 min)\n",
            "14:45:24 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (10 min)\n",
            "14:45:24 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (10 min)\n",
            "14:45:54 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (10 min)\n",
            "14:45:54 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (10 min)\n",
            "14:45:54 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (10 min)\n",
            "14:46:25 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (11 min)\n",
            "14:46:25 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (11 min)\n",
            "14:46:25 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (11 min)\n",
            "14:46:55 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (11 min)\n",
            "14:46:55 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (11 min)\n",
            "14:46:55 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (11 min)\n",
            "14:47:25 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (12 min)\n",
            "14:47:25 [INFO] Polling PT/2019 (0033867-260208012135463) -> RUNNING (12 min)\n",
            "14:47:26 [INFO] Polling PT/2018 (0033868-260208012135463) -> RUNNING (12 min)\n",
            "14:47:56 [INFO] Polling PT/2020 (0033866-260208012135463) -> RUNNING (12 min)\n",
            "14:47:56 [INFO] Polling PT/2019 (0033867-260208012135463) -> SUCCEEDED (12 min)\n",
            "14:47:56 [INFO] SUCCEEDED: PT/2019 | records=1258993 size=141.5 MB – starting ETL\n",
            "14:47:56 [INFO] Downloading ZIP for 0033867-260208012135463 …\n",
            "14:47:56 [INFO] Download file size: 141466820 bytes\n",
            "14:48:01 [INFO] On disk at data/etl_temp/PT_2019_0033867-260208012135463/zips/0033867-260208012135463.zip\n",
            "14:48:01 [INFO] ZIP saved: 0033867-260208012135463.zip (141.5 MB)\n",
            "14:48:03 [INFO] Extracted to: data/etl_temp/PT_2019_0033867-260208012135463/extracted/0033867-260208012135463\n",
            "14:48:03 [INFO] ZIP contents (4918 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585', 'occurrence.parquet/000317']\n",
            "14:48:03 [INFO] Reading occurrence.parquet/ directory: 4916 non-empty part file(s) (skipped 1 empty)\n",
            "14:48:09 [INFO] Read 1258993 rows, 50 columns from occurrence.parquet/\n",
            "14:48:10 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:48:12 [INFO] Enrichment for PT/2019: threatened=24231, invasive_em=0, invasive_doe=0, invasive_api=8542, invasive_any=8542\n",
            "14:48:12 [INFO] Writing 1258993 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2019 …\n",
            "14:48:44 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2019\n",
            "14:48:44 [INFO] ✓ Done: PT/2019 -> s3://ie-datalake/bronze/gbif/country=PT/year=2019 (1258993 rows)\n",
            "14:48:44 [INFO] Cleaned up temp dir: data/etl_temp/PT_2019_0033867-260208012135463\n",
            "14:48:45 [INFO] Polling PT/2018 (0033868-260208012135463) -> SUCCEEDED (13 min)\n",
            "14:48:45 [INFO] SUCCEEDED: PT/2018 | records=1121701 size=125.0 MB – starting ETL\n",
            "14:48:45 [INFO] Downloading ZIP for 0033868-260208012135463 …\n",
            "14:48:45 [INFO] Download file size: 124972359 bytes\n",
            "14:48:50 [INFO] On disk at data/etl_temp/PT_2018_0033868-260208012135463/zips/0033868-260208012135463.zip\n",
            "14:48:50 [INFO] ZIP saved: 0033868-260208012135463.zip (125.0 MB)\n",
            "14:48:52 [INFO] Extracted to: data/etl_temp/PT_2018_0033868-260208012135463/extracted/0033868-260208012135463\n",
            "14:48:52 [INFO] ZIP contents (4949 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "14:48:52 [INFO] Reading occurrence.parquet/ directory: 4947 non-empty part file(s) (skipped 1 empty)\n",
            "14:48:57 [INFO] Read 1121701 rows, 50 columns from occurrence.parquet/\n",
            "14:48:58 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:48:59 [INFO] Enrichment for PT/2018: threatened=23391, invasive_em=0, invasive_doe=0, invasive_api=4687, invasive_any=4687\n",
            "14:48:59 [INFO] Writing 1121701 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2018 …\n",
            "14:49:26 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2018\n",
            "14:49:26 [INFO] ✓ Done: PT/2018 -> s3://ie-datalake/bronze/gbif/country=PT/year=2018 (1121701 rows)\n",
            "14:49:26 [INFO] Cleaned up temp dir: data/etl_temp/PT_2018_0033868-260208012135463\n",
            "14:49:57 [INFO] Polling PT/2020 (0033866-260208012135463) -> SUCCEEDED (14 min)\n",
            "14:49:57 [INFO] SUCCEEDED: PT/2020 | records=1258159 size=145.2 MB – starting ETL\n",
            "14:49:57 [INFO] Downloading ZIP for 0033866-260208012135463 …\n",
            "14:49:57 [INFO] Download file size: 145156771 bytes\n",
            "14:50:13 [INFO] On disk at data/etl_temp/PT_2020_0033866-260208012135463/zips/0033866-260208012135463.zip\n",
            "14:50:13 [INFO] ZIP saved: 0033866-260208012135463.zip (145.2 MB)\n",
            "14:50:15 [INFO] Extracted to: data/etl_temp/PT_2020_0033866-260208012135463/extracted/0033866-260208012135463\n",
            "14:50:15 [INFO] ZIP contents (4909 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585', 'occurrence.parquet/000317']\n",
            "14:50:15 [INFO] Reading occurrence.parquet/ directory: 4907 non-empty part file(s) (skipped 1 empty)\n",
            "14:50:20 [INFO] Read 1258159 rows, 50 columns from occurrence.parquet/\n",
            "14:50:22 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "14:50:23 [INFO] Enrichment for PT/2020: threatened=21396, invasive_em=0, invasive_doe=0, invasive_api=5979, invasive_any=5979\n",
            "14:50:23 [INFO] Writing 1258159 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2020 …\n",
            "14:50:52 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2020\n",
            "14:50:52 [INFO] ✓ Done: PT/2020 -> s3://ie-datalake/bronze/gbif/country=PT/year=2020 (1258159 rows)\n",
            "14:50:53 [INFO] Cleaned up temp dir: data/etl_temp/PT_2020_0033866-260208012135463\n",
            "14:50:53 [INFO] Batch 3/6 complete. Running totals: 9 succeeded, 0 failed.\n",
            "14:50:53 [INFO] \n",
            "════════════════════════════════════════════════════════════\n",
            "14:50:53 [INFO] Batch 4/6: [('PT', 2017), ('PT', 2016), ('PT', 2015)]\n",
            "14:50:53 [INFO] ════════════════════════════════════════════════════════════\n",
            "\n",
            "14:50:55 [INFO] Your download key is 0033894-260208012135463\n",
            "14:50:55 [INFO] Submitted: country=PT year=2017 -> 0033894-260208012135463\n",
            "14:50:56 [INFO] Your download key is 0033895-260208012135463\n",
            "14:50:56 [INFO] Submitted: country=PT year=2016 -> 0033895-260208012135463\n",
            "14:50:58 [INFO] Your download key is 0033896-260208012135463\n",
            "14:50:58 [INFO] Submitted: country=PT year=2015 -> 0033896-260208012135463\n",
            "14:50:58 [INFO] Polling PT/2017 (0033894-260208012135463) -> PREPARING (0 min)\n",
            "14:50:58 [INFO] Polling PT/2016 (0033895-260208012135463) -> PREPARING (0 min)\n",
            "14:50:58 [INFO] Polling PT/2015 (0033896-260208012135463) -> PREPARING (0 min)\n",
            "14:51:28 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (1 min)\n",
            "14:51:29 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (1 min)\n",
            "14:51:29 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (1 min)\n",
            "14:51:59 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (1 min)\n",
            "14:51:59 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (1 min)\n",
            "14:51:59 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (1 min)\n",
            "14:52:29 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (2 min)\n",
            "14:52:29 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (2 min)\n",
            "14:52:29 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (2 min)\n",
            "14:52:59 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (2 min)\n",
            "14:53:00 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (2 min)\n",
            "14:53:00 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (2 min)\n",
            "14:53:30 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (3 min)\n",
            "14:53:30 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (3 min)\n",
            "14:53:30 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (3 min)\n",
            "14:54:00 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (3 min)\n",
            "14:54:00 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (3 min)\n",
            "14:54:00 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (3 min)\n",
            "14:54:30 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (4 min)\n",
            "14:54:30 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (4 min)\n",
            "14:54:31 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (4 min)\n",
            "14:55:01 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (4 min)\n",
            "14:55:01 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (4 min)\n",
            "14:55:01 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (4 min)\n",
            "14:55:31 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (5 min)\n",
            "14:55:31 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (5 min)\n",
            "14:55:31 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (5 min)\n",
            "14:56:01 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (5 min)\n",
            "14:56:02 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (5 min)\n",
            "14:56:02 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (5 min)\n",
            "14:56:32 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (6 min)\n",
            "14:56:32 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (6 min)\n",
            "14:56:32 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (6 min)\n",
            "14:57:02 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (6 min)\n",
            "14:57:02 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (6 min)\n",
            "14:57:02 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (6 min)\n",
            "14:57:33 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (7 min)\n",
            "14:57:33 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (7 min)\n",
            "14:57:33 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (7 min)\n",
            "14:58:03 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (7 min)\n",
            "14:58:03 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (7 min)\n",
            "14:58:03 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (7 min)\n",
            "14:58:33 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (8 min)\n",
            "14:58:33 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (8 min)\n",
            "14:58:34 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (8 min)\n",
            "14:59:04 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (8 min)\n",
            "14:59:04 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (8 min)\n",
            "14:59:04 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (8 min)\n",
            "14:59:34 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (9 min)\n",
            "14:59:34 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (9 min)\n",
            "14:59:34 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (9 min)\n",
            "15:00:05 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (9 min)\n",
            "15:00:05 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (9 min)\n",
            "15:00:05 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (9 min)\n",
            "15:00:35 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (10 min)\n",
            "15:00:35 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (10 min)\n",
            "15:00:35 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (10 min)\n",
            "15:01:05 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (10 min)\n",
            "15:01:05 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (10 min)\n",
            "15:01:05 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (10 min)\n",
            "15:01:36 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (11 min)\n",
            "15:01:36 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (11 min)\n",
            "15:01:36 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (11 min)\n",
            "15:02:06 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (11 min)\n",
            "15:02:06 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (11 min)\n",
            "15:02:06 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (11 min)\n",
            "15:02:36 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (12 min)\n",
            "15:02:36 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (12 min)\n",
            "15:02:37 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (12 min)\n",
            "15:03:07 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (12 min)\n",
            "15:03:07 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (12 min)\n",
            "15:03:07 [INFO] Polling PT/2015 (0033896-260208012135463) -> RUNNING (12 min)\n",
            "15:03:37 [INFO] Polling PT/2017 (0033894-260208012135463) -> RUNNING (13 min)\n",
            "15:03:37 [INFO] Polling PT/2016 (0033895-260208012135463) -> RUNNING (13 min)\n",
            "15:03:37 [INFO] Polling PT/2015 (0033896-260208012135463) -> SUCCEEDED (13 min)\n",
            "15:03:37 [INFO] SUCCEEDED: PT/2015 | records=1339977 size=116.6 MB – starting ETL\n",
            "15:03:37 [INFO] Downloading ZIP for 0033896-260208012135463 …\n",
            "15:03:37 [INFO] Download file size: 116646437 bytes\n",
            "15:03:42 [INFO] On disk at data/etl_temp/PT_2015_0033896-260208012135463/zips/0033896-260208012135463.zip\n",
            "15:03:42 [INFO] ZIP saved: 0033896-260208012135463.zip (116.6 MB)\n",
            "15:03:44 [INFO] Extracted to: data/etl_temp/PT_2015_0033896-260208012135463/extracted/0033896-260208012135463\n",
            "15:03:44 [INFO] ZIP contents (4983 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "15:03:44 [INFO] Reading occurrence.parquet/ directory: 4981 non-empty part file(s) (skipped 1 empty)\n",
            "15:03:49 [INFO] Read 1339977 rows, 50 columns from occurrence.parquet/\n",
            "15:03:51 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "15:03:52 [INFO] Enrichment for PT/2015: threatened=18606, invasive_em=0, invasive_doe=0, invasive_api=6531, invasive_any=6531\n",
            "15:03:52 [INFO] Writing 1339977 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2015 …\n",
            "15:04:33 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2015\n",
            "15:04:33 [INFO] ✓ Done: PT/2015 -> s3://ie-datalake/bronze/gbif/country=PT/year=2015 (1339977 rows)\n",
            "15:04:33 [INFO] Cleaned up temp dir: data/etl_temp/PT_2015_0033896-260208012135463\n",
            "15:05:04 [INFO] Polling PT/2017 (0033894-260208012135463) -> SUCCEEDED (14 min)\n",
            "15:05:04 [INFO] SUCCEEDED: PT/2017 | records=1076210 size=117.9 MB – starting ETL\n",
            "15:05:04 [INFO] Downloading ZIP for 0033894-260208012135463 …\n",
            "15:05:04 [INFO] Download file size: 117879325 bytes\n",
            "15:05:25 [INFO] On disk at data/etl_temp/PT_2017_0033894-260208012135463/zips/0033894-260208012135463.zip\n",
            "15:05:25 [INFO] ZIP saved: 0033894-260208012135463.zip (117.9 MB)\n",
            "15:05:26 [INFO] Extracted to: data/etl_temp/PT_2017_0033894-260208012135463/extracted/0033894-260208012135463\n",
            "15:05:27 [INFO] ZIP contents (4994 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "15:05:27 [INFO] Reading occurrence.parquet/ directory: 4992 non-empty part file(s) (skipped 1 empty)\n",
            "15:05:32 [INFO] Read 1076210 rows, 50 columns from occurrence.parquet/\n",
            "15:05:33 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "15:05:34 [INFO] Enrichment for PT/2017: threatened=23934, invasive_em=0, invasive_doe=0, invasive_api=4406, invasive_any=4406\n",
            "15:05:34 [INFO] Writing 1076210 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2017 …\n",
            "15:06:00 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2017\n",
            "15:06:00 [INFO] ✓ Done: PT/2017 -> s3://ie-datalake/bronze/gbif/country=PT/year=2017 (1076210 rows)\n",
            "15:06:01 [INFO] Cleaned up temp dir: data/etl_temp/PT_2017_0033894-260208012135463\n",
            "15:06:01 [INFO] Polling PT/2016 (0033895-260208012135463) -> SUCCEEDED (15 min)\n",
            "15:06:01 [INFO] SUCCEEDED: PT/2016 | records=899002 size=102.7 MB – starting ETL\n",
            "15:06:01 [INFO] Downloading ZIP for 0033895-260208012135463 …\n",
            "15:06:02 [INFO] Download file size: 102737425 bytes\n",
            "15:06:06 [INFO] On disk at data/etl_temp/PT_2016_0033895-260208012135463/zips/0033895-260208012135463.zip\n",
            "15:06:06 [INFO] ZIP saved: 0033895-260208012135463.zip (102.7 MB)\n",
            "15:06:08 [INFO] Extracted to: data/etl_temp/PT_2016_0033895-260208012135463/extracted/0033895-260208012135463\n",
            "15:06:08 [INFO] ZIP contents (4958 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/004929', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585']\n",
            "15:06:08 [INFO] Reading occurrence.parquet/ directory: 4956 non-empty part file(s) (skipped 1 empty)\n",
            "15:06:13 [INFO] Read 899002 rows, 50 columns from occurrence.parquet/\n",
            "15:06:14 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "15:06:15 [INFO] Enrichment for PT/2016: threatened=19420, invasive_em=0, invasive_doe=0, invasive_api=6340, invasive_any=6340\n",
            "15:06:15 [INFO] Writing 899002 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2016 …\n",
            "15:06:42 [INFO] Written: s3://ie-datalake/bronze/gbif/country=PT/year=2016\n",
            "15:06:42 [INFO] ✓ Done: PT/2016 -> s3://ie-datalake/bronze/gbif/country=PT/year=2016 (899002 rows)\n",
            "15:06:42 [INFO] Cleaned up temp dir: data/etl_temp/PT_2016_0033895-260208012135463\n",
            "15:06:43 [INFO] Batch 4/6 complete. Running totals: 12 succeeded, 0 failed.\n",
            "15:06:43 [INFO] \n",
            "════════════════════════════════════════════════════════════\n",
            "15:06:43 [INFO] Batch 5/6: [('PT', 2014), ('PT', 2013), ('PT', 2012)]\n",
            "15:06:43 [INFO] ════════════════════════════════════════════════════════════\n",
            "\n",
            "15:06:44 [INFO] Your download key is 0033930-260208012135463\n",
            "15:06:44 [INFO] Submitted: country=PT year=2014 -> 0033930-260208012135463\n",
            "15:06:46 [INFO] Your download key is 0033931-260208012135463\n",
            "15:06:46 [INFO] Submitted: country=PT year=2013 -> 0033931-260208012135463\n",
            "15:06:47 [INFO] Your download key is 0033932-260208012135463\n",
            "15:06:47 [INFO] Submitted: country=PT year=2012 -> 0033932-260208012135463\n",
            "15:06:48 [INFO] Polling PT/2014 (0033930-260208012135463) -> PREPARING (0 min)\n",
            "15:06:48 [INFO] Polling PT/2013 (0033931-260208012135463) -> PREPARING (0 min)\n",
            "15:06:48 [INFO] Polling PT/2012 (0033932-260208012135463) -> PREPARING (0 min)\n",
            "15:07:18 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (1 min)\n",
            "15:07:18 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (1 min)\n",
            "15:07:18 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (1 min)\n",
            "15:07:48 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (1 min)\n",
            "15:07:48 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (1 min)\n",
            "15:07:49 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (1 min)\n",
            "15:08:19 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (2 min)\n",
            "15:08:19 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (2 min)\n",
            "15:08:19 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (2 min)\n",
            "15:08:49 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (2 min)\n",
            "15:08:49 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (2 min)\n",
            "15:08:49 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (2 min)\n",
            "15:09:19 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (3 min)\n",
            "15:09:19 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (3 min)\n",
            "15:09:20 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (3 min)\n",
            "15:09:50 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (3 min)\n",
            "15:09:50 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (3 min)\n",
            "15:09:50 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (3 min)\n",
            "15:10:20 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (4 min)\n",
            "15:10:20 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (4 min)\n",
            "15:10:20 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (4 min)\n",
            "15:10:51 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (4 min)\n",
            "15:10:51 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (4 min)\n",
            "15:10:51 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (4 min)\n",
            "15:11:21 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (5 min)\n",
            "15:11:21 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (5 min)\n",
            "15:11:21 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (5 min)\n",
            "15:11:51 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (5 min)\n",
            "15:11:51 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (5 min)\n",
            "15:11:51 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (5 min)\n",
            "15:12:22 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (6 min)\n",
            "15:12:22 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (6 min)\n",
            "15:12:22 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (6 min)\n",
            "15:12:52 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (6 min)\n",
            "15:12:52 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (6 min)\n",
            "15:12:52 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (6 min)\n",
            "15:13:22 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (7 min)\n",
            "15:13:22 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (7 min)\n",
            "15:13:23 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (7 min)\n",
            "15:13:53 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (7 min)\n",
            "15:13:53 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (7 min)\n",
            "15:13:53 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (7 min)\n",
            "15:14:23 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (8 min)\n",
            "15:14:23 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (8 min)\n",
            "15:14:23 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (8 min)\n",
            "15:14:53 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (8 min)\n",
            "15:14:53 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (8 min)\n",
            "15:14:54 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (8 min)\n",
            "15:15:24 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (9 min)\n",
            "15:15:24 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (9 min)\n",
            "15:15:24 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (9 min)\n",
            "15:15:54 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (9 min)\n",
            "15:15:54 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (9 min)\n",
            "15:15:54 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (9 min)\n",
            "15:16:24 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (10 min)\n",
            "15:16:24 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (10 min)\n",
            "15:16:25 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (10 min)\n",
            "15:16:55 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (10 min)\n",
            "15:16:55 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (10 min)\n",
            "15:16:55 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (10 min)\n",
            "15:17:25 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (11 min)\n",
            "15:17:25 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (11 min)\n",
            "15:17:25 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (11 min)\n",
            "15:17:55 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (11 min)\n",
            "15:17:56 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (11 min)\n",
            "15:17:56 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (11 min)\n",
            "15:18:26 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (12 min)\n",
            "15:18:26 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (12 min)\n",
            "15:18:26 [INFO] Polling PT/2012 (0033932-260208012135463) -> RUNNING (12 min)\n",
            "15:18:56 [INFO] Polling PT/2014 (0033930-260208012135463) -> RUNNING (12 min)\n",
            "15:18:56 [INFO] Polling PT/2013 (0033931-260208012135463) -> RUNNING (12 min)\n",
            "15:18:56 [INFO] Polling PT/2012 (0033932-260208012135463) -> SUCCEEDED (12 min)\n",
            "15:18:56 [INFO] SUCCEEDED: PT/2012 | records=661196 size=69.6 MB – starting ETL\n",
            "15:18:56 [INFO] Downloading ZIP for 0033932-260208012135463 …\n",
            "15:18:57 [INFO] Download file size: 69642072 bytes\n",
            "15:19:01 [INFO] On disk at data/etl_temp/PT_2012_0033932-260208012135463/zips/0033932-260208012135463.zip\n",
            "15:19:01 [INFO] ZIP saved: 0033932-260208012135463.zip (69.6 MB)\n",
            "15:19:02 [INFO] Extracted to: data/etl_temp/PT_2012_0033932-260208012135463/extracted/0033932-260208012135463\n",
            "15:19:02 [INFO] ZIP contents (4876 entries): ['occurrence.parquet', 'occurrence.parquet/000787', 'occurrence.parquet/001655', 'occurrence.parquet/004327', 'occurrence.parquet/003102', 'occurrence.parquet/003330', 'occurrence.parquet/001467', 'occurrence.parquet/004115', 'occurrence.parquet/003754', 'occurrence.parquet/002686', 'occurrence.parquet/001003', 'occurrence.parquet/004571', 'occurrence.parquet/001231', 'occurrence.parquet/004743', 'occurrence.parquet/003566', 'occurrence.parquet/002672', 'occurrence.parquet/000125', 'occurrence.parquet/000919', 'occurrence.parquet/004585', 'occurrence.parquet/000317']\n",
            "15:19:02 [INFO] Reading occurrence.parquet/ directory: 4874 non-empty part file(s) (skipped 1 empty)\n",
            "15:19:07 [INFO] Read 661196 rows, 50 columns from occurrence.parquet/\n",
            "15:19:09 [INFO] Column mapping – speciesKey: specieskey, establishmentMeans: establishmentmeans, degreeOfEstablishment: None\n",
            "15:19:09 [INFO] Enrichment for PT/2012: threatened=19623, invasive_em=0, invasive_doe=0, invasive_api=4707, invasive_any=4707\n",
            "15:19:09 [INFO] Writing 661196 rows to s3://ie-datalake/bronze/gbif/country=PT/year=2012 …\n"
          ]
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# ETL JOB\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import pyarrow.parquet as pq\n",
        "import s3fs\n",
        "from dotenv import load_dotenv\n",
        "from pygbif import occurrences\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    force=True,\n",
        ")\n",
        "log = logging.getLogger(\"gbif_etl\")\n",
        "\n",
        "# ── GBIF credentials ──────────────────────────────────────────────────────────\n",
        "GBIF_USER = os.getenv(\"GBIF_USER\")\n",
        "GBIF_PWD = os.getenv(\"GBIF_PWD\")\n",
        "GBIF_EMAIL = os.getenv(\"GBIF_EMAIL\")\n",
        "\n",
        "if not (GBIF_USER and GBIF_PWD and GBIF_EMAIL):\n",
        "    raise RuntimeError(\n",
        "        \"Missing GBIF credentials. Set GBIF_USER, GBIF_PWD, GBIF_EMAIL in .env or shell.\"\n",
        "    )\n",
        "\n",
        "LOCAL_TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── S3 filesystem ─────────────────────────────────────────────────────────────\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "log.info(\"S3FileSystem initialized (profile=%s)\", AWS_PROFILE)\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# HELPER FUNCTIONS\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def _normalize_gbif_key(x) -> str:\n",
        "    \"\"\"Parse the polymorphic return value of pygbif occurrences.download().\"\"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    if isinstance(x, (tuple, list)):\n",
        "        if x and isinstance(x[0], str):\n",
        "            return x[0]\n",
        "    if isinstance(x, dict):\n",
        "        for k in (\"key\", \"downloadKey\", \"download_key\"):\n",
        "            if k in x:\n",
        "                return str(x[k])\n",
        "    raise ValueError(f\"Cannot parse GBIF download key from: {x!r}\")\n",
        "\n",
        "\n",
        "def _chunked(lst: list, n: int):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "# ── Step 1: Submit GBIF download job ─────────────────────────────────────────\n",
        "\n",
        "def submit_job(country: str, year: int) -> str:\n",
        "    \"\"\"Submit one GBIF occurrence download job and return the download key.\"\"\"\n",
        "    queries = [\n",
        "        f\"country = {country}\",\n",
        "        f\"year = {year}\",\n",
        "        \"hasCoordinate = TRUE\",\n",
        "    ]\n",
        "    resp = occurrences.download(\n",
        "        queries,\n",
        "        format=DOWNLOAD_FORMAT,\n",
        "        user=GBIF_USER,\n",
        "        pwd=GBIF_PWD,\n",
        "        email=GBIF_EMAIL,\n",
        "        pred_type=\"and\",\n",
        "    )\n",
        "    key = _normalize_gbif_key(resp)\n",
        "    log.info(\"Submitted: country=%s year=%s -> %s\", country, year, key)\n",
        "    return key\n",
        "\n",
        "\n",
        "# ── Step 2: Poll until SUCCEEDED ─────────────────────────────────────────────\n",
        "\n",
        "def wait_for_job(key: str, label: str = \"\") -> dict:\n",
        "    \"\"\"Poll GBIF until the download finishes. Raises on failure or timeout.\"\"\"\n",
        "    t0 = time.time()\n",
        "    while True:\n",
        "        meta = occurrences.download_meta(key) or {}\n",
        "        status = meta.get(\"status\")\n",
        "        elapsed_min = (time.time() - t0) / 60\n",
        "        log.info(\"Polling %s %s -> %s (%.0f min)\", label, key, status, elapsed_min)\n",
        "\n",
        "        if status == \"SUCCEEDED\":\n",
        "            records = meta.get(\"totalRecords\", \"?\")\n",
        "            size_mb = (meta.get(\"size\") or 0) / 1e6\n",
        "            log.info(\"SUCCEEDED: %s | records=%s size=%.1f MB\", key, records, size_mb)\n",
        "            return meta\n",
        "\n",
        "        if status in {\"KILLED\", \"CANCELLED\", \"FAILED\"}:\n",
        "            raise RuntimeError(f\"Download {key} ended with status={status}\")\n",
        "\n",
        "        if time.time() - t0 > GBIF_TIMEOUT_S:\n",
        "            raise TimeoutError(f\"Timeout ({GBIF_TIMEOUT_S}s) waiting for download {key}\")\n",
        "\n",
        "        time.sleep(GBIF_POLL_INTERVAL_S)\n",
        "\n",
        "\n",
        "# ── Step 3: Download ZIP and extract ─────────────────────────────────────────\n",
        "\n",
        "def download_and_extract(key: str, work_dir: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Download the GBIF ZIP for `key`, extract it, return the extract root directory.\n",
        "\n",
        "    GBIF SIMPLE_PARQUET ZIP structure:\n",
        "        KEY.zip/\n",
        "          occurrence.parquet/      <- directory (NOT a single file)\n",
        "            000000                 <- part file (no extension; first may be 0 bytes = empty part)\n",
        "            000001\n",
        "            ...\n",
        "    \"\"\"\n",
        "    zip_dir = work_dir / \"zips\"\n",
        "    zip_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    log.info(\"Downloading ZIP for %s …\", key)\n",
        "    result = occurrences.download_get(key, path=str(zip_dir))\n",
        "    zip_path = Path(result[\"path\"]) if isinstance(result, dict) else Path(str(result))\n",
        "    log.info(\"ZIP saved: %s (%.1f MB)\", zip_path.name, zip_path.stat().st_size / 1e6)\n",
        "\n",
        "    extract_dir = work_dir / \"extracted\" / key\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "    log.info(\"Extracted to: %s\", extract_dir)\n",
        "\n",
        "    # Log what's inside for debugging\n",
        "    all_entries = list(extract_dir.rglob(\"*\"))\n",
        "    log.info(\"ZIP contents (%d entries): %s\", len(all_entries),\n",
        "             [str(p.relative_to(extract_dir)) for p in all_entries[:20]])\n",
        "\n",
        "    return extract_dir\n",
        "\n",
        "\n",
        "# ── Step 4: Read GBIF SIMPLE_PARQUET extract into a single DataFrame ─────────\n",
        "\n",
        "def read_gbif_parquet(extract_dir: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read GBIF SIMPLE_PARQUET extract from the given directory.\n",
        "\n",
        "    GBIF packs data into `occurrence.parquet/` (a directory of numbered part files).\n",
        "    The first file (000000) is always 0 bytes and must be skipped.\n",
        "    We pass only non-empty files to pyarrow.dataset to avoid ArrowInvalid errors.\n",
        "    \"\"\"\n",
        "    # 1. Find `occurrence.parquet` directory (SIMPLE_PARQUET format)\n",
        "    occ_parquet_dir = extract_dir / \"occurrence.parquet\"\n",
        "    if occ_parquet_dir.is_dir():\n",
        "        part_files = sorted(\n",
        "            str(f) for f in occ_parquet_dir.iterdir()\n",
        "            if f.is_file() and f.stat().st_size > 0\n",
        "        )\n",
        "        if not part_files:\n",
        "            raise FileNotFoundError(\n",
        "                f\"All part files in {occ_parquet_dir} are empty (0 bytes). \"\n",
        "                f\"Files: {list(occ_parquet_dir.iterdir())}\"\n",
        "            )\n",
        "        log.info(\n",
        "            \"Reading occurrence.parquet/ directory: %d non-empty part file(s) \"\n",
        "            \"(skipped %d empty)\",\n",
        "            len(part_files),\n",
        "            sum(1 for f in occ_parquet_dir.iterdir()\n",
        "                if f.is_file() and f.stat().st_size == 0),\n",
        "        )\n",
        "        dataset = ds.dataset(part_files, format=\"parquet\")\n",
        "        table = dataset.to_table()\n",
        "        df = table.to_pandas()\n",
        "        log.info(\"Read %d rows, %d columns from occurrence.parquet/\", len(df), len(df.columns))\n",
        "        return df\n",
        "\n",
        "    # 2. Fallback: individual *.parquet files (e.g. DWCA or older format)\n",
        "    parquet_files = [\n",
        "        f for f in extract_dir.rglob(\"*.parquet\")\n",
        "        if f.is_file() and f.stat().st_size > 0\n",
        "    ]\n",
        "    if parquet_files:\n",
        "        log.info(\"Fallback: reading %d .parquet file(s)\", len(parquet_files))\n",
        "        dfs = [pd.read_parquet(f) for f in parquet_files]\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        log.info(\"Read %d rows total\", len(df))\n",
        "        return df\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"No readable parquet data found in {extract_dir}.\\n\"\n",
        "        f\"Contents: {[str(p.relative_to(extract_dir)) for p in extract_dir.rglob('*')]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ── Step 5: Fetch threatened species for a country (cached) ──────────────────\n",
        "\n",
        "_threatened_cache: dict[str, dict[int, str]] = {}\n",
        "\n",
        "def fetch_threatened_species(country: str) -> dict[int, str]:\n",
        "    \"\"\"\n",
        "    Return {speciesKey_int -> iucn_cat} for threatened species (VU/EN/CR)\n",
        "    in the given country, fetched via GBIF occurrence search facets.\n",
        "\n",
        "    iucnRedListCategory is only available in the search API, not in occurrence exports,\n",
        "    so we use facet=speciesKey to collect the set of threatened speciesKeys and join later.\n",
        "\n",
        "    Result is cached per country to avoid redundant API calls across years.\n",
        "    \"\"\"\n",
        "    if country in _threatened_cache:\n",
        "        return _threatened_cache[country]\n",
        "\n",
        "    log.info(\"Fetching threatened species (VU/EN/CR) for %s via GBIF search API …\", country)\n",
        "    # Severity for deduplication when a speciesKey appears under multiple categories\n",
        "    severity = {\"CR\": 3, \"EN\": 2, \"VU\": 1}\n",
        "    result: dict[int, str] = {}\n",
        "\n",
        "    for cat in THREATENED_CATS:\n",
        "        offset = 0\n",
        "        while True:\n",
        "            resp = occurrences.search(\n",
        "                country=country,\n",
        "                hasCoordinate=True,\n",
        "                iucnRedListCategory=cat,\n",
        "                facet=\"speciesKey\",\n",
        "                limit=0,\n",
        "                **{\n",
        "                    \"speciesKey.facetLimit\": FACET_LIMIT,\n",
        "                    \"speciesKey.facetOffset\": offset,\n",
        "                },\n",
        "            )\n",
        "            facets = resp.get(\"facets\") or []\n",
        "            sk_facet = next(\n",
        "                (f for f in facets if isinstance(f, dict) and f.get(\"field\", \"\").upper() == \"SPECIES_KEY\"),\n",
        "                None,\n",
        "            )\n",
        "            counts = sk_facet.get(\"counts\", []) if sk_facet else []\n",
        "            if not counts:\n",
        "                break\n",
        "            for item in counts:\n",
        "                raw_sk = item.get(\"name\")\n",
        "                if raw_sk is None:\n",
        "                    continue\n",
        "                try:\n",
        "                    sk = int(raw_sk)\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "                # keep most severe category if species appears under multiple\n",
        "                if sk not in result or severity[cat] > severity.get(result[sk], 0):\n",
        "                    result[sk] = cat\n",
        "            if len(counts) < FACET_LIMIT:\n",
        "                break\n",
        "            offset += FACET_LIMIT\n",
        "\n",
        "    log.info(\"Threatened species for %s: %d speciesKeys total\", country, len(result))\n",
        "    _threatened_cache[country] = result\n",
        "    return result\n",
        "\n",
        "\n",
        "# ── Step 6: Fetch invasive speciesKeys for a country (cached) ────────────────\n",
        "\n",
        "_invasive_cache: dict[str, dict[str, set[int]]] = {}\n",
        "# structure: { country -> {\"invasive\": set[int], \"introduced\": set[int]} }\n",
        "\n",
        "def _fetch_species_keys_by_filter(country: str, **search_kwargs) -> set[int]:\n",
        "    \"\"\"Generic helper: facet GBIF search by speciesKey with arbitrary filters.\"\"\"\n",
        "    result: set[int] = set()\n",
        "    offset = 0\n",
        "    while True:\n",
        "        resp = occurrences.search(\n",
        "            country=country,\n",
        "            hasCoordinate=True,\n",
        "            facet=\"speciesKey\",\n",
        "            limit=0,\n",
        "            **search_kwargs,\n",
        "            **{\n",
        "                \"speciesKey.facetLimit\": FACET_LIMIT,\n",
        "                \"speciesKey.facetOffset\": offset,\n",
        "            },\n",
        "        )\n",
        "        facets = resp.get(\"facets\") or []\n",
        "        sk_facet = next(\n",
        "            (f for f in facets if isinstance(f, dict) and f.get(\"field\", \"\").upper() == \"SPECIES_KEY\"),\n",
        "            None,\n",
        "        )\n",
        "        counts = sk_facet.get(\"counts\", []) if sk_facet else []\n",
        "        if not counts:\n",
        "            break\n",
        "        for item in counts:\n",
        "            raw_sk = item.get(\"name\")\n",
        "            if raw_sk is None:\n",
        "                continue\n",
        "            try:\n",
        "                result.add(int(raw_sk))\n",
        "            except (ValueError, TypeError):\n",
        "                pass\n",
        "        if len(counts) < FACET_LIMIT:\n",
        "            break\n",
        "        offset += FACET_LIMIT\n",
        "    return result\n",
        "\n",
        "\n",
        "def fetch_invasive_species(country: str) -> dict[str, set[int]]:\n",
        "    \"\"\"\n",
        "    Return {\n",
        "        \"invasive\":   set[speciesKey] with degreeOfEstablishment=invasive,\n",
        "        \"introduced\": set[speciesKey] with establishmentMeans=introduced,\n",
        "    } for the given country.\n",
        "\n",
        "    Cached per country.\n",
        "    \"\"\"\n",
        "    if country in _invasive_cache:\n",
        "        return _invasive_cache[country]\n",
        "\n",
        "    log.info(\"Fetching invasive/introduced speciesKeys for %s via GBIF search API …\", country)\n",
        "\n",
        "    invasive_keys = _fetch_species_keys_by_filter(\n",
        "        country, degreeOfEstablishment=\"invasive\"\n",
        "    )\n",
        "    introduced_keys = _fetch_species_keys_by_filter(\n",
        "        country, establishmentMeans=\"introduced\"\n",
        "    )\n",
        "\n",
        "    log.info(\n",
        "        \"Invasive API keys for %s: degreeOfEstablishment=invasive -> %d, establishmentMeans=introduced -> %d\",\n",
        "        country, len(invasive_keys), len(introduced_keys),\n",
        "    )\n",
        "\n",
        "    result = {\"invasive\": invasive_keys, \"introduced\": introduced_keys}\n",
        "    _invasive_cache[country] = result\n",
        "    return result\n",
        "\n",
        "\n",
        "# ── Step 7: Enrich DataFrame with threatened + invasive metadata ─────────────\n",
        "\n",
        "def enrich_dataframe(df: pd.DataFrame, country: str, year: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add enrichment columns to the occurrence DataFrame:\n",
        "\n",
        "    Partition columns (also as regular columns):\n",
        "      country        – ISO-2 country code\n",
        "      year           – observation year\n",
        "\n",
        "    Threatened species (IUCN Red List, via GBIF search API):\n",
        "      is_threatened  – bool: species in VU/EN/CR for this country\n",
        "      iucn_cat       – str: 'VU', 'EN', or 'CR' (null if not threatened)\n",
        "\n",
        "    Invasive / introduced status:\n",
        "      is_invasive_em  – bool: establishmentMeans in export == 'INVASIVE'\n",
        "      is_introduced   – bool: establishmentMeans in export == 'INTRODUCED'\n",
        "      is_naturalized  – bool: establishmentMeans in export == 'NATURALISED'\n",
        "      is_invasive_doe – bool: degreeOfEstablishment in export == 'invasive'\n",
        "      is_invasive_api – bool: speciesKey appears in degreeOfEstablishment=invasive records for this country (GBIF API)\n",
        "      is_introduced_api – bool: speciesKey appears in establishmentMeans=introduced records for this country (GBIF API)\n",
        "      is_invasive_any – bool: invasive from ANY of the above sources\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # ── Partition columns ──────────────────────────────────────────────────\n",
        "    df[\"country\"] = country\n",
        "    df[\"year\"] = int(year)\n",
        "\n",
        "    # ── Case-insensitive column lookup (GBIF SIMPLE_PARQUET may use lowercase) ──\n",
        "    def _find_col(name: str) -> str | None:\n",
        "        \"\"\"Find a column by stripping underscores and lowercasing.\"\"\"\n",
        "        norm = name.lower().replace(\"_\", \"\")\n",
        "        for c in df.columns:\n",
        "            if c.lower().replace(\"_\", \"\") == norm:\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    sk_col  = _find_col(\"speciesKey\")\n",
        "    em_col  = _find_col(\"establishmentMeans\")\n",
        "    doe_col = _find_col(\"degreeOfEstablishment\")\n",
        "\n",
        "    log.info(\n",
        "        \"Column mapping – speciesKey: %s, establishmentMeans: %s, degreeOfEstablishment: %s\",\n",
        "        sk_col, em_col, doe_col,\n",
        "    )\n",
        "\n",
        "    # ── Threatened species ────────────────────────────────────────────────\n",
        "    threatened_map = fetch_threatened_species(country)\n",
        "    if sk_col:\n",
        "        sk_numeric = pd.to_numeric(df[sk_col], errors=\"coerce\")\n",
        "        df[\"is_threatened\"] = sk_numeric.isin(threatened_map.keys())\n",
        "        df[\"iucn_cat\"] = sk_numeric.map(threatened_map).astype(\"string\")\n",
        "    else:\n",
        "        log.warning(\"speciesKey column not found in DataFrame – actual columns: %s\", list(df.columns))\n",
        "        df[\"is_threatened\"] = False\n",
        "        df[\"iucn_cat\"] = pd.NA\n",
        "\n",
        "    if em_col:\n",
        "        em_upper = df[em_col].fillna(\"\").astype(str).str.strip().str.upper()\n",
        "        df[\"is_invasive_em\"]  = em_upper == \"INVASIVE\"\n",
        "        df[\"is_introduced\"]   = em_upper == \"INTRODUCED\"\n",
        "        df[\"is_naturalized\"]  = em_upper.isin([\"NATURALISED\", \"NATURALIZED\"])\n",
        "    else:\n",
        "        df[\"is_invasive_em\"] = False\n",
        "        df[\"is_introduced\"]  = False\n",
        "        df[\"is_naturalized\"] = False\n",
        "\n",
        "    if doe_col:\n",
        "        doe_lower = df[doe_col].fillna(\"\").astype(str).str.strip().str.lower()\n",
        "        df[\"is_invasive_doe\"] = doe_lower == \"invasive\"\n",
        "    else:\n",
        "        df[\"is_invasive_doe\"] = False\n",
        "\n",
        "    # ── Invasive / introduced from API lookup (per species, per country) ───\n",
        "    invasive_api = fetch_invasive_species(country)\n",
        "    if sk_col:\n",
        "        sk_numeric = pd.to_numeric(df[sk_col], errors=\"coerce\")\n",
        "        df[\"is_invasive_api\"]   = sk_numeric.isin(invasive_api[\"invasive\"])\n",
        "        df[\"is_introduced_api\"] = sk_numeric.isin(invasive_api[\"introduced\"])\n",
        "    else:\n",
        "        df[\"is_invasive_api\"]   = False\n",
        "        df[\"is_introduced_api\"] = False\n",
        "\n",
        "    # ── Combined invasive flag (any source) ───────────────────────────────\n",
        "    df[\"is_invasive_any\"] = (\n",
        "        df[\"is_invasive_em\"]\n",
        "        | df[\"is_invasive_doe\"]\n",
        "        | df[\"is_invasive_api\"]\n",
        "    )\n",
        "\n",
        "    log.info(\n",
        "        \"Enrichment for %s/%s: threatened=%d, invasive_em=%d, invasive_doe=%d, invasive_api=%d, invasive_any=%d\",\n",
        "        country, year,\n",
        "        df[\"is_threatened\"].sum(),\n",
        "        df[\"is_invasive_em\"].sum(),\n",
        "        df[\"is_invasive_doe\"].sum(),\n",
        "        df[\"is_invasive_api\"].sum(),\n",
        "        df[\"is_invasive_any\"].sum(),\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "# ── Step 8: Write enriched DataFrame to S3 ───────────────────────────────────\n",
        "\n",
        "def write_to_s3(df: pd.DataFrame, country: str, year: int) -> str:\n",
        "    \"\"\"\n",
        "    Write DataFrame to S3 as snappy-compressed Parquet.\n",
        "\n",
        "    Path structure: s3://{S3_BUCKET}/{S3_PREFIX}/country={country}/year={year}/\n",
        "    country and year are ALSO stored as regular columns inside each file.\n",
        "\n",
        "    Any existing files at that prefix are overwritten (delete_matching).\n",
        "    \"\"\"\n",
        "    s3_root = f\"{S3_BUCKET}/{S3_PREFIX}/country={country}/year={year}\"\n",
        "    log.info(\"Writing %d rows to s3://%s …\", len(df), s3_root)\n",
        "\n",
        "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
        "\n",
        "    pq.write_to_dataset(\n",
        "        table,\n",
        "        root_path=f\"s3://{s3_root}\",\n",
        "        filesystem=fs,\n",
        "        existing_data_behavior=\"delete_matching\",\n",
        "        row_group_size=PARQUET_ROW_GROUP_SIZE,\n",
        "        compression=PARQUET_COMPRESSION,\n",
        "        write_statistics=True,\n",
        "    )\n",
        "    s3_uri = f\"s3://{s3_root}\"\n",
        "    log.info(\"Written: %s\", s3_uri)\n",
        "    return s3_uri\n",
        "\n",
        "\n",
        "# ── Step 9: Cleanup ───────────────────────────────────────────────────────────\n",
        "\n",
        "def cleanup(work_dir: Path) -> None:\n",
        "    if work_dir.exists():\n",
        "        shutil.rmtree(work_dir, ignore_errors=True)\n",
        "        log.info(\"Cleaned up temp dir: %s\", work_dir)\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# MAIN ETL LOOP\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def process_one_job(country: str, year: int, key: str) -> dict:\n",
        "    \"\"\"\n",
        "    Full ETL for a single succeeded GBIF job:\n",
        "    download → extract → read → enrich → write to S3 → cleanup.\n",
        "    Returns a summary dict on success. Raises on any error.\n",
        "    \"\"\"\n",
        "    work_dir = LOCAL_TEMP_DIR / f\"{country}_{year}_{key}\"\n",
        "    try:\n",
        "        extract_dir = download_and_extract(key, work_dir)\n",
        "        df = read_gbif_parquet(extract_dir)\n",
        "        df = enrich_dataframe(df, country, year)\n",
        "        s3_uri = write_to_s3(df, country, year)\n",
        "        summary = {\n",
        "            \"country\": country,\n",
        "            \"year\": year,\n",
        "            \"key\": key,\n",
        "            \"rows\": len(df),\n",
        "            \"s3_uri\": s3_uri,\n",
        "            \"threatened_records\": int(df[\"is_threatened\"].sum()),\n",
        "            \"invasive_any_records\": int(df[\"is_invasive_any\"].sum()),\n",
        "        }\n",
        "        log.info(\"✓ Done: %s/%s -> %s (%d rows)\", country, year, s3_uri, len(df))\n",
        "        return summary\n",
        "    finally:\n",
        "        cleanup(work_dir)\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# MAIN ETL LOOP\n",
        "#\n",
        "# Strategy:\n",
        "#   1. Split job_plan into batches of MAX_CONCURRENT_JOBS.\n",
        "#   2. Submit all jobs in the batch at once.\n",
        "#   3. Poll ALL pending jobs in a round-robin loop.\n",
        "#      → As soon as a job reaches SUCCEEDED, run its ETL immediately\n",
        "#        (download → enrich → S3) without waiting for the other jobs in the batch.\n",
        "#   4. Move to the next batch only when every job in the current batch\n",
        "#      has either succeeded+processed or failed.\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "job_plan = [\n",
        "    (country, year)\n",
        "    for country in COUNTRIES\n",
        "    for year in range(YEAR_END, YEAR_START - 1, -1)  # newest first\n",
        "]\n",
        "n_batches = -(-len(job_plan) // MAX_CONCURRENT_JOBS)  # ceil division\n",
        "log.info(\"Job plan: %d total jobs, %d batch(es) of max %d\", len(job_plan), n_batches, MAX_CONCURRENT_JOBS)\n",
        "\n",
        "completed: list[dict] = []\n",
        "errors: list[dict] = []\n",
        "\n",
        "for batch_num, batch in enumerate(_chunked(job_plan, MAX_CONCURRENT_JOBS), start=1):\n",
        "    log.info(\"\\n%s\", \"═\" * 60)\n",
        "    log.info(\"Batch %d/%d: %s\", batch_num, n_batches, batch)\n",
        "    log.info(\"%s\\n\", \"═\" * 60)\n",
        "\n",
        "    # ── 1. Submit all jobs in this batch ──────────────────────────────────────\n",
        "    # pending: maps (country, year) -> {\"key\": str, \"t0\": float}\n",
        "    pending: dict[tuple[str, int], dict] = {}\n",
        "    for country, year in batch:\n",
        "        try:\n",
        "            key = submit_job(country, year)\n",
        "            pending[(country, year)] = {\"key\": key, \"t0\": time.time()}\n",
        "        except Exception as exc:\n",
        "            log.error(\"Submit failed %s/%s: %s\", country, year, exc)\n",
        "            errors.append({\"country\": country, \"year\": year, \"stage\": \"submit\", \"error\": str(exc)})\n",
        "\n",
        "    # ── 2+3. Poll all pending jobs; process each as soon as it SUCCEEDS ───────\n",
        "    while pending:\n",
        "        for job_id in list(pending.keys()):\n",
        "            country, year = job_id\n",
        "            key = pending[job_id][\"key\"]\n",
        "            t0  = pending[job_id][\"t0\"]\n",
        "\n",
        "            try:\n",
        "                meta = occurrences.download_meta(key) or {}\n",
        "            except Exception as exc:\n",
        "                log.warning(\"download_meta failed for %s: %s – will retry\", key, exc)\n",
        "                continue\n",
        "\n",
        "            status = meta.get(\"status\")\n",
        "            elapsed_min = (time.time() - t0) / 60\n",
        "            log.info(\"Polling %s/%s (%s) -> %s (%.0f min)\", country, year, key, status, elapsed_min)\n",
        "\n",
        "            if status == \"SUCCEEDED\":\n",
        "                del pending[job_id]\n",
        "                records  = meta.get(\"totalRecords\", \"?\")\n",
        "                size_mb  = (meta.get(\"size\") or 0) / 1e6\n",
        "                log.info(\"SUCCEEDED: %s/%s | records=%s size=%.1f MB – starting ETL\", country, year, records, size_mb)\n",
        "                try:\n",
        "                    summary = process_one_job(country, year, key)\n",
        "                    completed.append(summary)\n",
        "                except Exception as exc:\n",
        "                    log.error(\"ETL failed %s/%s (%s): %s\", country, year, key, exc, exc_info=True)\n",
        "                    errors.append({\"country\": country, \"year\": year, \"key\": key, \"stage\": \"etl\", \"error\": str(exc)})\n",
        "\n",
        "            elif status in {\"KILLED\", \"CANCELLED\", \"FAILED\"}:\n",
        "                del pending[job_id]\n",
        "                log.error(\"Job ended with status=%s: %s/%s (%s)\", status, country, year, key)\n",
        "                errors.append({\"country\": country, \"year\": year, \"key\": key, \"stage\": \"poll\", \"error\": f\"status={status}\"})\n",
        "\n",
        "            elif time.time() - t0 > GBIF_TIMEOUT_S:\n",
        "                del pending[job_id]\n",
        "                log.error(\"Timeout waiting for %s/%s (%s)\", country, year, key)\n",
        "                errors.append({\"country\": country, \"year\": year, \"key\": key, \"stage\": \"poll\", \"error\": \"timeout\"})\n",
        "\n",
        "        if pending:\n",
        "            time.sleep(GBIF_POLL_INTERVAL_S)\n",
        "\n",
        "    log.info(\"Batch %d/%d complete. Running totals: %d succeeded, %d failed.\",\n",
        "             batch_num, n_batches, len(completed), len(errors))\n",
        "\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────────────────────\n",
        "print()\n",
        "print(\"═\" * 60)\n",
        "print(f\"ETL complete: {len(completed)} succeeded, {len(errors)} failed\")\n",
        "print(\"═\" * 60)\n",
        "\n",
        "if completed:\n",
        "    print(\"\\nCompleted jobs:\")\n",
        "    display(pd.DataFrame(completed))\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nFailed jobs:\")\n",
        "    display(pd.DataFrame(errors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading from: s3://ie-datalake/bronze/gbif/country=ES/year=2025\n",
            "\n",
            "Shape: 1,073,069 rows × 60 columns\n",
            "Columns: ['gbifid', 'datasetkey', 'occurrenceid', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'infraspecificepithet', 'taxonrank', 'scientificname', 'verbatimscientificname', 'verbatimscientificnameauthorship', 'countrycode', 'locality', 'stateprovince', 'occurrencestatus', 'individualcount', 'publishingorgkey', 'decimallatitude', 'decimallongitude', 'coordinateuncertaintyinmeters', 'coordinateprecision', 'elevation', 'elevationaccuracy', 'depth', 'depthaccuracy', 'eventdate', 'day', 'month', 'year', 'taxonkey', 'specieskey', 'basisofrecord', 'institutioncode', 'collectioncode', 'catalognumber', 'recordnumber', 'identifiedby', 'dateidentified', 'license', 'rightsholder', 'recordedby', 'typestatus', 'establishmentmeans', 'lastinterpreted', 'mediatype', 'issue', 'country', 'is_threatened', 'iucn_cat', 'is_invasive_em', 'is_introduced', 'is_naturalized', 'is_invasive_doe', 'is_invasive_api', 'is_introduced_api', 'is_invasive_any']\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gbifid</th>\n",
              "      <th>datasetkey</th>\n",
              "      <th>occurrenceid</th>\n",
              "      <th>kingdom</th>\n",
              "      <th>phylum</th>\n",
              "      <th>class</th>\n",
              "      <th>order</th>\n",
              "      <th>family</th>\n",
              "      <th>genus</th>\n",
              "      <th>species</th>\n",
              "      <th>...</th>\n",
              "      <th>country</th>\n",
              "      <th>is_threatened</th>\n",
              "      <th>iucn_cat</th>\n",
              "      <th>is_invasive_em</th>\n",
              "      <th>is_introduced</th>\n",
              "      <th>is_naturalized</th>\n",
              "      <th>is_invasive_doe</th>\n",
              "      <th>is_invasive_api</th>\n",
              "      <th>is_introduced_api</th>\n",
              "      <th>is_invasive_any</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5863345736</td>\n",
              "      <td>040c5662-da76-4782-a48e-cdea1892d14c</td>\n",
              "      <td>MUSBA3967-25</td>\n",
              "      <td>Animalia</td>\n",
              "      <td>Mollusca</td>\n",
              "      <td>Gastropoda</td>\n",
              "      <td>Aplysiida</td>\n",
              "      <td>Aplysiidae</td>\n",
              "      <td>Aplysia</td>\n",
              "      <td>Aplysia punctata</td>\n",
              "      <td>...</td>\n",
              "      <td>ES</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5863818323</td>\n",
              "      <td>040c5662-da76-4782-a48e-cdea1892d14c</td>\n",
              "      <td>GBAAW37591-24</td>\n",
              "      <td>Animalia</td>\n",
              "      <td>Arthropoda</td>\n",
              "      <td>Insecta</td>\n",
              "      <td>Hymenoptera</td>\n",
              "      <td>Aphelinidae</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>ES</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5864717037</td>\n",
              "      <td>040c5662-da76-4782-a48e-cdea1892d14c</td>\n",
              "      <td>MUSBA3973-25</td>\n",
              "      <td>Animalia</td>\n",
              "      <td>Mollusca</td>\n",
              "      <td>Gastropoda</td>\n",
              "      <td>Littorinimorpha</td>\n",
              "      <td>Pterotracheidae</td>\n",
              "      <td>Firoloida</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>ES</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5860671815</td>\n",
              "      <td>040c5662-da76-4782-a48e-cdea1892d14c</td>\n",
              "      <td>MUSBA3983-25</td>\n",
              "      <td>Animalia</td>\n",
              "      <td>Mollusca</td>\n",
              "      <td>Gastropoda</td>\n",
              "      <td>Cephalaspidea</td>\n",
              "      <td>Haminoeidae</td>\n",
              "      <td>Haminoea</td>\n",
              "      <td>Haminoea orbignyana</td>\n",
              "      <td>...</td>\n",
              "      <td>ES</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5860098267</td>\n",
              "      <td>040c5662-da76-4782-a48e-cdea1892d14c</td>\n",
              "      <td>MUSBA3743-25</td>\n",
              "      <td>Animalia</td>\n",
              "      <td>Mollusca</td>\n",
              "      <td>Gastropoda</td>\n",
              "      <td>Nudibranchia</td>\n",
              "      <td>Cuthonidae</td>\n",
              "      <td>Cuthona</td>\n",
              "      <td>Cuthona pallida</td>\n",
              "      <td>...</td>\n",
              "      <td>ES</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       gbifid                            datasetkey   occurrenceid   kingdom  \\\n",
              "0  5863345736  040c5662-da76-4782-a48e-cdea1892d14c   MUSBA3967-25  Animalia   \n",
              "1  5863818323  040c5662-da76-4782-a48e-cdea1892d14c  GBAAW37591-24  Animalia   \n",
              "2  5864717037  040c5662-da76-4782-a48e-cdea1892d14c   MUSBA3973-25  Animalia   \n",
              "3  5860671815  040c5662-da76-4782-a48e-cdea1892d14c   MUSBA3983-25  Animalia   \n",
              "4  5860098267  040c5662-da76-4782-a48e-cdea1892d14c   MUSBA3743-25  Animalia   \n",
              "\n",
              "       phylum       class            order           family      genus  \\\n",
              "0    Mollusca  Gastropoda        Aplysiida       Aplysiidae    Aplysia   \n",
              "1  Arthropoda     Insecta      Hymenoptera      Aphelinidae        NaN   \n",
              "2    Mollusca  Gastropoda  Littorinimorpha  Pterotracheidae  Firoloida   \n",
              "3    Mollusca  Gastropoda    Cephalaspidea      Haminoeidae   Haminoea   \n",
              "4    Mollusca  Gastropoda     Nudibranchia       Cuthonidae    Cuthona   \n",
              "\n",
              "               species  ... country is_threatened iucn_cat is_invasive_em  \\\n",
              "0     Aplysia punctata  ...      ES         False     <NA>          False   \n",
              "1                  NaN  ...      ES         False     <NA>          False   \n",
              "2                  NaN  ...      ES         False     <NA>          False   \n",
              "3  Haminoea orbignyana  ...      ES         False     <NA>          False   \n",
              "4      Cuthona pallida  ...      ES         False     <NA>          False   \n",
              "\n",
              "  is_introduced is_naturalized is_invasive_doe is_invasive_api  \\\n",
              "0         False          False           False           False   \n",
              "1         False          False           False           False   \n",
              "2         False          False           False           False   \n",
              "3         False          False           False           False   \n",
              "4         False          False           False           False   \n",
              "\n",
              "  is_introduced_api  is_invasive_any  \n",
              "0             False            False  \n",
              "1             False            False  \n",
              "2             False            False  \n",
              "3             False            False  \n",
              "4             False            False  \n",
              "\n",
              "[5 rows x 60 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Read from S3 → pandas\n",
        "# Adjust READ_COUNTRY / READ_YEAR to preview a specific partition,\n",
        "# or set both to None to read everything (can be large).\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "READ_COUNTRY = \"ES\"   # e.g. \"ES\" – or None for all countries\n",
        "READ_YEAR    = 2025   # e.g. 2024 – or None for all years\n",
        "\n",
        "import pyarrow.dataset as ds\n",
        "import pyarrow.parquet as pq\n",
        "import s3fs\n",
        "import pandas as pd\n",
        "\n",
        "_fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "\n",
        "if READ_COUNTRY and READ_YEAR:\n",
        "    s3_path = f\"{S3_BUCKET}/{S3_PREFIX}/country={READ_COUNTRY}/year={READ_YEAR}\"\n",
        "elif READ_COUNTRY:\n",
        "    s3_path = f\"{S3_BUCKET}/{S3_PREFIX}/country={READ_COUNTRY}\"\n",
        "elif READ_YEAR:\n",
        "    s3_path = f\"{S3_BUCKET}/{S3_PREFIX}\"\n",
        "else:\n",
        "    s3_path = f\"{S3_BUCKET}/{S3_PREFIX}\"\n",
        "\n",
        "print(f\"Reading from: s3://{s3_path}\")\n",
        "\n",
        "dataset = ds.dataset(s3_path, filesystem=_fs, format=\"parquet\")\n",
        "df = dataset.to_table().to_pandas()\n",
        "\n",
        "print(f\"\\nShape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "print(f\"Columns: {list(df.columns)}\\n\")\n",
        "\n",
        "display(df.head(5))\n",
        "# display(df.dtypes.rename(\"dtype\").to_frame())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_invasive_any\n",
              "False    1047224\n",
              "True       25845\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"is_invasive_any\"].value_counts()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
