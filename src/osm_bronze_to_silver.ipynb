{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OSM Bronze → Silver: Energy & Infrastructure Features\n",
        "\n",
        "Extracts OpenStreetMap features for Spain focused on **energy, extraction, transport infrastructure** (plus corridor and constraint layers) into a single silver Parquet table.\n",
        "\n",
        "## Tooling choice: **Pyosmium**\n",
        "\n",
        "| Tool | Pros | Cons | Verdict |\n",
        "|------|------|------|--------|\n",
        "| **Pyosmium** | Native Python, streaming, built-in geometry (WKBFactory), area callback for multipolygons, single-process | Requires ~2–4 GB RAM for Spain PBF | ✅ **Selected** |\n",
        "| osmium-tool | Fast CLI filter | Outputs PBF, needs separate parser; no direct Parquet | Extra step |\n",
        "| GDAL/ogr2ogr | OSM driver, GeoPackage | Slow for 1.3 GB; not recommended for large extracts | ❌ |\n",
        "| osm2pgsql | Battle-tested, PostGIS | Outputs Postgres, not Parquet; needs export | ❌ |\n",
        "| Spark+Sedona | Scalable | Overkill for single country; infra complexity | ❌ |\n",
        "\n",
        "**Pyosmium** provides: (1) single-pass PBF read with `with_locations()`, (2) `area` callback for relations → automatic multipolygon assembly, (3) `WKBFactory` for Point/LineString/MultiPolygon, (4) batch-friendly accumulation + Parquet writes.\n",
        "\n",
        "## Input / Output\n",
        "\n",
        "| Layer | S3 path |\n",
        "|-------|--------|\n",
        "| Bronze | `s3://<bucket>/bronze/osm/spain-latest.osm.pbf` |\n",
        "| Silver | `s3://<bucket>/silver/osm/features_energy/country=ES/snapshot_date=YYYY-MM-DD/feature_type=<TYPE>/part-*.parquet` |\n",
        "\n",
        "## Feature types (31 total)\n",
        "\n",
        "Energy/infra + Water & wetness + Protected + Human footprint + Landuse + Barriers + Green/urban + Trails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "COUNTRY = \"ES\"\n",
        "S3_BUCKET = \"ie-datalake\"\n",
        "BRONZE_PREFIX = \"bronze/osm\"\n",
        "SILVER_PREFIX = \"silver/osm/features_energy\"\n",
        "AWS_PROFILE = \"486717354268_PowerUserAccess\"\n",
        "\n",
        "# Input PBF (local path – already downloaded)\n",
        "PBF_PATH = \"../data/tmpptqi3rfv.osm.pbf\"\n",
        "\n",
        "# Bbox split: Spain ~(-9.5,36) to (4.5,43.5), 20 tiles = 4x5 grid\n",
        "N_BBOX_COLS = 4\n",
        "N_BBOX_ROWS = 5\n",
        "SPAIN_BBOX = (-9.5, 36.0, 4.5, 43.5)  # minlon, minlat, maxlon, maxlat\n",
        "BBOX_DIR = \"../data/osm_bboxes\"  # output of split, input for process loop\n",
        "\n",
        "# Snapshot date\n",
        "from datetime import date\n",
        "SNAPSHOT_DATE = date.today().isoformat()\n",
        "\n",
        "# Batch size for Parquet writes\n",
        "BATCH_SIZE = 100_000\n",
        "PARQUET_COMPRESSION = \"snappy\"\n",
        "\n",
        "# Write: direct to S3 per batch (no parallel, no local-first)\n",
        "STORAGE_OPTIONS = {\"profile\": AWS_PROFILE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q osmium geopandas pyarrow s3fs h3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import tempfile\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import geopandas as gpd\n",
        "import h3\n",
        "import osmium as osm\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import s3fs\n",
        "from shapely import wkb\n",
        "from shapely.geometry import shape\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    force=True,\n",
        ")\n",
        "log = logging.getLogger(\"osm_silver\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature type detection & tag filters\n",
        "\n",
        "Each feature type has tag conditions (OR within group). First match wins. Promote typed columns per spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURE_TYPES = [\n",
        "    # Energy & infrastructure (original)\n",
        "    \"PIPELINES\", \"POWER_LINES\", \"POWER_SUBSTATIONS\", \"POWER_PLANTS\",\n",
        "    \"INDUSTRIAL_AREAS\", \"STORAGE_TANKS\", \"FUEL_STATIONS\",\n",
        "    \"PORTS_TERMINALS\", \"AIRPORTS\", \"ROADS\", \"RAIL\",\n",
        "    \"ADMIN_BOUNDARIES\", \"PROTECTED_AREAS\",\n",
        "    # A) Water & wetness\n",
        "    \"WATERWAYS\", \"WATERBODIES\", \"WETLANDS\", \"COASTLINE\",\n",
        "    \"WATER_BARRIERS\", \"WATER_INFRA_POI\",\n",
        "    # B) Protected / regulated\n",
        "    \"RESTRICTED_AREAS\",\n",
        "    # C) Human footprint\n",
        "    \"BUILDINGS\", \"AMENITIES_POI\", \"WASTE_POLLUTION\",\n",
        "    # D) Landuse / habitat\n",
        "    \"LANDUSE_AGRICULTURE\", \"FORESTRY_MANAGED\", \"NATURAL_HABITATS\",\n",
        "    # E) Barriers & fragmentation\n",
        "    \"BARRIERS\", \"LINEAR_DISTURBANCE\",\n",
        "    # F) Green/urban ecology\n",
        "    \"PARKS_GREEN_URBAN\", \"TREE_ROWS_HEDGEROWS\",\n",
        "    # G) Soft transport\n",
        "    \"TRAILS_TRACKS\",\n",
        "]\n",
        "\n",
        "\n",
        "def _tags_dict(obj) -> dict[str, str]:\n",
        "    return {t.k: t.v for t in obj.tags}\n",
        "\n",
        "\n",
        "def _get_tag(tags: dict[str, str], *keys: str) -> str | None:\n",
        "    for k in keys:\n",
        "        if k in tags and tags[k]:\n",
        "            return tags[k]\n",
        "    return None\n",
        "\n",
        "\n",
        "def match_feature_type(tags: dict[str, str]) -> tuple[str | None, dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns (feature_type, promoted_cols).\n",
        "    First matching feature type wins. promoted_cols holds typed columns for that type.\n",
        "    \"\"\"\n",
        "    # A) PIPELINES\n",
        "    if tags.get(\"man_made\") == \"pipeline\" or tags.get(\"pipeline\") or tags.get(\"substance\") or tags.get(\"location\") in (\"underground\", \"overground\"):\n",
        "        return \"PIPELINES\", {\n",
        "            \"pipeline\": tags.get(\"pipeline\"),\n",
        "            \"substance\": tags.get(\"substance\"),\n",
        "            \"location\": tags.get(\"location\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"ref\": tags.get(\"ref\"),\n",
        "        }\n",
        "\n",
        "    # B) POWER_LINES\n",
        "    if tags.get(\"power\") in (\"line\", \"minor_line\", \"cable\"):\n",
        "        return \"POWER_LINES\", {\n",
        "            \"power\": tags.get(\"power\"),\n",
        "            \"voltage\": tags.get(\"voltage\"),\n",
        "            \"cables\": tags.get(\"cables\"),\n",
        "            \"location\": tags.get(\"location\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # C) POWER_SUBSTATIONS\n",
        "    if tags.get(\"power\") == \"substation\":\n",
        "        return \"POWER_SUBSTATIONS\", {\n",
        "            \"substation\": tags.get(\"substation\"),\n",
        "            \"voltage\": tags.get(\"voltage\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # D) POWER_PLANTS\n",
        "    if tags.get(\"power\") in (\"plant\", \"generator\"):\n",
        "        return \"POWER_PLANTS\", {\n",
        "            \"plant_source\": tags.get(\"plant:source\") or tags.get(\"plant_source\"),\n",
        "            \"generator_source\": tags.get(\"generator:source\") or tags.get(\"generator_source\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"capacity\": tags.get(\"plant:output:electricity\") or tags.get(\"capacity\"),\n",
        "            \"power\": tags.get(\"power\"),\n",
        "        }\n",
        "\n",
        "    # E) INDUSTRIAL_AREAS\n",
        "    if tags.get(\"landuse\") == \"industrial\" or tags.get(\"industrial\") or tags.get(\"man_made\") == \"works\":\n",
        "        return \"INDUSTRIAL_AREAS\", {\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"industrial\": tags.get(\"industrial\"),\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # F) STORAGE_TANKS\n",
        "    if tags.get(\"man_made\") == \"storage_tank\" or tags.get(\"landuse\") == \"depot\" or tags.get(\"industrial\") in (\"oil\", \"chemical\"):\n",
        "        return \"STORAGE_TANKS\", {\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"content\": tags.get(\"content\") or tags.get(\"substance\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # G) FUEL_STATIONS\n",
        "    if tags.get(\"amenity\") == \"fuel\":\n",
        "        return \"FUEL_STATIONS\", {\n",
        "            \"amenity\": tags.get(\"amenity\"),\n",
        "            \"brand\": tags.get(\"brand\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"opening_hours\": tags.get(\"opening_hours\"),\n",
        "        }\n",
        "\n",
        "    # H) PORTS_TERMINALS\n",
        "    if tags.get(\"harbour\") or tags.get(\"landuse\") == \"port\" or tags.get(\"man_made\") == \"pier\" or tags.get(\"amenity\") == \"ferry_terminal\":\n",
        "        return \"PORTS_TERMINALS\", {\n",
        "            \"harbour\": tags.get(\"harbour\"),\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # I) AIRPORTS\n",
        "    if tags.get(\"aeroway\") in (\"aerodrome\", \"terminal\", \"runway\", \"taxiway\"):\n",
        "        return \"AIRPORTS\", {\n",
        "            \"aeroway\": tags.get(\"aeroway\"),\n",
        "            \"iata\": tags.get(\"iata\"),\n",
        "            \"icao\": tags.get(\"icao\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # G) TRAILS_TRACKS (before ROADS – more specific)\n",
        "    if tags.get(\"highway\") in (\"path\", \"track\", \"footway\", \"bridleway\", \"cycleway\"):\n",
        "        return \"TRAILS_TRACKS\", {\n",
        "            \"highway\": tags.get(\"highway\"),\n",
        "            \"surface\": tags.get(\"surface\"),\n",
        "            \"access\": tags.get(\"access\"),\n",
        "            \"sac_scale\": tags.get(\"sac_scale\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "\n",
        "    # J) ROADS\n",
        "    if tags.get(\"highway\"):\n",
        "        return \"ROADS\", {\n",
        "            \"highway\": tags.get(\"highway\"),\n",
        "            \"surface\": tags.get(\"surface\"),\n",
        "            \"bridge\": tags.get(\"bridge\"),\n",
        "            \"tunnel\": tags.get(\"tunnel\"),\n",
        "            \"oneway\": tags.get(\"oneway\"),\n",
        "            \"maxspeed\": tags.get(\"maxspeed\"),\n",
        "            \"access\": tags.get(\"access\"),\n",
        "            \"ref\": tags.get(\"ref\"),\n",
        "        }\n",
        "\n",
        "    # K) RAIL\n",
        "    if tags.get(\"railway\") in (\"rail\", \"light_rail\", \"subway\", \"tram\"):\n",
        "        return \"RAIL\", {\n",
        "            \"railway\": tags.get(\"railway\"),\n",
        "            \"electrified\": tags.get(\"electrified\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"usage\": tags.get(\"usage\"),\n",
        "        }\n",
        "\n",
        "    # L) ADMIN_BOUNDARIES\n",
        "    if tags.get(\"boundary\") == \"administrative\":\n",
        "        return \"ADMIN_BOUNDARIES\", {\n",
        "            \"admin_level\": tags.get(\"admin_level\"),\n",
        "            \"boundary\": tags.get(\"boundary\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "\n",
        "    # M) PROTECTED_AREAS\n",
        "    if tags.get(\"boundary\") == \"protected_area\" or tags.get(\"leisure\") == \"nature_reserve\":\n",
        "        return \"PROTECTED_AREAS\", {\n",
        "            \"protect_class\": tags.get(\"protect_class\"),\n",
        "            \"designation\": tags.get(\"designation\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "\n",
        "    # A) Water & wetness\n",
        "    if tags.get(\"waterway\") in (\"dam\", \"weir\", \"lock_gate\") or tags.get(\"man_made\") == \"dam\":\n",
        "        return \"WATER_BARRIERS\", {\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"waterway\": tags.get(\"waterway\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "    if tags.get(\"waterway\"):\n",
        "        return \"WATERWAYS\", {\n",
        "            \"waterway\": tags.get(\"waterway\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"intermittent\": tags.get(\"intermittent\"),\n",
        "            \"tunnel\": tags.get(\"tunnel\"),\n",
        "            \"width\": tags.get(\"width\"),\n",
        "        }\n",
        "    if tags.get(\"natural\") == \"water\" or tags.get(\"water\"):\n",
        "        return \"WATERBODIES\", {\n",
        "            \"water\": tags.get(\"water\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"reservoir\": tags.get(\"reservoir\"),\n",
        "        }\n",
        "    if tags.get(\"natural\") == \"wetland\" or tags.get(\"wetland\"):\n",
        "        return \"WETLANDS\", {\"wetland\": tags.get(\"wetland\"), \"name\": tags.get(\"name\")}\n",
        "    if tags.get(\"natural\") == \"coastline\":\n",
        "        return \"COASTLINE\", {\"name\": tags.get(\"name\")}\n",
        "    if tags.get(\"man_made\") in (\"water_tower\",) or tags.get(\"amenity\") == \"drinking_water\" or tags.get(\"emergency\") == \"water_tank\":\n",
        "        return \"WATER_INFRA_POI\", {\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"amenity\": tags.get(\"amenity\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # B) RESTRICTED_AREAS\n",
        "    if tags.get(\"military\") or tags.get(\"landuse\") == \"military\" or tags.get(\"boundary\") == \"military\":\n",
        "        return \"RESTRICTED_AREAS\", {\n",
        "            \"military\": tags.get(\"military\"),\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "\n",
        "    # C) Human footprint\n",
        "    if tags.get(\"building\"):\n",
        "        return \"BUILDINGS\", {\n",
        "            \"building\": tags.get(\"building\"),\n",
        "            \"building_levels\": tags.get(\"building:levels\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "    if tags.get(\"amenity\") in (\"school\", \"hospital\", \"university\", \"marketplace\", \"prison\", \"waste_disposal\", \"recycling\"):\n",
        "        return \"AMENITIES_POI\", {\n",
        "            \"amenity\": tags.get(\"amenity\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "    if tags.get(\"landuse\") == \"landfill\" or tags.get(\"man_made\") == \"wastewater_plant\":\n",
        "        return \"WASTE_POLLUTION\", {\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"man_made\": tags.get(\"man_made\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"operator\": tags.get(\"operator\"),\n",
        "        }\n",
        "\n",
        "    # D) Landuse / habitat\n",
        "    if tags.get(\"landuse\") in (\"farmland\", \"farmyard\", \"orchard\", \"vineyard\", \"meadow\"):\n",
        "        return \"LANDUSE_AGRICULTURE\", {\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"crop\": tags.get(\"crop\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "    if tags.get(\"landuse\") == \"forest\" or tags.get(\"forest\"):\n",
        "        return \"FORESTRY_MANAGED\", {\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"forest\": tags.get(\"forest\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "    if tags.get(\"natural\") in (\"wood\", \"heath\", \"scrub\", \"grassland\", \"bare_rock\", \"sand\", \"beach\", \"cliff\"):\n",
        "        return \"NATURAL_HABITATS\", {\"natural\": tags.get(\"natural\"), \"name\": tags.get(\"name\")}\n",
        "\n",
        "    # E) Barriers & fragmentation\n",
        "    if tags.get(\"natural\") == \"tree_row\" or tags.get(\"barrier\") == \"hedge\":\n",
        "        return \"TREE_ROWS_HEDGEROWS\", {\"natural\": tags.get(\"natural\"), \"barrier\": tags.get(\"barrier\")}\n",
        "    if tags.get(\"barrier\") in (\"fence\", \"wall\", \"hedge\", \"gate\", \"bollard\"):\n",
        "        return \"BARRIERS\", {\"barrier\": tags.get(\"barrier\"), \"access\": tags.get(\"access\")}\n",
        "    if tags.get(\"man_made\") in (\"cutline\", \"embankment\") or tags.get(\"barrier\") == \"ditch\":\n",
        "        return \"LINEAR_DISTURBANCE\", {\"man_made\": tags.get(\"man_made\"), \"barrier\": tags.get(\"barrier\")}\n",
        "\n",
        "    # F) Green/urban ecology\n",
        "    if tags.get(\"leisure\") in (\"park\", \"garden\", \"common\", \"golf_course\", \"pitch\") or tags.get(\"landuse\") == \"recreation_ground\":\n",
        "        return \"PARKS_GREEN_URBAN\", {\n",
        "            \"leisure\": tags.get(\"leisure\"),\n",
        "            \"landuse\": tags.get(\"landuse\"),\n",
        "            \"name\": tags.get(\"name\"),\n",
        "        }\n",
        "\n",
        "    return None, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All promoted columns – every row gets all keys (str or None) for consistent Parquet schema\n",
        "PROMOTED_KEYS = (\n",
        "    \"pipeline\", \"substance\", \"location\", \"operator\", \"ref\",\n",
        "    \"power\", \"voltage\", \"cables\", \"substation\", \"plant_source\", \"generator_source\", \"capacity\",\n",
        "    \"landuse\", \"industrial\", \"man_made\", \"content\", \"amenity\", \"brand\", \"opening_hours\",\n",
        "    \"harbour\", \"aeroway\", \"iata\", \"icao\",\n",
        "    \"highway\", \"surface\", \"bridge\", \"tunnel\", \"oneway\", \"maxspeed\", \"access\",\n",
        "    \"railway\", \"electrified\", \"usage\",\n",
        "    \"admin_level\", \"boundary\", \"protect_class\", \"designation\",\n",
        "    \"waterway\", \"water\", \"wetland\", \"intermittent\", \"width\", \"reservoir\",\n",
        "    \"military\", \"building\", \"building_levels\", \"crop\", \"forest\",\n",
        "    \"natural\", \"barrier\", \"sac_scale\", \"leisure\",\n",
        ")\n",
        "\n",
        "# Keys to keep in tags_json (prune to avoid huge blobs)\n",
        "TAGS_JSON_KEYS = frozenset([\n",
        "    \"name\", \"operator\", \"ref\", \"brand\", \"opening_hours\",\n",
        "    \"pipeline\", \"substance\", \"location\", \"power\", \"voltage\", \"cables\",\n",
        "    \"substation\", \"plant:source\", \"generator:source\", \"plant:output:electricity\", \"capacity\",\n",
        "    \"landuse\", \"industrial\", \"man_made\", \"content\", \"amenity\",\n",
        "    \"harbour\", \"aeroway\", \"iata\", \"icao\",\n",
        "    \"highway\", \"surface\", \"bridge\", \"tunnel\", \"oneway\", \"maxspeed\", \"access\",\n",
        "    \"railway\", \"electrified\", \"usage\",\n",
        "    \"boundary\", \"admin_level\", \"protect_class\", \"designation\", \"leisure\",\n",
        "    \"waterway\", \"water\", \"wetland\", \"intermittent\", \"width\", \"reservoir\",\n",
        "    \"military\", \"building\", \"building:levels\", \"crop\", \"forest\",\n",
        "    \"natural\", \"barrier\", \"sac_scale\", \"emergency\",\n",
        "])\n",
        "\n",
        "\n",
        "def prune_tags(tags: dict[str, str]) -> dict[str, str]:\n",
        "    return {k: v for k, v in tags.items() if k in TAGS_JSON_KEYS and not k.startswith(\"name:\")}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pyosmium handler: extract features → batch Parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_h3_from_point(lat: float, lon: float) -> dict[str, str]:\n",
        "    try:\n",
        "        h9 = h3.latlng_to_cell(lat, lon, 9)\n",
        "        return {\n",
        "            \"h3_6\": h3.cell_to_parent(h9, 6),\n",
        "            \"h3_7\": h3.cell_to_parent(h9, 7),\n",
        "            \"h3_8\": h3.cell_to_parent(h9, 8),\n",
        "            \"h3_9\": h9,\n",
        "        }\n",
        "    except Exception:\n",
        "        return {\"h3_6\": None, \"h3_7\": None, \"h3_8\": None, \"h3_9\": None}\n",
        "\n",
        "\n",
        "def centroid_from_geom(geom) -> tuple[float | None, float | None]:\n",
        "    \"\"\"Use representative_point for polygons (PointOnSurface), centroid for points/lines.\"\"\"\n",
        "    if geom is None or geom.is_empty:\n",
        "        return None, None\n",
        "    try:\n",
        "        if geom.geom_type in (\"Polygon\", \"MultiPolygon\"):\n",
        "            pt = geom.representative_point()\n",
        "        else:\n",
        "            pt = geom.centroid\n",
        "        return float(pt.x), float(pt.y)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def length_area_from_geom(geom) -> tuple[float, float]:\n",
        "    \"\"\"Length (m) and area (m²) in EPSG:25830 for Gold layer.\"\"\"\n",
        "    if geom is None or geom.is_empty:\n",
        "        return 0.0, 0.0\n",
        "    try:\n",
        "        from shapely.ops import transform\n",
        "        from pyproj import Transformer\n",
        "        transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:25830\", always_xy=True)\n",
        "        geom_p = transform(lambda x, y: transformer.transform(x, y), geom)\n",
        "        length = geom_p.length if geom_p.geom_type in (\"LineString\", \"MultiLineString\") else 0.0\n",
        "        area = geom_p.area if geom_p.geom_type in (\"Polygon\", \"MultiPolygon\") else 0.0\n",
        "        return float(length), float(area)\n",
        "    except Exception:\n",
        "        return 0.0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnergyFeaturesHandler(osm.SimpleHandler):\n",
        "    \"\"\"\n",
        "    Extracts OSM features matching energy/infrastructure tags.\n",
        "    - Nodes → Point\n",
        "    - Ways (non-closed) → LineString\n",
        "    - Areas (closed ways + multipolygon relations) → Polygon/MultiPolygon\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wkb_factory: osm.geom.WKBFactory, country: str, snapshot_date: str, batch_callback):\n",
        "        super().__init__()\n",
        "        self.wkb_factory = wkb_factory\n",
        "        self.country = country\n",
        "        self.snapshot_date = snapshot_date\n",
        "        self.batch_callback = batch_callback\n",
        "        self._batch: list[dict] = []\n",
        "        self._counts: dict[str, int] = {ft: 0 for ft in FEATURE_TYPES}\n",
        "\n",
        "    def _emit(self, osm_id: str, osm_type: str, feature_type: str, geom_wkb: bytes, tags: dict, promoted: dict):\n",
        "        if geom_wkb is None:\n",
        "            geom = None\n",
        "        elif isinstance(geom_wkb, bytes):\n",
        "            geom = wkb.loads(geom_wkb)\n",
        "        else:\n",
        "            geom = wkb.loads(bytes.fromhex(geom_wkb))  # Pyosmium returns hex string\n",
        "        lon, lat = centroid_from_geom(geom)\n",
        "        if lat is None or lon is None:\n",
        "            return\n",
        "        length_m, area_m2 = length_area_from_geom(geom)\n",
        "        h3_cols = add_h3_from_point(lat, lon)\n",
        "        tags_pruned = prune_tags(tags)\n",
        "        # All promoted columns as str/None – consistent schema across partitions for Athena\n",
        "        promoted_full = {k: (str(v) if v is not None else None) for k in PROMOTED_KEYS for v in [promoted.get(k)]}\n",
        "        row = {\n",
        "            \"osm_id\": osm_id,\n",
        "            \"osm_type\": osm_type,\n",
        "            \"feature_type\": feature_type,\n",
        "            \"geometry\": geom,\n",
        "            \"length_m\": length_m,\n",
        "            \"area_m2\": area_m2,\n",
        "            \"centroid_lon\": lon,\n",
        "            \"centroid_lat\": lat,\n",
        "            **h3_cols,\n",
        "            \"name\": tags.get(\"name\"),\n",
        "            \"tags_json\": json.dumps(tags_pruned) if tags_pruned else None,\n",
        "            \"snapshot_date\": self.snapshot_date,\n",
        "            \"country\": self.country,\n",
        "            **promoted_full,\n",
        "        }\n",
        "        self._batch.append(row)\n",
        "        self._counts[feature_type] = self._counts.get(feature_type, 0) + 1\n",
        "        if len(self._batch) >= BATCH_SIZE:\n",
        "            self._flush()\n",
        "\n",
        "    def _flush(self):\n",
        "        if not self._batch:\n",
        "            return\n",
        "        self.batch_callback(self._batch)\n",
        "        self._batch = []\n",
        "\n",
        "    def node(self, n: osm.Node):\n",
        "        if not n.location.valid():\n",
        "            return\n",
        "        tags = _tags_dict(n)\n",
        "        ft, promoted = match_feature_type(tags)\n",
        "        if ft is None:\n",
        "            return\n",
        "        try:\n",
        "            wkb_bytes = self.wkb_factory.create_point(n)\n",
        "        except Exception:\n",
        "            return\n",
        "        self._emit(f\"n{n.id}\", \"node\", ft, wkb_bytes, tags, promoted)\n",
        "\n",
        "    def way(self, w: osm.Way):\n",
        "        if w.is_closed() and len(w.nodes) >= 4:\n",
        "            return  # Let area callback handle closed ways\n",
        "        tags = _tags_dict(w)\n",
        "        ft, promoted = match_feature_type(tags)\n",
        "        if ft is None:\n",
        "            return\n",
        "        try:\n",
        "            wkb_bytes = self.wkb_factory.create_linestring(w)\n",
        "        except Exception:\n",
        "            return\n",
        "        self._emit(f\"w{w.id}\", \"way\", ft, wkb_bytes, tags, promoted)\n",
        "\n",
        "    def area(self, a: osm.Area):\n",
        "        tags = _tags_dict(a)\n",
        "        ft, promoted = match_feature_type(tags)\n",
        "        if ft is None:\n",
        "            return\n",
        "        try:\n",
        "            wkb_bytes = self.wkb_factory.create_multipolygon(a)\n",
        "        except Exception:\n",
        "            return\n",
        "        osm_type = \"way\" if a.from_way() else \"relation\"\n",
        "        osm_id = f\"w{a.orig_id()}\" if a.from_way() else f\"r{a.orig_id()}\"\n",
        "        self._emit(osm_id, osm_type, ft, wkb_bytes, tags, promoted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_schema_with_promoted() -> dict[str, pa.DataType]:\n",
        "    base = {\n",
        "        \"osm_id\": pa.string(),\n",
        "        \"osm_type\": pa.string(),\n",
        "        \"feature_type\": pa.string(),\n",
        "        \"centroid_lon\": pa.float64(),\n",
        "        \"centroid_lat\": pa.float64(),\n",
        "        \"h3_6\": pa.string(),\n",
        "        \"h3_7\": pa.string(),\n",
        "        \"h3_8\": pa.string(),\n",
        "        \"h3_9\": pa.string(),\n",
        "        \"name\": pa.string(),\n",
        "        \"tags_json\": pa.string(),\n",
        "        \"snapshot_date\": pa.string(),\n",
        "        \"country\": pa.string(),\n",
        "    }\n",
        "    for k in PROMOTED_KEYS:\n",
        "        base[k] = pa.string()\n",
        "    return base\n",
        "\n",
        "\n",
        "def _table_no_dictionary(tbl: pa.Table) -> pa.Table:\n",
        "    \"\"\"Decode all dictionary columns to plain string – avoids Parquet merge errors.\"\"\"\n",
        "    for i in range(tbl.num_columns):\n",
        "        if pa.types.is_dictionary(tbl.schema.field(i).type):\n",
        "            tbl = tbl.set_column(i, tbl.schema.field(i).name, tbl.column(i).dictionary_decode())\n",
        "    return tbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_part_counter_from_s3(\n",
        "    s3_fs: s3fs.S3FileSystem,\n",
        "    country: str,\n",
        "    snapshot_date: str,\n",
        ") -> dict[str, int]:\n",
        "    \"\"\"Load part_counter from existing S3 files (for resume).\"\"\"\n",
        "    base = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/snapshot_date={snapshot_date}\"\n",
        "    part_counter = {ft: 0 for ft in FEATURE_TYPES}\n",
        "    for ft in FEATURE_TYPES:\n",
        "        prefix = f\"{base}/feature_type={ft}/\"\n",
        "        files = s3_fs.glob(f\"{prefix}part-*.parquet\")\n",
        "        if files:\n",
        "            nums = sorted(int(f.split(\"part-\")[1].split(\".\")[0]) for f in files)\n",
        "            part_counter[ft] = max(nums) if nums else 0\n",
        "    return part_counter\n",
        "\n",
        "\n",
        "def process_pbf_and_write(\n",
        "    pbf_path: str,\n",
        "    country: str,\n",
        "    snapshot_date: str,\n",
        "    s3_fs: s3fs.S3FileSystem,\n",
        "    part_counter: dict[str, int] | None = None,\n",
        ") -> dict[str, int]:\n",
        "    \"\"\"Process PBF, write directly to S3 per batch. part_counter persists across bbox runs.\"\"\"\n",
        "    wkb_factory = osm.geom.WKBFactory()\n",
        "    if part_counter is None:\n",
        "        part_counter = {ft: 0 for ft in FEATURE_TYPES}\n",
        "    s3_prefix = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/snapshot_date={snapshot_date}\"\n",
        "\n",
        "    STRING_COLS = list(PROMOTED_KEYS) + [\"osm_id\", \"osm_type\", \"feature_type\", \"name\", \"tags_json\", \"snapshot_date\", \"country\", \"h3_6\", \"h3_7\", \"h3_8\", \"h3_9\"]\n",
        "\n",
        "    def _coerce_string_cols(df):\n",
        "        \"\"\"Ensure all string columns are str|None (no NaN) for consistent Parquet schema.\"\"\"\n",
        "        for c in STRING_COLS:\n",
        "            if c not in df.columns:\n",
        "                continue\n",
        "            s = df[c]\n",
        "            null_mask = s.isna()\n",
        "            df[c] = s.where(~null_mask).astype(str).mask(null_mask, None)\n",
        "        return df\n",
        "\n",
        "    from geopandas.io.arrow import _geopandas_to_arrow\n",
        "\n",
        "    def on_batch(batch: list[dict]):\n",
        "        gdf = gpd.GeoDataFrame(batch, crs=\"EPSG:4326\", geometry=\"geometry\")\n",
        "        for ft, sub in gdf.groupby(\"feature_type\"):\n",
        "            part_counter[ft] = part_counter.get(ft, 0) + 1\n",
        "            out_path = f\"s3://{s3_prefix}/feature_type={ft}/part-{part_counter[ft]:05d}.parquet\"\n",
        "            sub = _coerce_string_cols(sub.copy())\n",
        "            try:\n",
        "                tbl = _geopandas_to_arrow(sub, index=False, geometry_encoding=\"WKB\")\n",
        "                tbl = _table_no_dictionary(tbl)\n",
        "                pq.write_table(\n",
        "                    tbl,\n",
        "                    out_path,\n",
        "                    compression=PARQUET_COMPRESSION,\n",
        "                    filesystem=s3_fs,\n",
        "                    use_dictionary=False,\n",
        "                    row_group_size=len(tbl),\n",
        "                )\n",
        "            except Exception as e:\n",
        "                log.error(\"Write failed %s: %s\", out_path, e)\n",
        "                raise RuntimeError(f\"Failed to write {out_path}: {e}\") from e\n",
        "            log.info(\"  Wrote %s: %d rows\", out_path, len(sub))\n",
        "\n",
        "    handler = EnergyFeaturesHandler(wkb_factory, country, snapshot_date, on_batch)\n",
        "    log.info(\"Applying handler to %s (with locations + areas)…\", pbf_path)\n",
        "    handler.apply_file(pbf_path, locations=True)\n",
        "    handler._flush()\n",
        "    return handler._counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_pbf_into_bboxes(\n",
        "    pbf_path: str,\n",
        "    bbox: tuple[float, float, float, float],\n",
        "    n_cols: int,\n",
        "    n_rows: int,\n",
        "    out_dir: str | Path,\n",
        ") -> list[tuple[int, str]]:\n",
        "    \"\"\"\n",
        "    Split PBF into n_cols x n_rows bbox tiles using osmium extract.\n",
        "    Saves to out_dir (e.g. data/osm_bboxes). Run once.\n",
        "    Requires: osmium-tool (brew install osmium-tool)\n",
        "    \"\"\"\n",
        "    minlon, minlat, maxlon, maxlat = bbox\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    lon_step = (maxlon - minlon) / n_cols\n",
        "    lat_step = (maxlat - minlat) / n_rows\n",
        "    results: list[tuple[int, str]] = []\n",
        "\n",
        "    for row in range(n_rows):\n",
        "        for col in range(n_cols):\n",
        "            idx = row * n_cols + col\n",
        "            left = minlon + col * lon_step\n",
        "            bottom = minlat + row * lat_step\n",
        "            right = minlon + (col + 1) * lon_step\n",
        "            top = minlat + (row + 1) * lat_step\n",
        "            bbox_str = f\"{left},{bottom},{right},{top}\"\n",
        "            out_path = out_dir / f\"bbox_{idx:02d}.osm.pbf\"\n",
        "\n",
        "            log.info(\"Extracting bbox %d/%d: %s\", idx + 1, n_cols * n_rows, bbox_str)\n",
        "            r = subprocess.run(\n",
        "                [\"osmium\", \"extract\", \"-b\", bbox_str, str(pbf_path), \"-o\", str(out_path), \"-f\", \"pbf\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "            )\n",
        "            if r.returncode != 0:\n",
        "                raise RuntimeError(f\"osmium extract failed: {r.stderr}\")\n",
        "            results.append((idx, str(out_path)))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Split PBF into bboxes (run once)\n",
        "\n",
        "**Requires:** `osmium-tool` (e.g. `brew install osmium-tool` or `apt install osmium-tool`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22:06:02 [INFO] Removed 11 existing bbox files (FORCE_RESPLIT=True)\n",
            "22:06:02 [INFO] Extracting bbox 1/20: -9.5,36.0,-6.0,37.5\n",
            "22:06:10 [INFO] Extracting bbox 2/20: -6.0,36.0,-2.5,37.5\n",
            "22:06:18 [INFO] Extracting bbox 3/20: -2.5,36.0,1.0,37.5\n",
            "22:06:25 [INFO] Extracting bbox 4/20: 1.0,36.0,4.5,37.5\n",
            "22:06:29 [INFO] Extracting bbox 5/20: -9.5,37.5,-6.0,39.0\n",
            "22:06:35 [INFO] Extracting bbox 6/20: -6.0,37.5,-2.5,39.0\n",
            "22:06:41 [INFO] Extracting bbox 7/20: -2.5,37.5,1.0,39.0\n",
            "22:06:49 [INFO] Extracting bbox 8/20: 1.0,37.5,4.5,39.0\n",
            "22:06:54 [INFO] Extracting bbox 9/20: -9.5,39.0,-6.0,40.5\n",
            "22:06:59 [INFO] Extracting bbox 10/20: -6.0,39.0,-2.5,40.5\n",
            "22:07:06 [INFO] Extracting bbox 11/20: -2.5,39.0,1.0,40.5\n",
            "22:07:13 [INFO] Extracting bbox 12/20: 1.0,39.0,4.5,40.5\n",
            "22:07:19 [INFO] Extracting bbox 13/20: -9.5,40.5,-6.0,42.0\n",
            "22:07:25 [INFO] Extracting bbox 14/20: -6.0,40.5,-2.5,42.0\n",
            "22:07:32 [INFO] Extracting bbox 15/20: -2.5,40.5,1.0,42.0\n",
            "22:07:40 [INFO] Extracting bbox 16/20: 1.0,40.5,4.5,42.0\n",
            "22:07:48 [INFO] Extracting bbox 17/20: -9.5,42.0,-6.0,43.5\n",
            "22:07:56 [INFO] Extracting bbox 18/20: -6.0,42.0,-2.5,43.5\n",
            "22:08:05 [INFO] Extracting bbox 19/20: -2.5,42.0,1.0,43.5\n",
            "22:08:12 [INFO] Extracting bbox 20/20: 1.0,42.0,4.5,43.5\n",
            "22:08:19 [INFO] Split into 20 tiles in /Users/jakubizewski/Desktop/repos/ie-microsoft-capstone/data/osm_bboxes\n"
          ]
        }
      ],
      "source": [
        "# Split PBF into 20 bbox tiles. Output: BBOX_DIR/bbox_00.osm.pbf, ...\n",
        "# Skips if BBOX_DIR has bbox_*.osm.pbf files, unless FORCE_RESPLIT=True.\n",
        "FORCE_RESPLIT = True  # Set True to delete existing bboxes and re-split (e.g. after schema change)\n",
        "bbox_dir = Path(BBOX_DIR)\n",
        "existing = list(bbox_dir.glob(\"bbox_*.osm.pbf\"))\n",
        "if existing and not FORCE_RESPLIT:\n",
        "    log.info(\"Skipping split: %d bbox files already in %s\", len(existing), bbox_dir)\n",
        "else:\n",
        "    if existing and FORCE_RESPLIT:\n",
        "        for f in existing:\n",
        "            f.unlink()\n",
        "        log.info(\"Removed %d existing bbox files (FORCE_RESPLIT=True)\", len(existing))\n",
        "    tiles = split_pbf_into_bboxes(PBF_PATH, SPAIN_BBOX, N_BBOX_COLS, N_BBOX_ROWS, out_dir=bbox_dir)\n",
        "    log.info(\"Split into %d tiles in %s\", len(tiles), bbox_dir.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Process bboxes → write to S3 (resumable)\n",
        "\n",
        "Reads from `BBOX_DIR`. On resume: loads part_counter from S3, processes only remaining bbox files, deletes each after success.\n",
        "\n",
        "**Why multiple part files per feature_type?** Each batch (100k rows) is grouped by feature_type; each group gets one file. So `part-00001`, `part-00002`, … = one file per batch that had that feature type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21:02:32 [INFO] Found credentials in shared credentials file: ~/.aws/credentials\n",
            "21:02:35 [INFO] Limiting to 3 bbox(es) (MAX_BBOX_PROCESSING=3)\n",
            "21:02:35 [INFO] Processing bbox 1/3: bbox_09.osm.pbf\n",
            "21:02:35 [INFO] Applying handler to ../data/osm_bboxes/bbox_09.osm.pbf (with locations + areas)…\n",
            "21:22:52 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=AIRPORTS/part-00045.parquet: 21 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=AMENITIES_POI/part-00046.parquet: 2437 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=BARRIERS/part-00046.parquet: 6893 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=BUILDINGS/part-00046.parquet: 204 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=FORESTRY_MANAGED/part-00044.parquet: 2 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=FUEL_STATIONS/part-00046.parquet: 675 rows\n",
            "21:22:53 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=INDUSTRIAL_AREAS/part-00046.parquet: 47 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=NATURAL_HABITATS/part-00046.parquet: 33 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=PARKS_GREEN_URBAN/part-00046.parquet: 129 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=PIPELINES/part-00046.parquet: 88 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=PORTS_TERMINALS/part-00044.parquet: 2 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_PLANTS/part-00046.parquet: 252 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_SUBSTATIONS/part-00046.parquet: 10 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=PROTECTED_AREAS/part-00043.parquet: 1 rows\n",
            "21:22:54 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=RESTRICTED_AREAS/part-00045.parquet: 133 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=ROADS/part-00046.parquet: 86377 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=STORAGE_TANKS/part-00046.parquet: 8 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=WASTE_POLLUTION/part-00046.parquet: 3 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=WATERBODIES/part-00046.parquet: 4 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=WATERWAYS/part-00046.parquet: 62 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=WATER_BARRIERS/part-00045.parquet: 37 rows\n",
            "21:22:56 [INFO]   Wrote s3://ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=WATER_INFRA_POI/part-00039.parquet: 2582 rows\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mProcessing bbox \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(to_process), bbox_path.name)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     counts = \u001b[43mprocess_pbf_and_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbbox_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOUNTRY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSNAPSHOT_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpart_counter\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ft, n \u001b[38;5;129;01min\u001b[39;00m counts.items():\n\u001b[32m     25\u001b[39m         total_counts[ft] = total_counts.get(ft, \u001b[32m0\u001b[39m) + n\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mprocess_pbf_and_write\u001b[39m\u001b[34m(pbf_path, country, snapshot_date, s3_fs, part_counter)\u001b[39m\n\u001b[32m     67\u001b[39m handler = EnergyFeaturesHandler(wkb_factory, country, snapshot_date, on_batch)\n\u001b[32m     68\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mApplying handler to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (with locations + areas)…\u001b[39m\u001b[33m\"\u001b[39m, pbf_path)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m handler._flush()\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m handler._counts\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/osmium/simple_handler.py:61\u001b[39m, in \u001b[36mSimpleHandler.apply_file\u001b[39m\u001b[34m(self, filename, locations, idx, filters)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mos.PathLike[str]\u001b[39m\u001b[33m'\u001b[39m, File],\n\u001b[32m     51\u001b[39m                locations: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, idx: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mflex_mem\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     52\u001b[39m                filters: List[\u001b[33m'\u001b[39m\u001b[33mHandlerLike\u001b[39m\u001b[33m'\u001b[39m] = []) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     53\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Apply the handler to the given file. If locations is true, then\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m        a location handler will be applied before, which saves the node\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m        positions. In that case, the type of this position index can be\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m        be executed.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/osmium/simple_handler.py:96\u001b[39m, in \u001b[36mSimpleHandler._apply_object\u001b[39m\u001b[34m(self, obj, locations, idx, filters)\u001b[39m\n\u001b[32m     93\u001b[39m     handlers = [*filters, \u001b[38;5;28mself\u001b[39m]\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Reader(obj, entities, thread_pool=thread_pool) \u001b[38;5;28;01mas\u001b[39;00m rd:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mEnergyFeaturesHandler.node\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     60\u001b[39m tags = _tags_dict(n)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m ft, promoted = \u001b[43mmatch_feature_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ft \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mmatch_feature_type\u001b[39m\u001b[34m(tags)\u001b[39m\n\u001b[32m     32\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m tags[k]\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatch_feature_type\u001b[39m(tags: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    Returns (feature_type, promoted_cols).\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    First matching feature type wins. promoted_cols holds typed columns for that type.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# A) PIPELINES\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "MAX_BBOX_PROCESSING = 3  # None = no limit; set to 1 for dry run\n",
        "\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "bbox_dir = Path(BBOX_DIR)\n",
        "\n",
        "# List bbox files (sorted). Empty = all done or split not run yet.\n",
        "bbox_files = sorted(bbox_dir.glob(\"bbox_*.osm.pbf\"))\n",
        "if not bbox_files:\n",
        "    log.info(\"No bbox files in %s. Run Step 1 (split) first, or all done.\", bbox_dir)\n",
        "else:\n",
        "    # Load part_counter from S3 (for resume – continue numbering from existing parts)\n",
        "    part_counter = get_part_counter_from_s3(fs, COUNTRY, SNAPSHOT_DATE)\n",
        "    total_counts: dict[str, int] = {ft: 0 for ft in FEATURE_TYPES}\n",
        "    to_process = bbox_files[:MAX_BBOX_PROCESSING] if MAX_BBOX_PROCESSING is not None else bbox_files\n",
        "    if len(to_process) < len(bbox_files):\n",
        "        log.info(\"Limiting to %d bbox(es) (MAX_BBOX_PROCESSING=%s)\", len(to_process), MAX_BBOX_PROCESSING)\n",
        "\n",
        "    for i, bbox_path in enumerate(to_process):\n",
        "        log.info(\"Processing bbox %d/%d: %s\", i + 1, len(to_process), bbox_path.name)\n",
        "        try:\n",
        "            counts = process_pbf_and_write(\n",
        "                str(bbox_path), COUNTRY, SNAPSHOT_DATE, fs, part_counter=part_counter\n",
        "            )\n",
        "            for ft, n in counts.items():\n",
        "                total_counts[ft] = total_counts.get(ft, 0) + n\n",
        "            bbox_path.unlink()  # delete after success → resume skips it\n",
        "        except Exception as e:\n",
        "            log.error(\"Failed on %s: %s. Re-run this cell to resume.\", bbox_path.name, e)\n",
        "            raise\n",
        "\n",
        "    total = sum(total_counts.values())\n",
        "    log.info(\"Extraction complete: %d total features\", total)\n",
        "    for ft, n in sorted(total_counts.items(), key=lambda x: -x[1]):\n",
        "        if n > 0:\n",
        "            log.info(\"  %s: %d\", ft, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QA checks & sanity metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_qa_checks(country: str, snapshot_date: str, s3_fs: s3fs.S3FileSystem) -> dict:\n",
        "    base = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/snapshot_date={snapshot_date}\"\n",
        "    results = {\"feature_type\": [], \"n_rows\": [], \"n_files\": [], \"geometry_types\": [], \"centroid_valid_pct\": []}\n",
        "\n",
        "    for ft in FEATURE_TYPES:\n",
        "        prefix = f\"{base}/feature_type={ft}/\"\n",
        "        files = s3_fs.glob(f\"{prefix}*.parquet\")\n",
        "        if not files:\n",
        "            continue\n",
        "        dfs = []\n",
        "        for f in files:\n",
        "            with s3_fs.open(f, \"rb\") as fh:\n",
        "                dfs.append(gpd.read_parquet(fh))\n",
        "        gdf = pd.concat(dfs, ignore_index=True)\n",
        "        n = len(gdf)\n",
        "        geom_types = gdf.geometry.geom_type.value_counts().to_dict() if hasattr(gdf, \"geometry\") else {}\n",
        "        centroid_ok = gdf[\"centroid_lon\"].notna().sum() / n * 100 if n else 0\n",
        "        results[\"feature_type\"].append(ft)\n",
        "        results[\"n_rows\"].append(n)\n",
        "        results[\"n_files\"].append(len(files))\n",
        "        results[\"geometry_types\"].append(geom_types)\n",
        "        results[\"centroid_valid_pct\"].append(round(centroid_ok, 2))\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_type</th>\n",
              "      <th>n_rows</th>\n",
              "      <th>n_files</th>\n",
              "      <th>geometry_types</th>\n",
              "      <th>centroid_valid_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PIPELINES</td>\n",
              "      <td>2040</td>\n",
              "      <td>27</td>\n",
              "      <td>{'LineString': 1369, 'Point': 498, 'MultiPolyg...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>POWER_LINES</td>\n",
              "      <td>9044</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 9041, 'MultiPolygon': 3}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POWER_SUBSTATIONS</td>\n",
              "      <td>2605</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 2598, 'Point': 7}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POWER_PLANTS</td>\n",
              "      <td>30449</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 27602, 'Point': 2847}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>INDUSTRIAL_AREAS</td>\n",
              "      <td>4280</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 4148, 'Point': 126, 'LineStri...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>STORAGE_TANKS</td>\n",
              "      <td>4145</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 3965, 'Point': 180}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>FUEL_STATIONS</td>\n",
              "      <td>2634</td>\n",
              "      <td>27</td>\n",
              "      <td>{'Point': 1359, 'MultiPolygon': 1275}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PORTS_TERMINALS</td>\n",
              "      <td>1282</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 1043, 'MultiPolygon': 141, 'Poi...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AIRPORTS</td>\n",
              "      <td>833</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 704, 'MultiPolygon': 89, 'Point...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ROADS</td>\n",
              "      <td>701721</td>\n",
              "      <td>27</td>\n",
              "      <td>{'LineString': 532063, 'Point': 153745, 'Multi...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RAIL</td>\n",
              "      <td>4981</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 4981}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ADMIN_BOUNDARIES</td>\n",
              "      <td>5920</td>\n",
              "      <td>25</td>\n",
              "      <td>{'LineString': 4350, 'MultiPolygon': 1569, 'Po...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PROTECTED_AREAS</td>\n",
              "      <td>284</td>\n",
              "      <td>25</td>\n",
              "      <td>{'MultiPolygon': 275, 'Point': 7, 'LineString'...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WATERWAYS</td>\n",
              "      <td>51618</td>\n",
              "      <td>27</td>\n",
              "      <td>{'LineString': 51263, 'Point': 302, 'MultiPoly...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>WATERBODIES</td>\n",
              "      <td>22323</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 22268, 'Point': 47, 'LineStri...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>WETLANDS</td>\n",
              "      <td>1441</td>\n",
              "      <td>26</td>\n",
              "      <td>{'MultiPolygon': 1427, 'Point': 14}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>COASTLINE</td>\n",
              "      <td>422</td>\n",
              "      <td>18</td>\n",
              "      <td>{'LineString': 314, 'MultiPolygon': 108}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>WATER_BARRIERS</td>\n",
              "      <td>2101</td>\n",
              "      <td>27</td>\n",
              "      <td>{'LineString': 1730, 'MultiPolygon': 258, 'Poi...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>WATER_INFRA_POI</td>\n",
              "      <td>4653</td>\n",
              "      <td>25</td>\n",
              "      <td>{'Point': 4503, 'MultiPolygon': 150}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>RESTRICTED_AREAS</td>\n",
              "      <td>956</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 643, 'Point': 216, 'LineStrin...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>BUILDINGS</td>\n",
              "      <td>819960</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 818795, 'Point': 1162, 'LineS...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>AMENITIES_POI</td>\n",
              "      <td>26285</td>\n",
              "      <td>27</td>\n",
              "      <td>{'Point': 21566, 'MultiPolygon': 4719}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>WASTE_POLLUTION</td>\n",
              "      <td>466</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 449, 'Point': 17}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LANDUSE_AGRICULTURE</td>\n",
              "      <td>79763</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 79749, 'Point': 11, 'LineStri...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>FORESTRY_MANAGED</td>\n",
              "      <td>6347</td>\n",
              "      <td>26</td>\n",
              "      <td>{'MultiPolygon': 6345, 'LineString': 1, 'Point...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NATURAL_HABITATS</td>\n",
              "      <td>33149</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 30131, 'LineString': 2926, 'P...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>BARRIERS</td>\n",
              "      <td>227728</td>\n",
              "      <td>27</td>\n",
              "      <td>{'LineString': 154174, 'Point': 62303, 'MultiP...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LINEAR_DISTURBANCE</td>\n",
              "      <td>2103</td>\n",
              "      <td>25</td>\n",
              "      <td>{'LineString': 2008, 'MultiPolygon': 95}</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>PARKS_GREEN_URBAN</td>\n",
              "      <td>60434</td>\n",
              "      <td>27</td>\n",
              "      <td>{'MultiPolygon': 60035, 'Point': 398, 'LineStr...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>TREE_ROWS_HEDGEROWS</td>\n",
              "      <td>25168</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 23044, 'MultiPolygon': 2122, 'P...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>TRAILS_TRACKS</td>\n",
              "      <td>358678</td>\n",
              "      <td>26</td>\n",
              "      <td>{'LineString': 352492, 'MultiPolygon': 6180, '...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           feature_type  n_rows  n_files  \\\n",
              "0             PIPELINES    2040       27   \n",
              "1           POWER_LINES    9044       26   \n",
              "2     POWER_SUBSTATIONS    2605       27   \n",
              "3          POWER_PLANTS   30449       27   \n",
              "4      INDUSTRIAL_AREAS    4280       27   \n",
              "5         STORAGE_TANKS    4145       27   \n",
              "6         FUEL_STATIONS    2634       27   \n",
              "7       PORTS_TERMINALS    1282       26   \n",
              "8              AIRPORTS     833       26   \n",
              "9                 ROADS  701721       27   \n",
              "10                 RAIL    4981       26   \n",
              "11     ADMIN_BOUNDARIES    5920       25   \n",
              "12      PROTECTED_AREAS     284       25   \n",
              "13            WATERWAYS   51618       27   \n",
              "14          WATERBODIES   22323       27   \n",
              "15             WETLANDS    1441       26   \n",
              "16            COASTLINE     422       18   \n",
              "17       WATER_BARRIERS    2101       27   \n",
              "18      WATER_INFRA_POI    4653       25   \n",
              "19     RESTRICTED_AREAS     956       27   \n",
              "20            BUILDINGS  819960       27   \n",
              "21        AMENITIES_POI   26285       27   \n",
              "22      WASTE_POLLUTION     466       27   \n",
              "23  LANDUSE_AGRICULTURE   79763       27   \n",
              "24     FORESTRY_MANAGED    6347       26   \n",
              "25     NATURAL_HABITATS   33149       27   \n",
              "26             BARRIERS  227728       27   \n",
              "27   LINEAR_DISTURBANCE    2103       25   \n",
              "28    PARKS_GREEN_URBAN   60434       27   \n",
              "29  TREE_ROWS_HEDGEROWS   25168       26   \n",
              "30        TRAILS_TRACKS  358678       26   \n",
              "\n",
              "                                       geometry_types  centroid_valid_pct  \n",
              "0   {'LineString': 1369, 'Point': 498, 'MultiPolyg...               100.0  \n",
              "1             {'LineString': 9041, 'MultiPolygon': 3}               100.0  \n",
              "2                  {'MultiPolygon': 2598, 'Point': 7}               100.0  \n",
              "3              {'MultiPolygon': 27602, 'Point': 2847}               100.0  \n",
              "4   {'MultiPolygon': 4148, 'Point': 126, 'LineStri...               100.0  \n",
              "5                {'MultiPolygon': 3965, 'Point': 180}               100.0  \n",
              "6               {'Point': 1359, 'MultiPolygon': 1275}               100.0  \n",
              "7   {'LineString': 1043, 'MultiPolygon': 141, 'Poi...               100.0  \n",
              "8   {'LineString': 704, 'MultiPolygon': 89, 'Point...               100.0  \n",
              "9   {'LineString': 532063, 'Point': 153745, 'Multi...               100.0  \n",
              "10                               {'LineString': 4981}               100.0  \n",
              "11  {'LineString': 4350, 'MultiPolygon': 1569, 'Po...               100.0  \n",
              "12  {'MultiPolygon': 275, 'Point': 7, 'LineString'...               100.0  \n",
              "13  {'LineString': 51263, 'Point': 302, 'MultiPoly...               100.0  \n",
              "14  {'MultiPolygon': 22268, 'Point': 47, 'LineStri...               100.0  \n",
              "15                {'MultiPolygon': 1427, 'Point': 14}               100.0  \n",
              "16           {'LineString': 314, 'MultiPolygon': 108}               100.0  \n",
              "17  {'LineString': 1730, 'MultiPolygon': 258, 'Poi...               100.0  \n",
              "18               {'Point': 4503, 'MultiPolygon': 150}               100.0  \n",
              "19  {'MultiPolygon': 643, 'Point': 216, 'LineStrin...               100.0  \n",
              "20  {'MultiPolygon': 818795, 'Point': 1162, 'LineS...               100.0  \n",
              "21             {'Point': 21566, 'MultiPolygon': 4719}               100.0  \n",
              "22                 {'MultiPolygon': 449, 'Point': 17}               100.0  \n",
              "23  {'MultiPolygon': 79749, 'Point': 11, 'LineStri...               100.0  \n",
              "24  {'MultiPolygon': 6345, 'LineString': 1, 'Point...               100.0  \n",
              "25  {'MultiPolygon': 30131, 'LineString': 2926, 'P...               100.0  \n",
              "26  {'LineString': 154174, 'Point': 62303, 'MultiP...               100.0  \n",
              "27           {'LineString': 2008, 'MultiPolygon': 95}               100.0  \n",
              "28  {'MultiPolygon': 60035, 'Point': 398, 'LineStr...               100.0  \n",
              "29  {'LineString': 23044, 'MultiPolygon': 2122, 'P...               100.0  \n",
              "30  {'LineString': 352492, 'MultiPolygon': 6180, '...               100.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "qa = run_qa_checks(COUNTRY, SNAPSHOT_DATE, fs)\n",
        "display(qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema reference\n",
        "\n",
        "| Column | Type | Description |\n",
        "|--------|------|-------------|\n",
        "| osm_id | string | OSM object ID (e.g. n123, w456, r789) |\n",
        "| osm_type | string | node, way, relation |\n",
        "| feature_type | string | PIPELINES, POWER_LINES, … |\n",
        "| geometry | geometry | Point/LineString/Polygon (WGS84) |\n",
        "| centroid_lon | double | Longitude of centroid/representative point |\n",
        "| centroid_lat | double | Latitude of centroid/representative point |\n",
        "| h3_6 | string | H3 cell ID at res 6 (~36 km²), separate column |\n",
        "| h3_7 | string | H3 cell ID at res 7 (~5 km²), separate column |\n",
        "| h3_8 | string | H3 cell ID at res 8 (~0.7 km²), separate column |\n",
        "| h3_9 | string | H3 cell ID at res 9 (~0.1 km²), separate column |\n",
        "| name | string | OSM name tag |\n",
        "| tags_json | string | Pruned tags as JSON |\n",
        "| snapshot_date | string | YYYY-MM-DD |\n",
        "| country | string | ES |\n",
        "| + promoted columns | string | Per feature type (pipeline, voltage, highway, …) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema verification (read-back test for Gold)\n",
        "\n",
        "Checks if Parquet files can be read and if column types are consistent across partitions. Use to debug Athena `HIVE_BAD_DATA` errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Low-level Parquet read (row-group by row-group)\n",
        "\n",
        "When `pq.read_table` fails with \"Unable to merge\" (string vs dictionary), the data is still in the file – PyArrow just can't merge row groups. This cell reads each row group separately, decodes dictionary columns, and concatenates. Proves the data is readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read OK: 72 rows from part-00001.parquet\n",
            "       osm_id country plant_source generator_source\n",
            "0  w399679169      ES        solar             None\n",
            "1  w399679170      ES        solar             None\n",
            "2  w399679171      ES        solar             None\n",
            "3  w399679172      ES        solar             None\n",
            "4  w405183555      ES        solar             None\n",
            "\n",
            "generator_source value_counts:\n",
            "generator_source\n",
            "solar     35\n",
            "gas        3\n",
            "diesel     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "country: ['ES']\n",
            "\n",
            "Columns: ['osm_id', 'osm_type', 'feature_type', 'geometry', 'centroid_lon', 'centroid_lat', 'h3_6', 'h3_7', 'h3_8', 'h3_9', 'name', 'tags_json', 'snapshot_date', 'country', 'pipeline', 'substance', 'location', 'operator', 'ref', 'power', 'voltage', 'cables', 'substation', 'plant_source', 'generator_source', 'capacity', 'landuse', 'industrial', 'man_made', 'content', 'amenity', 'brand', 'opening_hours', 'harbour', 'aeroway', 'iata', 'icao', 'highway', 'surface', 'bridge', 'tunnel', 'oneway', 'maxspeed', 'access', 'railway', 'electrified', 'usage', 'admin_level', 'boundary', 'protect_class', 'designation', 'waterway', 'water', 'wetland', 'intermittent', 'width', 'reservoir', 'military', 'building', 'building_levels', 'crop', 'forest', 'natural', 'barrier', 'sac_scale', 'leisure']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "h3_6\n",
              "86391c467ffffff    3085\n",
              "863903db7ffffff    1669\n",
              "86391ab1fffffff    1618\n",
              "86390e647ffffff    1593\n",
              "86391c65fffffff    1561\n",
              "                   ... \n",
              "86391c027ffffff       1\n",
              "863919127ffffff       1\n",
              "8639180f7ffffff       1\n",
              "8639181a7ffffff       1\n",
              "86390aa8fffffff       1\n",
              "Name: count, Length: 721, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read Parquet row-group by row-group (bypasses merge error), decode dict cols → use for Gold\n",
        "def read_parquet_row_groups(path: str, s3_fs) -> pd.DataFrame:\n",
        "    \"\"\"Read single Parquet file; decode dictionary cols to avoid merge error. Full data, no Nones.\"\"\"\n",
        "    pf = pq.ParquetFile(s3_fs.open(path, \"rb\"))\n",
        "    tables = []\n",
        "    for i in range(pf.num_row_groups):\n",
        "        rg = pf.read_row_group(i)\n",
        "        for j in range(rg.num_columns):\n",
        "            if pa.types.is_dictionary(rg.schema.field(j).type):\n",
        "                rg = rg.set_column(j, rg.schema.field(j).name, rg.column(j).dictionary_decode())\n",
        "        tables.append(rg)\n",
        "    tbl = pa.concat_tables(tables)\n",
        "    return tbl.to_pandas()\n",
        "\n",
        "def read_silver_parquet_dir(prefix: str, s3_fs, feature_type: str | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Read all Parquet files under prefix (optionally filtered by feature_type). Use for Gold.\"\"\"\n",
        "    base = f\"{prefix}/feature_type=\"\n",
        "    fts = [feature_type] if feature_type else FEATURE_TYPES\n",
        "    dfs = []\n",
        "    for ft in fts:\n",
        "        p = f\"{base}{ft}/\"\n",
        "        files = sorted(s3_fs.glob(f\"{p}*.parquet\"))\n",
        "        for f in files:\n",
        "            dfs.append(read_parquet_row_groups(f, s3_fs))\n",
        "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "\n",
        "# Single file\n",
        "path = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={COUNTRY}/snapshot_date={SNAPSHOT_DATE}/feature_type=POWER_PLANTS/part-00002.parquet\"\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "df = read_parquet_row_groups(path, fs)\n",
        "print(f\"Read OK: {len(df)} rows from part-00001.parquet\")\n",
        "print(df[[\"osm_id\", \"country\", \"plant_source\", \"generator_source\"]].head())\n",
        "print(\"\\ngenerator_source value_counts:\")\n",
        "print(df[\"generator_source\"].value_counts().head(10))\n",
        "print(\"\\ncountry:\", df[\"country\"].unique())\n",
        "print(\"\\nColumns:\", list(df.columns))\n",
        "\n",
        "# Example: read all POWER_PLANTS for Gold\n",
        "base = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={COUNTRY}/snapshot_date={SNAPSHOT_DATE}\"\n",
        "all_pp = read_silver_parquet_dir(base, fs, feature_type=\"POWER_PLANTS\")\n",
        "# print(f\"Gold-ready: {len(all_pp)} POWER_PLANTS\")\n",
        "all_pp['h3_6'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  POWER_PLANTS READ FAILED ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_PLANTS/part-00001.parquet: The provided token has expired.\n",
            "  POWER_PLANTS READ FAILED ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_PLANTS/part-00002.parquet: The provided token has expired.\n",
            "  POWER_PLANTS READ FAILED ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_PLANTS/part-00003.parquet: The provided token has expired.\n",
            "  POWER_PLANTS: all reads failed, skipping schema check\n",
            "\n",
            "Read FAILED (e.g. schema mismatch): ie-datalake/silver/osm/features_energy/country=ES/snapshot_date=2026-02-25/feature_type=POWER_PLANTS\n"
          ]
        }
      ],
      "source": [
        "# Schema verification: check if we can read silver Parquet and if column types are consistent\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "def verify_silver_schema(country: str, snapshot_date: str, s3_fs: s3fs.S3FileSystem, feature_type: str | None = None):\n",
        "    \"\"\"\n",
        "    For each feature_type (or a single one), read Parquet schemas from S3.\n",
        "    Reports: schema per file, type mismatches across files, and a test read.\n",
        "    \"\"\"\n",
        "    base = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/snapshot_date={snapshot_date}\"\n",
        "    fts = [feature_type] if feature_type else FEATURE_TYPES\n",
        "\n",
        "    for ft in fts:\n",
        "        prefix = f\"{base}/feature_type={ft}/\"\n",
        "        files = sorted(s3_fs.glob(f\"{prefix}*.parquet\"))\n",
        "        if not files:\n",
        "            print(f\"  {ft}: no files\")\n",
        "            continue\n",
        "\n",
        "        schemas = {}\n",
        "        for path in files:\n",
        "            uri = f\"s3://{path}\"\n",
        "            try:\n",
        "                tbl = pq.read_table(uri, filesystem=s3_fs)\n",
        "                schemas[path] = tbl.schema\n",
        "            except Exception as e:\n",
        "                print(f\"  {ft} READ FAILED {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not schemas:\n",
        "            print(f\"  {ft}: all reads failed, skipping schema check\")\n",
        "            continue\n",
        "\n",
        "        # Compare schemas\n",
        "        ref = list(schemas.values())[0]\n",
        "        ref_str = {f.name: str(f.type) for f in ref}\n",
        "        mismatches = []\n",
        "        for path, schema in schemas.items():\n",
        "            for f in schema:\n",
        "                if ref_str.get(f.name) != str(f.type):\n",
        "                    mismatches.append((path.split(\"/\")[-1], f.name, ref_str.get(f.name), str(f.type)))\n",
        "\n",
        "        if mismatches:\n",
        "            print(f\"  {ft}: SCHEMA MISMATCHES:\")\n",
        "            for fname, col, t1, t2 in mismatches[:10]:\n",
        "                print(f\"    {fname} col={col}: ref={t1} vs {t2}\")\n",
        "            if len(mismatches) > 10:\n",
        "                print(f\"    ... and {len(mismatches)-10} more\")\n",
        "        else:\n",
        "            print(f\"  {ft}: OK ({len(files)} files, {ref.num_fields} cols)\")\n",
        "\n",
        "        # Sample promoted columns (e.g. plant_source) – use first successfully read file\n",
        "        first_ok = next((p for p in files if p in schemas), None)\n",
        "        if first_ok:\n",
        "            promoted = [\"plant_source\", \"generator_source\", \"capacity\", \"building_levels\"]\n",
        "            tbl = pq.read_table(f\"s3://{first_ok}\", filesystem=s3_fs)\n",
        "            sample_types = [f\"{c}={tbl.schema.field(c).type}\" for c in promoted if tbl.schema.get_field_index(c) >= 0]\n",
        "            print(f\"    Sample types: \" + \", \".join(sample_types) if sample_types else \"    (no promoted cols in schema)\")\n",
        "\n",
        "# Run verification (optionally limit to problematic feature_type)\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "verify_silver_schema(COUNTRY, SNAPSHOT_DATE, fs, feature_type=\"POWER_PLANTS\")  # or None for all\n",
        "\n",
        "# Full read test (simulates Gold layer – use gpd.read_parquet for GeoParquet)\n",
        "base = f\"s3://{S3_BUCKET}/{SILVER_PREFIX}/country={COUNTRY}/snapshot_date={SNAPSHOT_DATE}/feature_type=POWER_PLANTS/\"\n",
        "try:\n",
        "    df = gpd.read_parquet(base, storage_options=STORAGE_OPTIONS)\n",
        "    print(f\"\\nRead OK: {len(df)} rows\")\n",
        "    cols = [c for c in [\"plant_source\", \"capacity\", \"generator_source\"] if c in df.columns]\n",
        "    print(df[cols].head(3) if cols else df.iloc[:, :5].head(3))\n",
        "except Exception as e:\n",
        "    print(f\"\\nRead FAILED (e.g. schema mismatch): {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
