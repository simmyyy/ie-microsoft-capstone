{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBIF Bronze → Silver\n",
    "\n",
    "Reads enriched GBIF occurrence data from the **bronze** layer written by `gbif_etl_job.ipynb`,\n",
    "applies coordinate cleaning, adds **H3 spatial index** columns at resolutions 6–9, and writes\n",
    "the result to the **silver** layer.\n",
    "\n",
    "| Layer | S3 path | Description |\n",
    "|-------|---------|-------------|\n",
    "| Bronze | `s3://ie-datalake/bronze/gbif/country=XX/year=YYYY/` | Raw enriched export from GBIF |\n",
    "| Silver | `s3://ie-datalake/silver/gbif/country=XX/year=YYYY/` | Cleaned + H3-indexed |\n",
    "\n",
    "### H3 resolutions\n",
    "\n",
    "| Column | Resolution | Avg cell area | Use |\n",
    "|--------|-----------|--------------|-----|\n",
    "| `h3_9` | 9 | ~0.1 km² | Fine-grained point lookup |\n",
    "| `h3_8` | 8 | ~0.7 km² | Neighbourhood |\n",
    "| `h3_7` | 7 | ~5 km² | Local area |\n",
    "| `h3_6` | 6 | ~36 km² | Regional |\n",
    "\n",
    "### Steps\n",
    "1. `read_input()` – read Parquet from bronze S3 path (pushed-down country/year filters)\n",
    "2. `clean_coordinates()` – drop null / out-of-range / (0,0) coordinates\n",
    "3. `add_h3()` – compute h3_9, derive h3_8/7/6 from parents\n",
    "4. `write_silver()` – write partitioned Parquet to silver layer\n",
    "\n",
    "### Requirements\n",
    "```\n",
    "pip install h3>=4.0.0 pyarrow s3fs pandas numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Countries to process (ISO-2 codes). Process one at a time for large datasets.\n",
    "COUNTRIES: list[str] = [\"ES\", \"PT\"]\n",
    "\n",
    "# Year range (inclusive)\n",
    "YEAR_START: int = 2014\n",
    "YEAR_END: int = 2025\n",
    "\n",
    "# S3 paths\n",
    "S3_BUCKET: str       = \"ie-datalake\"\n",
    "BRONZE_PREFIX: str   = \"bronze/gbif\"\n",
    "SILVER_PREFIX: str   = \"silver/gbif\"\n",
    "AWS_PROFILE: str     = \"486717354268_PowerUserAccess\"\n",
    "\n",
    "# H3 resolutions to add (finest first; parents derived from h3_9)\n",
    "H3_RESOLUTIONS: list[int] = [9, 8, 7, 6]\n",
    "\n",
    "# Drop coordinates exactly at (0, 0) – likely null-island artefacts\n",
    "DROP_NULL_ISLAND: bool = True\n",
    "\n",
    "# Parquet write settings\n",
    "PARQUET_COMPRESSION: str   = \"snappy\"\n",
    "PARQUET_ROW_GROUP_SIZE: int = 250_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:23:32 [INFO] Found credentials in shared credentials file: ~/.aws/credentials\n",
      "15:23:32 [INFO] S3FileSystem ready (profile=486717354268_PowerUserAccess)\n",
      "15:23:32 [INFO] Silver job plan: 26 partition(s) | [('ES', 2026), ('ES', 2025), ('ES', 2024), ('ES', 2023), ('ES', 2022), ('ES', 2021), ('ES', 2020), ('ES', 2019), ('ES', 2018), ('ES', 2017), ('ES', 2016), ('ES', 2015), ('ES', 2014), ('PT', 2026), ('PT', 2025), ('PT', 2024), ('PT', 2023), ('PT', 2022), ('PT', 2021), ('PT', 2020), ('PT', 2019), ('PT', 2018), ('PT', 2017), ('PT', 2016), ('PT', 2015), ('PT', 2014)]\n",
      "15:23:32 [INFO] \n",
      "── ES / 2026 ──────────────────────────────────────\n",
      "15:23:32 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2026\n",
      "15:23:33 [ERROR] ✗ ES/2026 failed: Bronze partition not found: s3://ie-datalake/bronze/gbif/country=ES/year=2026. Run gbif_etl_job.ipynb for country=ES year=2026 first.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/7t/yrpkmv3n0vj6ml2djk_2bdk80000gn/T/ipykernel_31254/2817903520.py\", line 225, in <module>\n",
      "    df = read_input(country, year)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7t/yrpkmv3n0vj6ml2djk_2bdk80000gn/T/ipykernel_31254/2817903520.py\", line 46, in read_input\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Bronze partition not found: s3://ie-datalake/bronze/gbif/country=ES/year=2026. Run gbif_etl_job.ipynb for country=ES year=2026 first.\n",
      "15:23:33 [INFO] \n",
      "── ES / 2025 ──────────────────────────────────────\n",
      "15:23:33 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2025\n",
      "15:23:44 [INFO] Read 1073069 rows, 60 columns\n",
      "15:23:44 [INFO] Coordinate cleaning: 3 dropped (null=0, out-of-range=0, null-island=3) / 1073069 total\n",
      "15:23:45 [INFO] Computing h3_9 from 1073066 coordinates …\n",
      "15:23:46 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:23:47 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:23:47 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:23:48 [INFO] H3 done in 2.9s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:23:48 [INFO] Writing 1073066 rows to s3://ie-datalake/silver/gbif/country=ES/year=2025 …\n",
      "15:23:59 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2025\n",
      "15:23:59 [INFO] ✓ ES/2025 done in 26s (1073066 rows → silver)\n",
      "15:23:59 [INFO] \n",
      "── ES / 2024 ──────────────────────────────────────\n",
      "15:23:59 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2024\n",
      "15:25:10 [INFO] Read 7421317 rows, 60 columns\n",
      "15:25:13 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 7421317 total\n",
      "15:25:14 [INFO] Computing h3_9 from 7421317 coordinates …\n",
      "15:25:23 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:25:27 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:25:31 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:25:35 [INFO] H3 done in 21.5s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:25:36 [INFO] Writing 7421317 rows to s3://ie-datalake/silver/gbif/country=ES/year=2024 …\n",
      "15:27:28 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2024\n",
      "15:27:28 [INFO] ✓ ES/2024 done in 210s (7421317 rows → silver)\n",
      "15:27:28 [INFO] \n",
      "── ES / 2023 ──────────────────────────────────────\n",
      "15:27:28 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2023\n",
      "15:28:36 [INFO] Read 7038346 rows, 60 columns\n",
      "15:28:46 [INFO] Coordinate cleaning: 2 dropped (null=0, out-of-range=0, null-island=2) / 7038346 total\n",
      "15:29:01 [INFO] Computing h3_9 from 7038344 coordinates …\n",
      "15:29:11 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:29:15 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:29:19 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:29:22 [INFO] H3 done in 21.4s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:29:23 [INFO] Writing 7038344 rows to s3://ie-datalake/silver/gbif/country=ES/year=2023 …\n",
      "15:30:33 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2023\n",
      "15:30:33 [INFO] ✓ ES/2023 done in 184s (7038344 rows → silver)\n",
      "15:30:33 [INFO] \n",
      "── ES / 2022 ──────────────────────────────────────\n",
      "15:30:33 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2022\n",
      "15:31:41 [INFO] Read 6792518 rows, 60 columns\n",
      "15:31:46 [INFO] Coordinate cleaning: 12 dropped (null=0, out-of-range=0, null-island=12) / 6792518 total\n",
      "15:32:00 [INFO] Computing h3_9 from 6792506 coordinates …\n",
      "15:32:10 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:32:13 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:32:17 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:32:21 [INFO] H3 done in 20.8s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:32:21 [INFO] Writing 6792506 rows to s3://ie-datalake/silver/gbif/country=ES/year=2022 …\n",
      "15:33:24 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2022\n",
      "15:33:24 [INFO] ✓ ES/2022 done in 171s (6792506 rows → silver)\n",
      "15:33:24 [INFO] \n",
      "── ES / 2021 ──────────────────────────────────────\n",
      "15:33:24 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2021\n",
      "15:34:07 [INFO] Read 6336788 rows, 60 columns\n",
      "15:34:13 [INFO] Coordinate cleaning: 16 dropped (null=0, out-of-range=0, null-island=16) / 6336788 total\n",
      "15:34:24 [INFO] Computing h3_9 from 6336772 coordinates …\n",
      "15:34:32 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:34:35 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:34:39 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:34:42 [INFO] H3 done in 18.4s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:34:43 [INFO] Writing 6336772 rows to s3://ie-datalake/silver/gbif/country=ES/year=2021 …\n",
      "15:35:33 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2021\n",
      "15:35:33 [INFO] ✓ ES/2021 done in 130s (6336772 rows → silver)\n",
      "15:35:33 [INFO] \n",
      "── ES / 2020 ──────────────────────────────────────\n",
      "15:35:33 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2020\n",
      "15:36:29 [INFO] Read 5153935 rows, 60 columns\n",
      "15:36:33 [INFO] Coordinate cleaning: 1 dropped (null=0, out-of-range=0, null-island=1) / 5153935 total\n",
      "15:36:40 [INFO] Computing h3_9 from 5153934 coordinates …\n",
      "15:36:45 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:36:48 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:36:51 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:36:54 [INFO] H3 done in 14.2s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:36:54 [INFO] Writing 5153934 rows to s3://ie-datalake/silver/gbif/country=ES/year=2020 …\n",
      "15:37:44 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2020\n",
      "15:37:44 [INFO] ✓ ES/2020 done in 131s (5153934 rows → silver)\n",
      "15:37:44 [INFO] \n",
      "── ES / 2019 ──────────────────────────────────────\n",
      "15:37:44 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2019\n",
      "15:38:29 [INFO] Read 4511366 rows, 60 columns\n",
      "15:38:32 [INFO] Coordinate cleaning: 1311 dropped (null=0, out-of-range=0, null-island=1311) / 4511366 total\n",
      "15:38:38 [INFO] Computing h3_9 from 4510055 coordinates …\n",
      "15:38:43 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:38:46 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:38:48 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:38:50 [INFO] H3 done in 12.2s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:38:50 [INFO] Writing 4510055 rows to s3://ie-datalake/silver/gbif/country=ES/year=2019 …\n",
      "15:39:28 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2019\n",
      "15:39:28 [INFO] ✓ ES/2019 done in 104s (4510055 rows → silver)\n",
      "15:39:28 [INFO] \n",
      "── ES / 2018 ──────────────────────────────────────\n",
      "15:39:28 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2018\n",
      "15:40:14 [INFO] Read 3885079 rows, 60 columns\n",
      "15:40:17 [INFO] Coordinate cleaning: 2657 dropped (null=0, out-of-range=0, null-island=2657) / 3885079 total\n",
      "15:40:21 [INFO] Computing h3_9 from 3882422 coordinates …\n",
      "15:40:25 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:40:27 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:40:29 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:40:31 [INFO] H3 done in 10.3s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:40:31 [INFO] Writing 3882422 rows to s3://ie-datalake/silver/gbif/country=ES/year=2018 …\n",
      "15:41:04 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2018\n",
      "15:41:04 [INFO] ✓ ES/2018 done in 96s (3882422 rows → silver)\n",
      "15:41:04 [INFO] \n",
      "── ES / 2017 ──────────────────────────────────────\n",
      "15:41:04 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2017\n",
      "15:41:27 [INFO] Read 3011518 rows, 60 columns\n",
      "15:41:29 [INFO] Coordinate cleaning: 2423 dropped (null=0, out-of-range=0, null-island=2423) / 3011518 total\n",
      "15:41:32 [INFO] Computing h3_9 from 3009095 coordinates …\n",
      "15:41:35 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:41:37 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:41:38 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:41:40 [INFO] H3 done in 7.9s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:41:40 [INFO] Writing 3009095 rows to s3://ie-datalake/silver/gbif/country=ES/year=2017 …\n",
      "15:42:02 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2017\n",
      "15:42:02 [INFO] ✓ ES/2017 done in 58s (3009095 rows → silver)\n",
      "15:42:02 [INFO] \n",
      "── ES / 2016 ──────────────────────────────────────\n",
      "15:42:02 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2016\n",
      "15:42:35 [INFO] Read 2060045 rows, 60 columns\n",
      "15:42:36 [INFO] Coordinate cleaning: 4097 dropped (null=0, out-of-range=0, null-island=4097) / 2060045 total\n",
      "15:42:38 [INFO] Computing h3_9 from 2055948 coordinates …\n",
      "15:42:41 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:42:42 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:42:43 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:42:44 [INFO] H3 done in 5.3s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:42:44 [INFO] Writing 2055948 rows to s3://ie-datalake/silver/gbif/country=ES/year=2016 …\n",
      "15:43:00 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2016\n",
      "15:43:00 [INFO] ✓ ES/2016 done in 58s (2055948 rows → silver)\n",
      "15:43:00 [INFO] \n",
      "── ES / 2015 ──────────────────────────────────────\n",
      "15:43:00 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2015\n",
      "15:43:16 [INFO] Read 1825862 rows, 60 columns\n",
      "15:43:16 [INFO] Coordinate cleaning: 3299 dropped (null=0, out-of-range=0, null-island=3299) / 1825862 total\n",
      "15:43:18 [INFO] Computing h3_9 from 1822563 coordinates …\n",
      "15:43:20 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:43:21 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:43:22 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:43:23 [INFO] H3 done in 4.7s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:43:23 [INFO] Writing 1822563 rows to s3://ie-datalake/silver/gbif/country=ES/year=2015 …\n",
      "15:43:38 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2015\n",
      "15:43:38 [INFO] ✓ ES/2015 done in 38s (1822563 rows → silver)\n",
      "15:43:38 [INFO] \n",
      "── ES / 2014 ──────────────────────────────────────\n",
      "15:43:38 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=ES/year=2014\n",
      "15:43:52 [INFO] Read 1659807 rows, 60 columns\n",
      "15:43:53 [INFO] Coordinate cleaning: 2663 dropped (null=0, out-of-range=0, null-island=2663) / 1659807 total\n",
      "15:43:55 [INFO] Computing h3_9 from 1657144 coordinates …\n",
      "15:43:56 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:43:57 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:43:58 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:43:59 [INFO] H3 done in 4.3s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:43:59 [INFO] Writing 1657144 rows to s3://ie-datalake/silver/gbif/country=ES/year=2014 …\n",
      "15:44:11 [INFO] Written: s3://ie-datalake/silver/gbif/country=ES/year=2014\n",
      "15:44:11 [INFO] ✓ ES/2014 done in 34s (1657144 rows → silver)\n",
      "15:44:11 [INFO] \n",
      "── PT / 2026 ──────────────────────────────────────\n",
      "15:44:11 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2026\n",
      "15:44:14 [INFO] Read 22710 rows, 60 columns\n",
      "15:44:14 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 22710 total\n",
      "15:44:14 [INFO] Computing h3_9 from 22710 coordinates …\n",
      "15:44:14 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:44:14 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:44:14 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:44:14 [INFO] H3 done in 0.1s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:44:14 [INFO] Writing 22710 rows to s3://ie-datalake/silver/gbif/country=PT/year=2026 …\n",
      "15:44:18 [INFO] Written: s3://ie-datalake/silver/gbif/country=PT/year=2026\n",
      "15:44:18 [INFO] ✓ PT/2026 done in 7s (22710 rows → silver)\n",
      "15:44:18 [INFO] \n",
      "── PT / 2025 ──────────────────────────────────────\n",
      "15:44:18 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2025\n",
      "15:44:24 [INFO] Read 336391 rows, 60 columns\n",
      "15:44:24 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 336391 total\n",
      "15:44:24 [INFO] Computing h3_9 from 336391 coordinates …\n",
      "15:44:24 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:44:25 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:44:25 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:44:25 [INFO] H3 done in 0.9s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:44:25 [INFO] Writing 336391 rows to s3://ie-datalake/silver/gbif/country=PT/year=2025 …\n",
      "15:44:37 [INFO] Written: s3://ie-datalake/silver/gbif/country=PT/year=2025\n",
      "15:44:37 [INFO] ✓ PT/2025 done in 19s (336391 rows → silver)\n",
      "15:44:37 [INFO] \n",
      "── PT / 2024 ──────────────────────────────────────\n",
      "15:44:37 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2024\n",
      "15:45:10 [INFO] Read 1942667 rows, 60 columns\n",
      "15:45:10 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 1942667 total\n",
      "15:45:11 [INFO] Computing h3_9 from 1942667 coordinates …\n",
      "15:45:13 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:45:14 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:45:15 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:45:16 [INFO] H3 done in 5.2s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:45:16 [INFO] Writing 1942667 rows to s3://ie-datalake/silver/gbif/country=PT/year=2024 …\n",
      "15:46:07 [INFO] Written: s3://ie-datalake/silver/gbif/country=PT/year=2024\n",
      "15:46:07 [INFO] ✓ PT/2024 done in 90s (1942667 rows → silver)\n",
      "15:46:07 [INFO] \n",
      "── PT / 2023 ──────────────────────────────────────\n",
      "15:46:07 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2023\n",
      "15:46:29 [INFO] Read 1911867 rows, 60 columns\n",
      "15:46:30 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 1911867 total\n",
      "15:46:31 [INFO] Computing h3_9 from 1911867 coordinates …\n",
      "15:46:33 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:46:34 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:46:35 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:46:36 [INFO] H3 done in 5.0s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:46:36 [INFO] Writing 1911867 rows to s3://ie-datalake/silver/gbif/country=PT/year=2023 …\n",
      "15:47:26 [INFO] Written: s3://ie-datalake/silver/gbif/country=PT/year=2023\n",
      "15:47:26 [INFO] ✓ PT/2023 done in 79s (1911867 rows → silver)\n",
      "15:47:26 [INFO] \n",
      "── PT / 2022 ──────────────────────────────────────\n",
      "15:47:26 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2022\n",
      "15:47:56 [INFO] Read 1847577 rows, 60 columns\n",
      "15:47:58 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 1847577 total\n",
      "15:47:59 [INFO] Computing h3_9 from 1847577 coordinates …\n",
      "15:48:01 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:48:02 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:48:03 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:48:04 [INFO] H3 done in 4.8s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:48:04 [INFO] Writing 1847577 rows to s3://ie-datalake/silver/gbif/country=PT/year=2022 …\n",
      "15:48:54 [INFO] Written: s3://ie-datalake/silver/gbif/country=PT/year=2022\n",
      "15:48:54 [INFO] ✓ PT/2022 done in 89s (1847577 rows → silver)\n",
      "15:48:54 [INFO] \n",
      "── PT / 2021 ──────────────────────────────────────\n",
      "15:48:54 [INFO] Reading bronze: s3://ie-datalake/bronze/gbif/country=PT/year=2021\n",
      "15:49:12 [INFO] Read 1629388 rows, 60 columns\n",
      "15:49:14 [INFO] Coordinate cleaning: 0 dropped (null=0, out-of-range=0, null-island=0) / 1629388 total\n",
      "15:49:15 [INFO] Computing h3_9 from 1629388 coordinates …\n",
      "15:49:16 [INFO] Deriving h3_8 from h3_9 …\n",
      "15:49:17 [INFO] Deriving h3_7 from h3_8 …\n",
      "15:49:18 [INFO] Deriving h3_6 from h3_7 …\n",
      "15:49:19 [INFO] H3 done in 4.3s. Columns added: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "15:49:19 [INFO] Writing 1629388 rows to s3://ie-datalake/silver/gbif/country=PT/year=2021 …\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# BRONZE → SILVER PIPELINE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import h3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "log = logging.getLogger(\"gbif_silver\")\n",
    "\n",
    "# ── S3 filesystem ─────────────────────────────────────────────────────────────\n",
    "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
    "log.info(\"S3FileSystem ready (profile=%s)\", AWS_PROFILE)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# 1. READ\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def read_input(country: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read one bronze partition (country × year) from S3 into a DataFrame.\n",
    "\n",
    "    Uses pyarrow.dataset for efficient columnar reads – only the partition\n",
    "    path is opened, no full-scan of the bronze prefix.\n",
    "    \"\"\"\n",
    "    s3_path = f\"{S3_BUCKET}/{BRONZE_PREFIX}/country={country}/year={year}\"\n",
    "    log.info(\"Reading bronze: s3://%s\", s3_path)\n",
    "\n",
    "    if not fs.exists(s3_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Bronze partition not found: s3://{s3_path}. \"\n",
    "            f\"Run gbif_etl_job.ipynb for country={country} year={year} first.\"\n",
    "        )\n",
    "\n",
    "    dataset = ds.dataset(s3_path, filesystem=fs, format=\"parquet\")\n",
    "    df = dataset.to_table().to_pandas()\n",
    "    log.info(\"Read %d rows, %d columns\", len(df), len(df.columns))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# 2. CLEAN\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _find_col(df: pd.DataFrame, name: str) -> str | None:\n",
    "    \"\"\"Case-insensitive column lookup, ignoring underscores.\"\"\"\n",
    "    norm = name.lower().replace(\"_\", \"\")\n",
    "    for col in df.columns:\n",
    "        if col.lower().replace(\"_\", \"\") == norm:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows with invalid or missing coordinates:\n",
    "      - lat / lon null or non-numeric\n",
    "      - lat outside [-90, 90]\n",
    "      - lon outside [-180, 180]\n",
    "      - (lat == 0) AND (lon == 0)  →  null-island artefact (if DROP_NULL_ISLAND)\n",
    "\n",
    "    Returns a cleaned copy with numeric lat/lon columns.\n",
    "    \"\"\"\n",
    "    lat_col = _find_col(df, \"decimalLatitude\")\n",
    "    lon_col = _find_col(df, \"decimalLongitude\")\n",
    "\n",
    "    if not lat_col or not lon_col:\n",
    "        raise ValueError(\n",
    "            f\"Latitude/longitude columns not found. Available: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    n_before = len(df)\n",
    "    df = df.copy()\n",
    "\n",
    "    # Coerce to numeric (non-parseable → NaN)\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    df[lon_col] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "\n",
    "    # Null check\n",
    "    mask_null = df[lat_col].isna() | df[lon_col].isna()\n",
    "\n",
    "    # Range check\n",
    "    mask_range = (\n",
    "        (df[lat_col] < -90)  | (df[lat_col] > 90)\n",
    "        | (df[lon_col] < -180) | (df[lon_col] > 180)\n",
    "    )\n",
    "\n",
    "    # Null-island check\n",
    "    mask_null_island = (\n",
    "        (df[lat_col] == 0.0) & (df[lon_col] == 0.0)\n",
    "        if DROP_NULL_ISLAND else pd.Series(False, index=df.index)\n",
    "    )\n",
    "\n",
    "    bad = mask_null | mask_range | mask_null_island\n",
    "    n_dropped = bad.sum()\n",
    "\n",
    "    log.info(\n",
    "        \"Coordinate cleaning: %d dropped (null=%d, out-of-range=%d, null-island=%d) / %d total\",\n",
    "        n_dropped,\n",
    "        mask_null.sum(),\n",
    "        mask_range.sum(),\n",
    "        mask_null_island.sum(),\n",
    "        n_before,\n",
    "    )\n",
    "\n",
    "    df = df[~bad].reset_index(drop=True)\n",
    "\n",
    "    # Normalise column names for downstream steps\n",
    "    if lat_col != \"decimalLatitude\":\n",
    "        df = df.rename(columns={lat_col: \"decimalLatitude\", lon_col: \"decimalLongitude\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# 3. H3\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Vectorized wrappers around h3 4.x API\n",
    "_h3_latlng_to_cell = np.vectorize(h3.latlng_to_cell, otypes=[str])\n",
    "_h3_cell_to_parent = np.vectorize(h3.cell_to_parent,  otypes=[str])\n",
    "\n",
    "\n",
    "def add_h3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add H3 spatial index columns at resolutions defined in H3_RESOLUTIONS.\n",
    "\n",
    "    Strategy:\n",
    "      - Compute h3_{max_res} from lat/lon (most expensive, vectorized).\n",
    "      - Derive coarser resolutions via cell_to_parent (cheap, vectorized).\n",
    "\n",
    "    Requires clean, numeric decimalLatitude / decimalLongitude columns.\n",
    "    \"\"\"\n",
    "    if \"decimalLatitude\" not in df.columns or \"decimalLongitude\" not in df.columns:\n",
    "        raise ValueError(\"clean_coordinates() must be called before add_h3()\")\n",
    "\n",
    "    resolutions = sorted(H3_RESOLUTIONS, reverse=True)  # finest first\n",
    "    finest = resolutions[0]\n",
    "\n",
    "    t0 = time.time()\n",
    "    log.info(\"Computing h3_%d from %d coordinates …\", finest, len(df))\n",
    "\n",
    "    df = df.copy()\n",
    "    lat = df[\"decimalLatitude\"].to_numpy(dtype=float)\n",
    "    lon = df[\"decimalLongitude\"].to_numpy(dtype=float)\n",
    "\n",
    "    col_finest = f\"h3_{finest}\"\n",
    "    df[col_finest] = _h3_latlng_to_cell(lat, lon, finest)\n",
    "\n",
    "    for res in resolutions[1:]:\n",
    "        parent_col = f\"h3_{res}\"\n",
    "        child_col  = f\"h3_{res + 1}\"\n",
    "        log.info(\"Deriving h3_%d from h3_%d …\", res, res + 1)\n",
    "        df[parent_col] = _h3_cell_to_parent(df[child_col].to_numpy(), res)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    cols_added = [f\"h3_{r}\" for r in resolutions]\n",
    "    log.info(\"H3 done in %.1fs. Columns added: %s\", elapsed, cols_added)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# 4. WRITE\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def write_silver(df: pd.DataFrame, country: str, year: int) -> str:\n",
    "    \"\"\"\n",
    "    Write cleaned, H3-enriched DataFrame to the silver S3 partition.\n",
    "\n",
    "    Path: s3://{S3_BUCKET}/{SILVER_PREFIX}/country={country}/year={year}/\n",
    "    Existing files at that prefix are overwritten (delete_matching).\n",
    "    \"\"\"\n",
    "    s3_root = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/year={year}\"\n",
    "    log.info(\"Writing %d rows to s3://%s …\", len(df), s3_root)\n",
    "\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=f\"s3://{s3_root}\",\n",
    "        filesystem=fs,\n",
    "        existing_data_behavior=\"delete_matching\",\n",
    "        row_group_size=PARQUET_ROW_GROUP_SIZE,\n",
    "        compression=PARQUET_COMPRESSION,\n",
    "        write_statistics=True,\n",
    "    )\n",
    "\n",
    "    s3_uri = f\"s3://{s3_root}\"\n",
    "    log.info(\"Written: %s\", s3_uri)\n",
    "    return s3_uri\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# MAIN PIPELINE\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "years = list(range(YEAR_END, YEAR_START - 1, -1))  # newest first\n",
    "job_plan = [(c, y) for c in COUNTRIES for y in years]\n",
    "log.info(\"Silver job plan: %d partition(s) | %s\", len(job_plan), job_plan)\n",
    "\n",
    "completed: list[dict] = []\n",
    "errors:    list[dict] = []\n",
    "\n",
    "for country, year in job_plan:\n",
    "    log.info(\"\\n── %s / %s ──────────────────────────────────────\", country, year)\n",
    "    try:\n",
    "        t_start = time.time()\n",
    "\n",
    "        df = read_input(country, year)\n",
    "        df = clean_coordinates(df)\n",
    "        df = add_h3(df)\n",
    "        s3_uri = write_silver(df, country, year)\n",
    "\n",
    "        elapsed = time.time() - t_start\n",
    "        completed.append({\n",
    "            \"country\":     country,\n",
    "            \"year\":        year,\n",
    "            \"rows_silver\": len(df),\n",
    "            \"s3_uri\":      s3_uri,\n",
    "            \"elapsed_s\":   round(elapsed, 1),\n",
    "        })\n",
    "        log.info(\"✓ %s/%s done in %.0fs (%d rows → silver)\", country, year, elapsed, len(df))\n",
    "\n",
    "    except Exception as exc:\n",
    "        log.error(\"✗ %s/%s failed: %s\", country, year, exc, exc_info=True)\n",
    "        errors.append({\"country\": country, \"year\": year, \"error\": str(exc)})\n",
    "\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"═\" * 60)\n",
    "print(f\"Silver pipeline complete: {len(completed)} succeeded, {len(errors)} failed\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "if completed:\n",
    "    print(\"\\nCompleted:\")\n",
    "    display(pd.DataFrame(completed))\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nFailed:\")\n",
    "    display(pd.DataFrame(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: s3://ie-datalake/silver/gbif/country=ES/year=2024\n",
      "\n",
      "Shape: 7,421,317 rows × 64 columns\n",
      "H3 columns: ['h3_9', 'h3_8', 'h3_7', 'h3_6']\n",
      "Null h3_9:  0 (should be 0)\n",
      "\n",
      "Lat range:  [24.6366, 53.9403]\n",
      "Lon range:  [-21.1756, 6.2507]\n",
      "Null-island (0,0): 0\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>h3_9</th>\n",
       "      <th>h3_8</th>\n",
       "      <th>h3_7</th>\n",
       "      <th>h3_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.5492</td>\n",
       "      <td>-4.2577</td>\n",
       "      <td>89392d9296fffff</td>\n",
       "      <td>88392d9297fffff</td>\n",
       "      <td>87392d929ffffff</td>\n",
       "      <td>86392d92fffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.4995</td>\n",
       "      <td>-2.2130</td>\n",
       "      <td>89184b805cbffff</td>\n",
       "      <td>88184b805dfffff</td>\n",
       "      <td>87184b805ffffff</td>\n",
       "      <td>86184b807ffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.8460</td>\n",
       "      <td>-14.3122</td>\n",
       "      <td>89181136d83ffff</td>\n",
       "      <td>88181136d9fffff</td>\n",
       "      <td>87181136dffffff</td>\n",
       "      <td>86181136fffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0553</td>\n",
       "      <td>-13.3577</td>\n",
       "      <td>8918066f497ffff</td>\n",
       "      <td>8818066f49fffff</td>\n",
       "      <td>8718066f4ffffff</td>\n",
       "      <td>8618066f7ffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.6362</td>\n",
       "      <td>-13.6295</td>\n",
       "      <td>891802cea0fffff</td>\n",
       "      <td>881802cea1fffff</td>\n",
       "      <td>871802ceaffffff</td>\n",
       "      <td>861802cefffffff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decimalLatitude  decimalLongitude             h3_9             h3_8  \\\n",
       "0          43.5492           -4.2577  89392d9296fffff  88392d9297fffff   \n",
       "1          43.4995           -2.2130  89184b805cbffff  88184b805dfffff   \n",
       "2          51.8460          -14.3122  89181136d83ffff  88181136d9fffff   \n",
       "3          53.0553          -13.3577  8918066f497ffff  8818066f49fffff   \n",
       "4          52.6362          -13.6295  891802cea0fffff  881802cea1fffff   \n",
       "\n",
       "              h3_7             h3_6  \n",
       "0  87392d929ffffff  86392d92fffffff  \n",
       "1  87184b805ffffff  86184b807ffffff  \n",
       "2  87181136dffffff  86181136fffffff  \n",
       "3  8718066f4ffffff  8618066f7ffffff  \n",
       "4  871802ceaffffff  861802cefffffff  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# VERIFY – quick sanity check on one silver partition\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import h3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "VERIFY_COUNTRY = COUNTRIES[0]\n",
    "VERIFY_YEAR    = 2024\n",
    "\n",
    "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
    "\n",
    "s3_path = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={VERIFY_COUNTRY}/year={VERIFY_YEAR}\"\n",
    "print(f\"Reading: s3://{s3_path}\")\n",
    "\n",
    "sample = ds.dataset(s3_path, filesystem=fs, format=\"parquet\").to_table().to_pandas()\n",
    "\n",
    "print(f\"\\nShape: {sample.shape[0]:,} rows × {sample.shape[1]} columns\")\n",
    "\n",
    "h3_cols = [c for c in sample.columns if c.startswith(\"h3_\")]\n",
    "print(f\"H3 columns: {h3_cols}\")\n",
    "print(f\"Null h3_9:  {sample['h3_9'].isna().sum()} (should be 0)\")\n",
    "\n",
    "print(f\"\\nLat range:  [{sample['decimalLatitude'].min():.4f}, {sample['decimalLatitude'].max():.4f}]\")\n",
    "print(f\"Lon range:  [{sample['decimalLongitude'].min():.4f}, {sample['decimalLongitude'].max():.4f}]\")\n",
    "print(f\"Null-island (0,0): {((sample['decimalLatitude'] == 0) & (sample['decimalLongitude'] == 0)).sum()}\")\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "display(sample[[\"decimalLatitude\", \"decimalLongitude\"] + h3_cols].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h3_9\n",
       "893919594c7ffff    44971\n",
       "89184b1a0d7ffff    28282\n",
       "8918494f4cbffff    26282\n",
       "89390ca265bffff    25955\n",
       "89395415b47ffff    23415\n",
       "                   ...  \n",
       "8939547b397ffff        1\n",
       "89397319507ffff        1\n",
       "89395656e0fffff        1\n",
       "8939568e897ffff        1\n",
       "8939727a697ffff        1\n",
       "Name: count, Length: 272584, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['h3_6'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
