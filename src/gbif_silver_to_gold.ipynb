{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GBIF Silver → Gold: `gbif_cell_metrics`\n",
        "\n",
        "Reads H3-indexed occurrences from the **silver** layer and aggregates them into\n",
        "a **gold** metrics table partitioned by `country / year / h3_resolution`.\n",
        "\n",
        "| Layer | S3 path |\n",
        "|-------|---------|\n",
        "| Silver in | `s3://ie-datalake/silver/gbif/country=XX/year=YYYY/` |\n",
        "| Gold out  | `s3://ie-datalake/gold/gbif_cell_metrics/country=XX/year=YYYY/h3_resolution=N/` |\n",
        "\n",
        "## Metrics per `(country, year, h3_resolution, h3_index)`\n",
        "\n",
        "### Observation\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `observation_count` | Total occurrence records |\n",
        "| `species_richness_cell` | Distinct species (speciesKey → taxonKey → species string) |\n",
        "| `unique_datasets` | Distinct datasetKey |\n",
        "| `avg_coordinate_uncertainty_m` | Mean coordinateUncertaintyInMeters |\n",
        "| `pct_uncertainty_gt_10km` | Share of records with uncertainty > 10 000 m |\n",
        "\n",
        "### IUCN / Threat\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `n_assessed_species` | Distinct species with any IUCN category |\n",
        "| `n_sp_cr / en / vu / nt / lc / dd / ne` | Distinct species per category (only if present) |\n",
        "| `n_threatened_species` | Distinct CR + EN + VU species |\n",
        "| `threat_score_weighted` | Σ weight(iucn) per **distinct** species; CR=5, EN=4, VU=3, NT=2 |\n",
        "\n",
        "### Diversity\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `shannon_H` | Shannon-Wiener entropy (numerically stable) |\n",
        "| `simpson_1_minus_D` | Simpson diversity index 1 − D |\n",
        "\n",
        "### Data Quality Index (0–1)\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `dqi` | Composite: coord completeness, species-id completeness, uncertainty quality, iucn coverage |\n",
        "\n",
        "## Memory strategy\n",
        "- Only **required columns** are loaded (pyarrow projection pushdown).\n",
        "- One `(country, year)` partition is held in RAM at a time.\n",
        "- Diversity metrics are computed on the **species-count table** (groupby result),\n",
        "  not on the full record-level DataFrame.\n",
        "\n",
        "## Requirements\n",
        "```\n",
        "pip install pyarrow s3fs pandas numpy\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "COUNTRIES: list[str] = [\"ES\"]\n",
        "YEAR_START: int = 2024\n",
        "YEAR_END:   int = 2024\n",
        "\n",
        "# H3 resolutions to aggregate (must already be present in silver)\n",
        "H3_RESOLUTIONS: list[int] = [9, 8, 7, 6]\n",
        "\n",
        "S3_BUCKET:     str = \"ie-datalake\"\n",
        "SILVER_PREFIX: str = \"silver/gbif\"\n",
        "GOLD_PREFIX:   str = \"gold/gbif_cell_metrics\"\n",
        "AWS_PROFILE:   str = \"486717354268_PowerUserAccess\"\n",
        "\n",
        "PARQUET_COMPRESSION:    str = \"snappy\"\n",
        "PARQUET_ROW_GROUP_SIZE: int = 250_000\n",
        "\n",
        "# IUCN categories to pivot (only columns with ≥1 species will be written)\n",
        "IUCN_ALL_CATS: list[str] = [\"CR\", \"EN\", \"VU\", \"NT\", \"LC\", \"DD\", \"NE\"]\n",
        "IUCN_WEIGHTS:  dict[str, int] = {\"CR\": 5, \"EN\": 4, \"VU\": 3, \"NT\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16:36:55 [INFO] Found credentials in shared credentials file: ~/.aws/credentials\n",
            "16:36:55 [INFO] Found credentials in shared credentials file: ~/.aws/credentials\n",
            "16:36:55 [INFO] S3 ready (profile=486717354268_PowerUserAccess, region=eu-west-1, io_threads=16)\n",
            "16:36:55 [INFO] Gold pipeline: 1 (country×year) partitions × 4 resolutions = 4 total writes\n",
            "16:36:55 [INFO] \n",
            "── ES / 2024 ──────────────────────────────────────────────────\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "When getting information for key 'silver/gbif/country=ES/year=2024' in bucket 'ie-datalake': AWS Error UNKNOWN (HTTP status 301) during HeadObject operation: No response body. Looks like the configured region is 'eu-west-1' while the bucket is located in 'eu-west-2'.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 431\u001b[39m\n\u001b[32m    428\u001b[39m t_read = time.time()\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     df = \u001b[43mread_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    433\u001b[39m     log.error(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, exc)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mread_input\u001b[39m\u001b[34m(country, year)\u001b[39m\n\u001b[32m    119\u001b[39m log_path    = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnative_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Existence check via pyarrow native (avoids s3fs recursive listing)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m info = \u001b[43mfs_read\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnative_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info.type == pafs.FileType.NotFound:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    125\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSilver partition not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRun gbif_bronze_to_silver.ipynb first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/pyarrow/_fs.pyx:616\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.get_file_info\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/ie-microsoft-capstone/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mOSError\u001b[39m: When getting information for key 'silver/gbif/country=ES/year=2024' in bucket 'ie-datalake': AWS Error UNKNOWN (HTTP status 301) during HeadObject operation: No response body. Looks like the configured region is 'eu-west-1' while the bucket is located in 'eu-west-2'."
          ]
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# SILVER → GOLD PIPELINE\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import pyarrow.fs as pafs\n",
        "import pyarrow.parquet as pq\n",
        "import s3fs\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    force=True,\n",
        ")\n",
        "log = logging.getLogger(\"gbif_gold\")\n",
        "\n",
        "# s3fs – used only for writes (pq.write_to_dataset)\n",
        "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
        "\n",
        "# PyArrow native S3FileSystem – used for reads.\n",
        "# The C++ S3 client pre-fetches row-groups in parallel and uses connection\n",
        "# pooling, making it 5-10× faster than s3fs for reading Parquet files.\n",
        "_boto_session = boto3.Session(profile_name=AWS_PROFILE)\n",
        "_creds = _boto_session.get_credentials().get_frozen_credentials()\n",
        "# _region = _boto_session.region_name or \"eu-west-2\"\n",
        "_region = \"eu-west-2\"\n",
        "fs_read = pafs.S3FileSystem(\n",
        "    access_key=_creds.access_key,\n",
        "    secret_key=_creds.secret_key,\n",
        "    session_token=_creds.token,\n",
        "    region=_region,\n",
        ")\n",
        "\n",
        "# Maximise I/O and CPU parallelism for Arrow operations\n",
        "pa.set_io_thread_count(min(16, (os.cpu_count() or 4) * 2))\n",
        "pa.set_cpu_count(os.cpu_count() or 4)\n",
        "\n",
        "log.info(\n",
        "    \"S3 ready (profile=%s, region=%s, io_threads=%d)\",\n",
        "    AWS_PROFILE, _region, pa.io_thread_count(),\n",
        ")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# UTILITIES\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def _find_col(df: pd.DataFrame, name: str) -> Optional[str]:\n",
        "    \"\"\"Case-insensitive column lookup, normalising underscores.\"\"\"\n",
        "    norm = name.lower().replace(\"_\", \"\")\n",
        "    for col in df.columns:\n",
        "        if col.lower().replace(\"_\", \"\") == norm:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_species_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    \"\"\"Return the best available species-identifier column.\"\"\"\n",
        "    for candidate in (\"speciesKey\", \"taxonKey\", \"species\", \"scientificName\"):\n",
        "        col = _find_col(df, candidate)\n",
        "        if col:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_iucn_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    \"\"\"Return the IUCN category column (from bronze enrichment or raw GBIF).\"\"\"\n",
        "    for candidate in (\"iucn_cat\", \"iucnRedListCategory\"):\n",
        "        col = _find_col(df, candidate)\n",
        "        if col and df[col].notna().any():\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 1. READ INPUT (with column projection)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Columns we want to read from silver (subset – much smaller than full schema)\n",
        "_CANDIDATE_COLS: list[str] = [\n",
        "    # H3 spatial index\n",
        "    \"h3_9\", \"h3_8\", \"h3_7\", \"h3_6\",\n",
        "    # Species identification\n",
        "    \"speciesKey\", \"taxonKey\", \"species\", \"scientificName\",\n",
        "    # Dataset\n",
        "    \"datasetKey\",\n",
        "    # Coordinate quality\n",
        "    \"coordinateUncertaintyInMeters\",\n",
        "    # IUCN\n",
        "    \"iucn_cat\", \"iucnRedListCategory\",\n",
        "    # Invasive flags (for future DQI extension)\n",
        "    \"is_invasive_any\",\n",
        "    # Partition keys (may already be present as columns)\n",
        "    \"country\", \"year\",\n",
        "]\n",
        "\n",
        "\n",
        "def read_input(country: str, year: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read one silver partition from S3 with column projection.\n",
        "\n",
        "    Uses pyarrow's native C++ S3FileSystem (fs_read) which pre-fetches\n",
        "    row-groups in parallel – significantly faster than s3fs for large files.\n",
        "    Only columns relevant to metric computation are loaded.\n",
        "    \"\"\"\n",
        "    # Native pyarrow paths don't have an \"s3://\" prefix\n",
        "    native_path = f\"{S3_BUCKET}/{SILVER_PREFIX}/country={country}/year={year}\"\n",
        "    log_path    = f\"s3://{native_path}\"\n",
        "\n",
        "    # Existence check via pyarrow native (avoids s3fs recursive listing)\n",
        "    info = fs_read.get_file_info(native_path)\n",
        "    if info.type == pafs.FileType.NotFound:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Silver partition not found: {log_path}. \"\n",
        "            \"Run gbif_bronze_to_silver.ipynb first.\"\n",
        "        )\n",
        "\n",
        "    log.info(\"Reading silver: %s\", log_path)\n",
        "    t0 = time.time()\n",
        "\n",
        "    dataset = ds.dataset(native_path, filesystem=fs_read, format=\"parquet\")\n",
        "\n",
        "    # Intersect desired columns with what actually exists in the schema\n",
        "    available = set(dataset.schema.names)\n",
        "    avail_lower = {c.lower().replace(\"_\", \"\"): c for c in available}\n",
        "    project = []\n",
        "    for want in _CANDIDATE_COLS:\n",
        "        key = want.lower().replace(\"_\", \"\")\n",
        "        if key in avail_lower:\n",
        "            project.append(avail_lower[key])\n",
        "\n",
        "    log.info(\"Projecting %d / %d columns – starting download…\", len(project), len(available))\n",
        "\n",
        "    # use_threads=True lets pyarrow read columns in parallel across row-groups\n",
        "    table = dataset.scanner(columns=project, use_threads=True).to_table()\n",
        "    df = table.to_pandas()\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    log.info(\n",
        "        \"Loaded %d rows, %d columns in %.1fs (%.0f MB RAM)\",\n",
        "        len(df), len(df.columns), elapsed,\n",
        "        df.memory_usage(deep=True).sum() / 1e6,\n",
        "    )\n",
        "\n",
        "    # Ensure partition key columns exist\n",
        "    if \"country\" not in df.columns:\n",
        "        df[\"country\"] = country\n",
        "    if \"year\" not in df.columns:\n",
        "        df[\"year\"] = int(year)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 2. COMPUTE METRICS PER H3 RESOLUTION\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def _diversity_vectorized(sp_counts: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute Shannon-Wiener H and Simpson 1-D **without any Python-level loops**.\n",
        "\n",
        "    sp_counts: Series indexed by (h3_cell, species_id) with observation counts.\n",
        "\n",
        "    Strategy (fully vectorized):\n",
        "      1. total_per_cell = groupby(h3).sum()  → broadcast via transform\n",
        "      2. p_i = count / total  (element-wise)\n",
        "      3. shannon_H = -Σ p_i * log(p_i)  per cell  (groupby.sum on the product)\n",
        "      4. simpson   = 1 - Σ p_i²          per cell  (groupby.sum on p²)\n",
        "\n",
        "    Eliminates the O(n_cells) Python-function-call overhead of groupby.apply().\n",
        "    \"\"\"\n",
        "    h3_level = sp_counts.index.names[0]\n",
        "\n",
        "    # Proportions: broadcast total back to each (cell, species) row\n",
        "    total = sp_counts.groupby(level=h3_level).transform(\"sum\")\n",
        "    p = sp_counts / total                           # p_i for every (cell, species)\n",
        "\n",
        "    # Shannon: -Σ p·log(p)  – clip to avoid log(0), zeros contribute 0\n",
        "    log_p = np.log(p.clip(lower=1e-300))\n",
        "    shannon = -(p * log_p).groupby(level=h3_level).sum().rename(\"shannon_H\")\n",
        "\n",
        "    # Simpson: 1 - Σ p²\n",
        "    simpson = (1.0 - (p ** 2).groupby(level=h3_level).sum()).rename(\"simpson_1_minus_D\")\n",
        "\n",
        "    return pd.concat([shannon, simpson], axis=1).reset_index()\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    df: pd.DataFrame,\n",
        "    country: str,\n",
        "    year: int,\n",
        "    h3_resolution: int,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate all metrics for one (country, year, h3_resolution).\n",
        "\n",
        "    Returns a DataFrame with one row per H3 cell and all metric columns.\n",
        "    \"\"\"\n",
        "    h3_col    = f\"h3_{h3_resolution}\"\n",
        "    sk_col    = _resolve_species_col(df)\n",
        "    ds_col    = _find_col(df, \"datasetKey\")\n",
        "    unc_col   = _find_col(df, \"coordinateUncertaintyInMeters\")\n",
        "    iucn_col  = _resolve_iucn_col(df)\n",
        "\n",
        "    if h3_col not in df.columns:\n",
        "        raise ValueError(f\"Column {h3_col!r} not found. Available: {list(df.columns)}\")\n",
        "\n",
        "    log.info(\n",
        "        \"  res=%d | rows=%d | sk=%s | unc=%s | iucn=%s\",\n",
        "        h3_resolution, len(df), sk_col, unc_col, iucn_col,\n",
        "    )\n",
        "\n",
        "    # ── 2a. Base counts ───────────────────────────────────────────────────────\n",
        "    g = df.groupby(h3_col)\n",
        "    agg = g.size().rename(\"observation_count\").reset_index()\n",
        "\n",
        "    # ── 2b. Species richness ─────────────────────────────────────────────────\n",
        "    if sk_col:\n",
        "        sr = g[sk_col].nunique().rename(\"species_richness_cell\")\n",
        "        agg = agg.merge(sr.reset_index(), on=h3_col, how=\"left\")\n",
        "    else:\n",
        "        agg[\"species_richness_cell\"] = pd.NA\n",
        "\n",
        "    # ── 2c. Unique datasets ───────────────────────────────────────────────────\n",
        "    if ds_col:\n",
        "        ud = g[ds_col].nunique().rename(\"unique_datasets\")\n",
        "        agg = agg.merge(ud.reset_index(), on=h3_col, how=\"left\")\n",
        "    else:\n",
        "        agg[\"unique_datasets\"] = pd.NA\n",
        "\n",
        "    # ── 2d. Coordinate uncertainty ────────────────────────────────────────────\n",
        "    if unc_col:\n",
        "        unc = pd.to_numeric(df[unc_col], errors=\"coerce\")\n",
        "        tmp = df.assign(_unc=unc)\n",
        "        avg_unc = tmp.groupby(h3_col)[\"_unc\"].mean().rename(\"avg_coordinate_uncertainty_m\")\n",
        "        pct_gt  = (\n",
        "            tmp.assign(_gt10k=(unc > 10_000).astype(float))\n",
        "               .groupby(h3_col)[\"_gt10k\"].mean()\n",
        "               .rename(\"pct_uncertainty_gt_10km\")\n",
        "        )\n",
        "        agg = agg.merge(avg_unc.reset_index(), on=h3_col, how=\"left\")\n",
        "        agg = agg.merge(pct_gt.reset_index(),  on=h3_col, how=\"left\")\n",
        "    else:\n",
        "        agg[\"avg_coordinate_uncertainty_m\"] = pd.NA\n",
        "        agg[\"pct_uncertainty_gt_10km\"] = pd.NA\n",
        "\n",
        "    # ── 2e. IUCN / Threat metrics ─────────────────────────────────────────────\n",
        "    if iucn_col and sk_col:\n",
        "        df_iucn = df.loc[\n",
        "            df[iucn_col].notna() & (df[iucn_col].astype(str).str.strip() != \"\")\n",
        "        ].copy()\n",
        "\n",
        "        # n_assessed_species\n",
        "        n_assessed = (\n",
        "            df_iucn.groupby(h3_col)[sk_col].nunique().rename(\"n_assessed_species\")\n",
        "        )\n",
        "        agg = agg.merge(n_assessed.reset_index(), on=h3_col, how=\"left\")\n",
        "\n",
        "        # n_sp_{cat} – only emit column when ≥1 species has that category\n",
        "        present_cats = [\n",
        "            c for c in IUCN_ALL_CATS if (df_iucn[iucn_col] == c).any()\n",
        "        ]\n",
        "        for cat in present_cats:\n",
        "            col_name = f\"n_sp_{cat.lower()}\"\n",
        "            cnt = (\n",
        "                df_iucn[df_iucn[iucn_col] == cat]\n",
        "                .groupby(h3_col)[sk_col].nunique()\n",
        "                .rename(col_name)\n",
        "            )\n",
        "            agg = agg.merge(cnt.reset_index(), on=h3_col, how=\"left\")\n",
        "\n",
        "        # n_threatened_species (CR + EN + VU)\n",
        "        df_thr = df_iucn[df_iucn[iucn_col].isin([\"CR\", \"EN\", \"VU\"])]\n",
        "        n_thr = (\n",
        "            df_thr.groupby(h3_col)[sk_col].nunique().rename(\"n_threatened_species\")\n",
        "        )\n",
        "        agg = agg.merge(n_thr.reset_index(), on=h3_col, how=\"left\")\n",
        "\n",
        "        # threat_score_weighted – over DISTINCT (cell, species)\n",
        "        # Vectorized: map each iucn value to a severity int, take max per\n",
        "        # (cell, species), map back to weight, sum per cell.  No Python apply().\n",
        "        _SEV_MAP = {\"CR\": 5, \"EN\": 4, \"VU\": 3, \"NT\": 2, \"LC\": 1, \"DD\": 0, \"NE\": 0}\n",
        "        _SEV_TO_WEIGHT = {5: IUCN_WEIGHTS.get(\"CR\", 0), 4: IUCN_WEIGHTS.get(\"EN\", 0),\n",
        "                          3: IUCN_WEIGHTS.get(\"VU\", 0), 2: IUCN_WEIGHTS.get(\"NT\", 0),\n",
        "                          1: 0, 0: 0}\n",
        "        sev = df_iucn[iucn_col].map(_SEV_MAP).fillna(-1).astype(np.int8)\n",
        "        # max severity per (cell, species) → one row per distinct species per cell\n",
        "        sp_max_sev = (\n",
        "            df_iucn.assign(_sev=sev)\n",
        "            .groupby([h3_col, sk_col])[\"_sev\"].max()\n",
        "        )\n",
        "        weight = sp_max_sev.map(_SEV_TO_WEIGHT).fillna(0)\n",
        "        threat_score = weight.groupby(level=h3_col).sum().rename(\"threat_score_weighted\")\n",
        "        agg = agg.merge(threat_score.reset_index(), on=h3_col, how=\"left\")\n",
        "\n",
        "    else:\n",
        "        for col in [\"n_assessed_species\", \"n_threatened_species\", \"threat_score_weighted\"]:\n",
        "            agg[col] = pd.NA\n",
        "\n",
        "    # ── 2f. Diversity metrics (fully vectorized, no groupby.apply) ───────────\n",
        "    if sk_col:\n",
        "        # Species counts per (h3_cell, species) – re-used for both diversity\n",
        "        # and species-richness (avoids a second full scan).\n",
        "        sp_counts = df.groupby([h3_col, sk_col]).size().rename(\"_n\")\n",
        "        div = _diversity_vectorized(sp_counts)\n",
        "        agg = agg.merge(div, on=h3_col, how=\"left\")\n",
        "    else:\n",
        "        agg[\"shannon_H\"] = pd.NA\n",
        "        agg[\"simpson_1_minus_D\"] = pd.NA\n",
        "\n",
        "    # ── 2g. Data Quality Index (0–1) ──────────────────────────────────────────\n",
        "    #\n",
        "    # Components (each in [0, 1]):\n",
        "    #   c1 – species-id completeness: pct of records with a valid species ID\n",
        "    #   c2 – uncertainty quality: (1 - pct_uncertainty_gt_10km)  [if available]\n",
        "    #   c3 – iucn coverage: (1 - pct_iucn_missing)             [if iucn column exists]\n",
        "    #\n",
        "    # DQI = mean of available components\n",
        "    # Coordinate completeness is already 1.0 after silver cleaning (all rows have coords).\n",
        "\n",
        "    dqi_parts: list[pd.Series] = []\n",
        "\n",
        "    if sk_col:\n",
        "        c1 = (\n",
        "            df.assign(_has_sp=df[sk_col].notna().astype(float))\n",
        "              .groupby(h3_col)[\"_has_sp\"].mean()\n",
        "              .rename(\"_c1\")\n",
        "        )\n",
        "        dqi_parts.append(c1)\n",
        "\n",
        "    if \"pct_uncertainty_gt_10km\" in agg.columns:\n",
        "        c2 = (1.0 - agg.set_index(h3_col)[\"pct_uncertainty_gt_10km\"].fillna(0)).rename(\"_c2\")\n",
        "        dqi_parts.append(c2)\n",
        "\n",
        "    if iucn_col:\n",
        "        c3 = (\n",
        "            df.assign(_iucn_missing=df[iucn_col].isna().astype(float))\n",
        "              .groupby(h3_col)[\"_iucn_missing\"].mean()\n",
        "              .rsub(1)\n",
        "              .rename(\"_c3\")\n",
        "        )\n",
        "        dqi_parts.append(c3)\n",
        "\n",
        "    if dqi_parts:\n",
        "        dqi_df = pd.concat(dqi_parts, axis=1).reset_index()\n",
        "        dqi_df[\"dqi\"] = dqi_df.iloc[:, 1:].mean(axis=1)\n",
        "        agg = agg.merge(dqi_df[[h3_col, \"dqi\"]], on=h3_col, how=\"left\")\n",
        "    else:\n",
        "        agg[\"dqi\"] = pd.NA\n",
        "\n",
        "    # ── 2h. Partition metadata ────────────────────────────────────────────────\n",
        "    agg.rename(columns={h3_col: \"h3_index\"}, inplace=True)\n",
        "    agg[\"h3_resolution\"] = h3_resolution\n",
        "    agg[\"country\"]       = country\n",
        "    agg[\"year\"]          = int(year)\n",
        "\n",
        "    # Normalise int columns (nunique returns int64, NaN forces float64 after merge)\n",
        "    int_cols = [\n",
        "        \"observation_count\", \"species_richness_cell\", \"unique_datasets\",\n",
        "        \"n_assessed_species\", \"n_threatened_species\",\n",
        "    ] + [f\"n_sp_{c.lower()}\" for c in IUCN_ALL_CATS if f\"n_sp_{c.lower()}\" in agg.columns]\n",
        "    for col in int_cols:\n",
        "        if col in agg.columns:\n",
        "            agg[col] = agg[col].astype(\"Int64\")  # nullable integer\n",
        "\n",
        "    return agg\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 3. WRITE GOLD\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def write_gold(agg: pd.DataFrame, country: str, year: int, h3_resolution: int) -> str:\n",
        "    \"\"\"\n",
        "    Write metrics for one (country, year, h3_resolution) slice to the gold layer.\n",
        "\n",
        "    Path: s3://{S3_BUCKET}/{GOLD_PREFIX}/country={country}/year={year}/h3_resolution={h3_resolution}/\n",
        "    \"\"\"\n",
        "    s3_root = (\n",
        "        f\"{S3_BUCKET}/{GOLD_PREFIX}\"\n",
        "        f\"/country={country}/year={year}/h3_resolution={h3_resolution}\"\n",
        "    )\n",
        "    log.info(\"Writing %d cells to s3://%s …\", len(agg), s3_root)\n",
        "\n",
        "    table = pa.Table.from_pandas(agg, preserve_index=False)\n",
        "    pq.write_to_dataset(\n",
        "        table,\n",
        "        root_path=f\"s3://{s3_root}\",\n",
        "        filesystem=fs,\n",
        "        existing_data_behavior=\"delete_matching\",\n",
        "        row_group_size=PARQUET_ROW_GROUP_SIZE,\n",
        "        compression=PARQUET_COMPRESSION,\n",
        "        write_statistics=True,\n",
        "    )\n",
        "    s3_uri = f\"s3://{s3_root}\"\n",
        "    log.info(\"Written: %s\", s3_uri)\n",
        "    return s3_uri\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# MAIN PIPELINE\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "years = list(range(YEAR_END, YEAR_START - 1, -1))  # newest first\n",
        "partition_plan = [(c, y) for c in COUNTRIES for y in years]\n",
        "\n",
        "log.info(\n",
        "    \"Gold pipeline: %d (country×year) partitions × %d resolutions = %d total writes\",\n",
        "    len(partition_plan), len(H3_RESOLUTIONS), len(partition_plan) * len(H3_RESOLUTIONS),\n",
        ")\n",
        "\n",
        "completed: list[dict] = []\n",
        "errors:    list[dict] = []\n",
        "\n",
        "for country, year in partition_plan:\n",
        "    log.info(\"\\n── %s / %s ──────────────────────────────────────────────────\", country, year)\n",
        "    t_read = time.time()\n",
        "\n",
        "    try:\n",
        "        df = read_input(country, year)\n",
        "    except FileNotFoundError as exc:\n",
        "        log.error(\"%s\", exc)\n",
        "        errors.append({\"country\": country, \"year\": year, \"h3_resolution\": \"all\", \"error\": str(exc)})\n",
        "        continue\n",
        "\n",
        "    log.info(\"Read done in %.1fs\", time.time() - t_read)\n",
        "\n",
        "    for res in H3_RESOLUTIONS:\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            agg     = compute_metrics(df, country, year, res)\n",
        "            s3_uri  = write_gold(agg, country, year, res)\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            completed.append({\n",
        "                \"country\":      country,\n",
        "                \"year\":         year,\n",
        "                \"h3_resolution\": res,\n",
        "                \"n_cells\":      len(agg),\n",
        "                \"s3_uri\":       s3_uri,\n",
        "                \"elapsed_s\":    round(elapsed, 1),\n",
        "            })\n",
        "            log.info(\"✓ res=%d | %d cells | %.1fs\", res, len(agg), elapsed)\n",
        "\n",
        "        except Exception as exc:\n",
        "            log.error(\"✗ %s/%s res=%d: %s\", country, year, res, exc, exc_info=True)\n",
        "            errors.append({\"country\": country, \"year\": year, \"h3_resolution\": res, \"error\": str(exc)})\n",
        "\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────────────────────\n",
        "print()\n",
        "print(\"═\" * 60)\n",
        "print(f\"Gold pipeline complete: {len(completed)} succeeded, {len(errors)} failed\")\n",
        "print(\"═\" * 60)\n",
        "\n",
        "if completed:\n",
        "    print(\"\\nCompleted:\")\n",
        "    display(pd.DataFrame(completed))\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nFailed:\")\n",
        "    display(pd.DataFrame(errors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# VERIFY – read back one slice and inspect the schema + sample rows\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "VERIFY_COUNTRY    = COUNTRIES[0]\n",
        "VERIFY_YEAR       = YEAR_END\n",
        "VERIFY_RESOLUTION = 8  # medium resolution for a readable preview\n",
        "\n",
        "s3_path = (\n",
        "    f\"{S3_BUCKET}/{GOLD_PREFIX}\"\n",
        "    f\"/country={VERIFY_COUNTRY}/year={VERIFY_YEAR}\"\n",
        "    f\"/h3_resolution={VERIFY_RESOLUTION}\"\n",
        ")\n",
        "print(f\"Reading: s3://{s3_path}\")\n",
        "\n",
        "sample = ds.dataset(s3_path, filesystem=fs_read, format=\"parquet\").to_table().to_pandas()\n",
        "\n",
        "print(f\"\\nShape: {sample.shape[0]:,} cells × {sample.shape[1]} columns\")\n",
        "print(f\"Columns: {list(sample.columns)}\")\n",
        "\n",
        "print(\"\\nTop 10 cells by species richness:\")\n",
        "display(\n",
        "    sample.sort_values(\"species_richness_cell\", ascending=False)\n",
        "          [[\"h3_index\", \"observation_count\", \"species_richness_cell\",\n",
        "            \"shannon_H\", \"simpson_1_minus_D\",\n",
        "            \"n_threatened_species\", \"threat_score_weighted\", \"dqi\"]]\n",
        "          .head(10)\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"\\nMetric summary:\")\n",
        "display(\n",
        "    sample[[\n",
        "        \"observation_count\", \"species_richness_cell\",\n",
        "        \"shannon_H\", \"simpson_1_minus_D\", \"dqi\",\n",
        "        \"n_threatened_species\", \"threat_score_weighted\",\n",
        "        \"avg_coordinate_uncertainty_m\",\n",
        "    ]].describe()\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
